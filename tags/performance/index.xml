<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Performance on Ask Asya </title>
    <link>http://asya999.github.io/tags/performance/</link>
    <language>en-US</language>
    <author>Asya Kamsky</author>
    <rights>Copyright (c) 2016, Asya Kamsky; all rights reserved.</rights>
    <updated>Wed, 29 Oct 2014 00:00:00 UTC</updated>
    
    <item>
      <title>How to Find and Kill Slow Running Queries</title>
      <link>http://asya999.github.io/post/findkillslowqueries/</link>
      <pubDate>Wed, 29 Oct 2014 00:00:00 UTC</pubDate>
      <author>Asya Kamsky</author>
      <guid>http://asya999.github.io/post/findkillslowqueries/</guid>
      <description>

&lt;h3 id=&#34;question&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Is there a way that I can prevent slow queries in MongoDB?  I&amp;rsquo;d like to be able to set something on the server that will kill all queries running longer than a certain amount of time.&lt;/p&gt;

&lt;h3 id=&#34;answer&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;There are some options available on the &lt;strong&gt;client&lt;/strong&gt; side, for example &lt;a href=&#34;http://docs.mongodb.org/manual/reference/operator/meta/maxTimeMS/&#34;&gt;$maxTimeMS&lt;/a&gt; starting in 2.6 release.  This gives you a way to inject an option to your queries &lt;em&gt;before&lt;/em&gt; they get to the server that tells the server to kill this query if it takes longer than a certain amount of time.  However, this does not help with any query which already got to the server without having this option added to it.&lt;/p&gt;

&lt;p&gt;On the &lt;strong&gt;server&lt;/strong&gt; side, there is no global option, because it would impact all databases and all operations, even ones that the system needs to be long running for internal operation (for example tailing the oplog for replication).  In addition, it may be okay for some of your queries to be longer running by design but not others.&lt;/p&gt;

&lt;p&gt;The correct way to solve this would be to monitor currently running queries via a script and kill the ones that are both long running &lt;em&gt;and&lt;/em&gt; user/client initiated - you can then build in exceptions for queries that are long running by design, or have different thresholds for different queries/collections/etc.&lt;/p&gt;

&lt;p&gt;The way to implement this script is by using &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/db.currentOp/&#34;&gt;db.currentOp() command&lt;/a&gt; (in the shell) to see all currently running operations.  The field &amp;ldquo;secs_running&amp;rdquo; indicates how long the operation has been running.  Other fields visible to you will be the namespace (&amp;ldquo;ns&amp;rdquo;) whether the operation is &amp;ldquo;active&amp;rdquo;, whether it&amp;rsquo;s waiting on a lock and for how long.   The &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/db.currentOp/#examples&#34;&gt;docs contain some good examples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Be careful not to kill any long running operations that are not initiated by your application/client - it may be a necessary system operation, like chunk migration in a sharded cluster as just one example, replication threads would be another.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Does FindAndModify Do</title>
      <link>http://asya999.github.io/post/findandmodify/</link>
      <pubDate>Mon, 19 May 2014 00:00:00 UTC</pubDate>
      <author>Asya Kamsky</author>
      <guid>http://asya999.github.io/post/findandmodify/</guid>
      <description>

&lt;h3 id=&#34;question&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;I saw &lt;a href=&#34;1&#34; title=&#34;Actually [William_Shakespeare][2] said it. 
&#34;&gt;your answer on SO&lt;/a&gt; about the difference between &amp;ldquo;update&amp;rdquo; and &amp;ldquo;findAndModify&amp;rdquo;, could you explain in more detail what the difference is, and why MongoDB findAndModify is named what it is?&lt;/p&gt;

&lt;h3 id=&#34;answer&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;What&amp;rsquo;s in a name?  that which we call a rose&lt;br /&gt;
 By any other name would smell as sweet&lt;/em&gt;;
&lt;p style=&#34;text-align:right&#34; markdown=&#34;1&#34;&gt; - said Juliet&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, a lot is in a name.  A poorly chosen name can confuse many users, year after year.  I believe &lt;code&gt;findAndModify&lt;/code&gt; was probably not the best name for the role that it plays.&lt;/p&gt;

&lt;h5 id=&#34;update&#34;&gt;update&lt;/h5&gt;

&lt;p&gt;An &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/db.collection.update/&#34;&gt;update&lt;/a&gt; finds an appropriate document (by default it&amp;rsquo;s just one) and then it changes its contents according to your specification.&lt;/p&gt;

&lt;h5 id=&#34;findandmodify&#34;&gt;findAndModify&lt;/h5&gt;

&lt;p&gt;The &lt;a href=&#34;http://docs.mongodb.org/manual/reference/command/findAndModify/#dbcmd.findAndModify&#34;&gt;findAndModify command&lt;/a&gt; finds an appropriate document (it&amp;rsquo;s always just one) and then it changes its contents according to your specification and &lt;em&gt;then it returns that exact document that it changed&lt;/em&gt; (old version or new version, depending on which you ask for)&lt;/p&gt;

&lt;h5 id=&#34;what-s-the-difference&#34;&gt;What&amp;rsquo;s the Difference?&lt;/h5&gt;

&lt;p&gt;They both find a document and update it atomically.  What that means is that it&amp;rsquo;s not possible for another thread to change part of this document between the time we find it and start updating it and when we finish updating it.   It also means that no other thread will see this document in &amp;ldquo;half-updated&amp;rdquo; state.  That&amp;rsquo;s what &amp;ldquo;atomic&amp;rdquo; means - all-or-nothing.&lt;/p&gt;

&lt;h5 id=&#34;why-do-we-even-need-findandmodify-then&#34;&gt;Why do we even need &lt;code&gt;findAndModify&lt;/code&gt; then?&lt;/h5&gt;

&lt;p&gt;What if we need to get the full document that we just updated (like marking an item in a queue &amp;ldquo;yours&amp;rdquo; and then working on it)?&lt;/p&gt;

&lt;p&gt;What I said on &lt;a href=&#34;http://stackoverflow.com&#34;&gt;StackOverflow&lt;/a&gt; was:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you fetch an item and then update it, there may be an update by another thread between those two steps. If you update an item first and then fetch it, there may be another update in-between and you will get back a different item than what you updated.&lt;/p&gt;

&lt;p&gt;Doing it &amp;ldquo;atomically&amp;rdquo; means you are guaranteed that you are getting back the exact same item you are updating - i.e. no other operation can happen in between.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That&amp;rsquo;s why you&amp;rsquo;ll hear people talk about &lt;code&gt;findAndModify&lt;/code&gt; in the context of implementing a queue mechanism - &lt;code&gt;findAndModify&lt;/code&gt; can update a single document to indicate that you are now working on it, and return that same document to you in one operation.&lt;/p&gt;

&lt;h5 id=&#34;when-not-to-use-findandmodify&#34;&gt;When Not to Use &lt;code&gt;findAndModify&lt;/code&gt;&lt;/h5&gt;

&lt;p&gt;There are scenarios where &lt;code&gt;findAndModify&lt;/code&gt; cannot help you.   If you need to update a document based on existing values of a document, you can use many &lt;a href=&#34;http://docs.mongodb.org/manual/reference/operator/update/#id1&#34;&gt;update operators&lt;/a&gt; which are atomic and allow you to change a field value without knowing what its current value is, like &lt;code&gt;$inc&lt;/code&gt; and &lt;code&gt;$addToSet&lt;/code&gt; and &lt;code&gt;$min&lt;/code&gt;  and &lt;code&gt;$max&lt;/code&gt;, etc.  They allow you to modify a field without having to read the value of that field first.  And they work with a regular &lt;code&gt;update&lt;/code&gt; as well as with &lt;code&gt;findAndModify&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But if you need to set field &lt;code&gt;a1&lt;/code&gt; based on the current value of the field &lt;code&gt;b2&lt;/code&gt; then you would have to read the document first and then when executing your update, you would have to ensure that the update is conditional on no one else having changed that document in the meantime and/or by having unique constraints to guarantee it.&lt;/p&gt;

&lt;p&gt;There is no way to utilize &lt;code&gt;findAndModify&lt;/code&gt; here, because it&amp;rsquo;s limited to the exact set of operators that &lt;code&gt;update&lt;/code&gt; uses, all it adds is the ability to return the exact document you modified.  Of course, &lt;code&gt;findAndModify&lt;/code&gt; has to do more work than &lt;code&gt;update&lt;/code&gt; so for best performance you should only use &lt;code&gt;findAndModify&lt;/code&gt; when you must have the document that you just updated back in the application.   If you just want to know if an &lt;code&gt;update&lt;/code&gt; succeeded, you can examine the &lt;a href=&#34;http://docs.mongodb.org/manual/reference/command/update/#output&#34;&gt;WriteResult&lt;/a&gt; that update returns.&lt;/p&gt;

&lt;h3 id=&#34;proposal&#34;&gt;Proposal&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s rename &lt;code&gt;findAndModify&lt;/code&gt; to a name that more accurately describes its function.  It updates a document and returns it, but to maintain a small connection to its current name, I nearby propose we rename it:&lt;/p&gt;

&lt;h2 id=&#34;modifyandreturn&#34;&gt;modifyAndReturn&lt;/h2&gt;

&lt;p&gt;Who&amp;rsquo;s with me? &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Actually &lt;a href=&#34;2&#34; title=&#34;Please vote for [SERVER-13979][a] if you agree.
&#34;&gt;William_Shakespeare&lt;/a&gt; said it.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Please vote for &lt;a href=&#34;https://jira.mongodb.org/browse/SERVER-13979&#34;&gt;SERVER-13979&lt;/a&gt; if you agree.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Can I use more replica nodes to scale?</title>
      <link>http://asya999.github.io/post/canreplicashelpscaling/</link>
      <pubDate>Thu, 20 Feb 2014 00:00:00 UTC</pubDate>
      <author>Asya Kamsky</author>
      <guid>http://asya999.github.io/post/canreplicashelpscaling/</guid>
      <description>

&lt;h3 id=&#34;question&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Do replica sets help with read scaling?  If there are more servers to service all my read requests, why wouldn&amp;rsquo;t they be able to service more read requests, or service the same number of read requests faster?&lt;/p&gt;

&lt;h3 id=&#34;answer&#34;&gt;Answer:&lt;/h3&gt;

&lt;h5 id=&#34;replica-sets&#34;&gt;Replica Sets.&lt;/h5&gt;

&lt;p&gt;Replica sets are an awesome feature of MongoDB.  They give you &amp;ldquo;High Availability&amp;rdquo; - meaning that when the primary node becomes unavailable (crashes, gets unplugged from the network, gets DOS&amp;rsquo;ed by another process on the same box) the rest of the nodes will &lt;strong&gt;elect&lt;/strong&gt; a new primary and the driver (which your application uses to communicate with MongoDB) will automatically track all nodes and when the primary role changes from one server to another, it will automatically detect to send requests there.&lt;/p&gt;

&lt;h5 id=&#34;single-master&#34;&gt;Single Master:&lt;/h5&gt;

&lt;p&gt;MongoDB Replica Sets are a &amp;ldquo;single master&amp;rdquo; architecture.  That means that all writes must go to the one primary and from there they are asynchronously replicated to all secondaries.   Your reads also go to the primary, meaning you can always read your own writes.  Your read requests would &lt;em&gt;never&lt;/em&gt; be sent to a secondary unless your application &lt;em&gt;explicitly&lt;/em&gt; requests that the read go somewhere other than the primary, so you would never be getting &amp;ldquo;stale&amp;rdquo; data without being aware of it.&lt;/p&gt;

&lt;h5 id=&#34;the-questions-we-have-are&#34;&gt;The questions we have are:&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;Should you use secondary reads?&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;Will they help you handle more reads?&lt;/li&gt;
&lt;li&gt;Will they help you handle same reads faster?&lt;/li&gt;
&lt;li&gt;Are there downsides?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;When should you use secondary reads?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because secondary reads create complexity for your application which now needs to handle (possibly very) stale data and situations where just-made-write is not available for read, I would caution users to be sure that they are getting some benefits from secondary reads.  I’m going to look at possible benefits and discuss whether they are likely to be realized using secondary replica nodes or not.&lt;/p&gt;

&lt;h5 id=&#34;will-more-servers-help-you-handle-same-reads-faster&#34;&gt;Will more servers help you handle same reads faster?&lt;/h5&gt;

&lt;p&gt;I think the answer for simple operational reads is obviously no.  If a read takes 10μs then it&amp;rsquo;s not likely to take 1/5th of that just because there are five servers - this is a single unit of work.  That&amp;rsquo;s the actual duration of the read.&lt;/p&gt;

&lt;h5 id=&#34;will-more-servers-help-you-handle-more-reads&#34;&gt;Will more servers help you handle more reads?&lt;/h5&gt;

&lt;p&gt;Intuitively, it feels like the answer should be “yes” - but that would only be the case if the reads somehow interfered with each other on the single node.  If they are reading the same “hot” data then they can be working in parallel up to the limit of your CPUs.   So in real life, the answer to whether all your replica nodes together can handle more reads than just your primary is maybe yes and maybe no. Usually no. It all depends on why your single primary cannot handle all of the reads by itself.&lt;/p&gt;

&lt;p&gt;My assumption is if your primary can handle all of the reads by itself, then you would have very little reason to even consider reading stale data from a secondary - you gain very little and lose strong consistency of data. There can be some scenarios where reading from a secondary will reduce the latency to the server (not the actual duration of the read) but those are rather specific use cases I’ll point out at the end of the article.&lt;/p&gt;

&lt;p&gt;Okay, so that leaves us with the sad case of a primary that is not able to handle all the reads all by itself. Now we must ask ourselves why it cannot handle all the reads. Depending on the reason why, we can try to predict whether directing some of those reads to secondaries will help the overall situation.&lt;/p&gt;

&lt;p&gt;If reads are not handled by primary alone because they are too slow then it doesn&amp;rsquo;t matter why they are too slow - they will be too slow on the secondaries as well. You can try to tune your queries, but it’s more likely that the queries are slow because of underlying root causes&amp;hellip; let&amp;rsquo;s look at those&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Indexes don&amp;rsquo;t fit in RAM - oops! we know that every replica set member has identical data so if indexes don&amp;rsquo;t fit in RAM on the primary, they don&amp;rsquo;t fit in RAM on the secondary either!&lt;/li&gt;
&lt;li&gt;Too much data being scanned (usually because working set doesn&amp;rsquo;t fit in RAM) and the disks are slow - pretty much the same as (1) since the secondaries will have the same limitation&lt;/li&gt;
&lt;li&gt;The large number of writes are starving out the reads (i.e. readers have to wait for writers and if there are too many writers when writers yield other writers go and reads can get starved in extreme cases).  But the writes that happen on the primary also have to happen on the secondary!  So moving the readers to the secondary will just starve them on the secondary instead of starving them on the primary.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I know what some of you may be thinking on (3) - you&amp;rsquo;re thinking that all the reads may be starving on the primary but if you split the reads between the primary and the secondary maybe they won&amp;rsquo;t get starved?&lt;/p&gt;

&lt;p&gt;Maybe.  But you know what you did when you achieved your read
requirements by splitting the load between the primary and the
secondary?   &lt;strong&gt;You gave up HA&lt;/strong&gt;.  Remember, if you can only service the incoming load when all of your nodes are up, then you don&amp;rsquo;t have &lt;em&gt;any&lt;/em&gt; high availability, as losing a node will basically start starving out some of the incoming requests which means you won&amp;rsquo;t meet your SLAs.&lt;/p&gt;

&lt;p&gt;Since replication is for High Availability, it means some of its capacity simply must not be tied up so that it can be “standing by” in case of a failover. If you want to use some extra capacity that you perceive is otherwise going &amp;ldquo;wasted&amp;rdquo; to service your every day load, maybe you can do it, as long as you have a very clear understanding that you may have given up some of that High Availability, and I would definitely recommend against that.&lt;/p&gt;

&lt;p&gt;One thing I invite you to try is to set up a simple test &lt;code&gt;mongod&lt;/code&gt; instance where you take some collection with some indexes that all fit in RAM and start up a bunch of multi-threaded clients (from other machines otherwise you&amp;rsquo;ll be testing something else) and have those clients hammer the server with read requests. See how many clients you have to add and how many requests you have to throw at &lt;code&gt;mongod&lt;/code&gt; before you can&amp;rsquo;t add any more clients requesting more reads without performance of existing reads suffering.  Trying it out will give you an idea of the maximum read throughput that a single node can handle.&lt;/p&gt;

&lt;p&gt;Of course, this is not the complete story.  It turns out that there are some excellent use cases for secondary reads, some more common, some less so.&lt;/p&gt;

&lt;h5 id=&#34;the-types-of-reads-to-route-to-secondaries&#34;&gt;The types of reads to route to secondaries&lt;/h5&gt;

&lt;p&gt;There are two types of use cases for reads that you do want to route to secondaries. One I already alluded to: if you have a read heavy system and reads are not super-sensitive to staleness of data but they &lt;em&gt;are&lt;/em&gt; super sensitive to overall latency of satisfying a read request, you can realize a big win reading from the nearest member of the replica set rather than the primary.  If your SLA says reads must return in under 100ms but your one way network latency to the primary is 75ms how can you satisfy the SLA from various parts of the world? That&amp;rsquo;s a use case for distributing data all over the world via secondaries in your replica set!  If you have nodes in data centers on different continents (and the PRIMARY near your write source) you can specify read preference of &amp;ldquo;nearest&amp;rdquo; and make sure that each read request goes to the node that has the lowest network latency from the requester.&lt;/p&gt;

&lt;p&gt;Note that this is &lt;em&gt;not&lt;/em&gt; technically &amp;ldquo;scaling&amp;rdquo; your read capacity, this is basically taking advantage of replication already having pulled the data over the long network connection and reducing your network latency to the DB.  Don’t forget that if you are relying on reading from “nearest” to meet your SLAs for total round trip to get the data, you have to consider whether your SLAs will still be met if a secondary in a particular region fails.&lt;/p&gt;

&lt;p&gt;The second use case is about reads that are not &amp;ldquo;typical&amp;rdquo; of your &amp;ldquo;normal&amp;rdquo; operational load.  Now, “typical” and “normal” is going to be different for different use cases, but some common examples are things like nightly ETL jobs, ad hoc &amp;ldquo;historical&amp;rdquo; or analytical queries, regular backup jobs, or a Hadoop job.  It could also be something else.  The reason you want to isolate this “atypical” load to the secondary is because it will &amp;ldquo;mess up&amp;rdquo; your memory-resident data set on the primary!&lt;/p&gt;

&lt;p&gt;Imagine you worked very hard to optimize your queries so that indexes are always in RAM and there is just enough RAM left over for the &amp;ldquo;hot&amp;rdquo; subset of data to give you excellent performance. You then start running an analytics query and it&amp;rsquo;s pulling &lt;em&gt;all&lt;/em&gt; the data from mongo which means your entire data set, hot and cold, now gets pulled into resident memory. Guess what just got evicted to make room for it? Yep, your &amp;ldquo;normal&amp;rdquo; data set, or at least a very large portion of it.†&lt;/p&gt;

&lt;p&gt;To keep &amp;ldquo;atypical&amp;rdquo; jobs from interfering with your &amp;ldquo;typical&amp;rdquo; work load, configure the atypical ones to use read preference Secondary. Note that Secondary is different from SecondaryPreferred as the latter will go to the Primary if there is no available Secondary but the former will return an error if there is no Secondary to read from.  Since these jobs are usually not urgent, you’d rather have them wait and retry later than interfere with operational responsiveness of your app anyway.  You can even use &lt;a href=&#34;http://docs.mongodb.org/manual/tutorial/configure-replica-set-tag-sets/&#34;&gt;&amp;ldquo;tags&amp;rdquo;&lt;/a&gt; to isolate different jobs to specific nodes even further.&lt;/p&gt;

&lt;h5 id=&#34;bottom-line&#34;&gt;Bottom line&lt;/h5&gt;

&lt;p&gt;Make your primary do the work that the application relies on every second.  Make sure that &lt;em&gt;at least&lt;/em&gt; one secondary is idle and ready to take over for the primary in case of failure.  Use additional secondaries (&lt;em&gt;not&lt;/em&gt; the hot standby one!) for all other needs whether it’s backups, analytical reports, ad hoc queries, ETL, etc.  This way, you know that you can handle the application requirements after a failover just as well as before.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;†  By the way, this is exactly what happens when you have a query that doesn&amp;rsquo;t have a good index to use - it does a full collection scan - more on that in a future &amp;ldquo;answer&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why shouldn&#39;t I embed large arrays in my documents?</title>
      <link>http://asya999.github.io/post/largeembeddedarrays/</link>
      <pubDate>Thu, 13 Feb 2014 00:00:00 UTC</pubDate>
      <author>Asya Kamsky</author>
      <guid>http://asya999.github.io/post/largeembeddedarrays/</guid>
      <description>

&lt;p&gt;+++
Categories = [&amp;ldquo;MongoDB&amp;rdquo;]
Title= &amp;ldquo;Why shouldn&amp;rsquo;t I embed large arrays in my documents?&amp;rdquo;
Date= &amp;ldquo;2014-02-13&amp;rdquo;
Slug= &amp;ldquo;largeembeddedarrays&amp;rdquo;
Tags= [&amp;ldquo;schema&amp;rdquo;,&amp;ldquo;performance&amp;rdquo;,&amp;ldquo;arrays&amp;rdquo;,&amp;ldquo;mongodb&amp;rdquo;]
+++&lt;/p&gt;

&lt;h3 id=&#34;question&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Why shouldn&amp;rsquo;t I embed large arrays in my documents?  It seems incredibly convenient and intuitive but I&amp;rsquo;ve heard there are performance penalties.  What causes them and how do I know if I should avoid using arrays?&lt;/p&gt;

&lt;h3 id=&#34;answer&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;Arrays are wonderful when used properly.  When talking about performance, the main reason to be wary of arrays is when they grow without bounds.&lt;/p&gt;

&lt;p&gt;Imagine you create a document:
&lt;pre class=&#34;prettyprint&#34;&gt;
{ user: &amp;ldquo;Asya&amp;rdquo;,
  email: &amp;ldquo;asya@mongodb.com&amp;rdquo;,
  twitter: [&amp;ldquo;@asya999&amp;rdquo;, &amp;ldquo;@ask-asya&amp;rdquo;]
}
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;Notice that twitter field is an array.  That&amp;rsquo;s perfectly fine and excellent - we don&amp;rsquo;t want to create a separate collection like we&amp;rsquo;d have to do in relational model, just because a person might have multiple twitter accounts/handles.&lt;/p&gt;

&lt;p&gt;Now that the document has been created, a certain amount of space has been allocated for it.  If we continue growing the document by adding new fields to it, it will have to be moved and a larger allocation will be made for it because MongoDB dynamically tracks how often documents outgrow their allocation and tries to allocate more space for newly written or moved documents to account for the future growth.&lt;/p&gt;

&lt;p&gt;Compare the cost of an update to a document when you can make an in-place change, versus rewriting the entire document somewhere else.  First, instead of just rewriting part of a document &amp;ldquo;in place&amp;rdquo; we have to allocate new space for it.  We have to rewrite the entire document, put the space that it used to occupy on the free list so that it can get re-used, and then repoint all the index entries that used to point to the old document location to the new location.  All of this must be done atomically, so your single write suddenly took a bit longer than a few microseconds that it used to take when the document didn&amp;rsquo;t have to move.&lt;/p&gt;

&lt;p&gt;Now imagine what happens if you add a new array field to the document representing something that&amp;rsquo;s not naturally bound the way someone&amp;rsquo;s twitter handles or shipping addresses would be bound.  What if we want to embed into this document every time I perform some activity, let&amp;rsquo;s say click on a like button, or make a comment on someone&amp;rsquo;s blog?&lt;/p&gt;

&lt;p&gt;First of all, we have to consider why we would want to do such a thing.  Normally, I would advise people to embed things that they always want to get back when they are fetching this document.  The flip side of this is that you don&amp;rsquo;t want to embed things in the document that you don&amp;rsquo;t want to get back with it.&lt;/p&gt;

&lt;p&gt;If you embed activity I perform into the document, it&amp;rsquo;ll work great at first because all of my activity is right there and with a single read you can get back everything you might want to show me: &amp;ldquo;you recently clicked on this and here are your last two comments&amp;rdquo; but what happens after six months go by and I don&amp;rsquo;t care about things I did a long time ago and you don&amp;rsquo;t want to show them to me unless I specifically go to look for some old activity?&lt;/p&gt;

&lt;p&gt;First, you&amp;rsquo;ll end up returning bigger and bigger document and caring about smaller and smaller portion of it.  But you can use projection to only return some of the array, the real pain is that the document on disk will get bigger and it will still all be read even if you&amp;rsquo;re only going to return part of it to the end user, but since my activity is not going to stop as long as I&amp;rsquo;m active, the document will continue growing and growing.&lt;/p&gt;

&lt;p&gt;The most obvious problem with this is eventually you&amp;rsquo;ll hit the 16MB document limit, but that&amp;rsquo;s not at all what you should be concerned about.  A document that continuously grows will incur higher and higher cost every time it has to get relocated on disk, and even if you take steps to mitigate the effects of fragmentation, your writes will overall be unnecessarily long, impacting overall performance of your entire application.&lt;/p&gt;

&lt;p&gt;There is one more thing that you can do that will completely kill your application&amp;rsquo;s performance and that&amp;rsquo;s to index this ever-increasing array.  What that means is that every single time the document with this array is relocated, the number of index entries that need to be updated is directly proportional to the number of indexed values in that document, and the bigger the array, the larger that number will be.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t want this to scare you from using arrays when they are a good fit for the data model - they are a powerful feature of the document database data model, but like all powerful tools, it needs to be used in the right circumstances and it should be used with care.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
