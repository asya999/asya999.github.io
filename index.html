<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
       &middot; 
    
  </title>

  
  <link rel="stylesheet" href="http://asya999.github.io/css/poole.css">
  <link rel="stylesheet" href="http://asya999.github.io/css/syntax.css">
  <link rel="stylesheet" href="http://asya999.github.io/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://asya999.github.io/assets/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="http://asya999.github.io/assets/favicon.ico">

  
  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://asya999.github.io/atom.xml">
</head>


  <body class="theme-base-0b">

    
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">


<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A reserved <a href="http://jekyllrb.com" target="_blank">Jekyll</a> theme that places the utmost gravity on content with a hidden drawer. Made by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item  active " href="http://asya999.github.io/">Home</a>
    <a class="sidebar-nav-item " href="http://asya999.github.io/post">Posts</a>

    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a class="sidebar-nav-item " href="http://asya999.github.io/about/">About</a>
      
    
      
    
      
    

    <a class="sidebar-nav-item" href="http://github.com/asya999/bits-n-pieces">GitHub project</a>
    <span class="sidebar-nav-item">Currently on master</span>
  </nav>

  <div class="sidebar-item">
    <p>&copy; 2014. All rights reserved.</p>
  </div>
</div>


    
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="http://asya999.github.io/" title="Home"></a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">




<div class="posts">
  
    
      <div class="post">
        <h1 class="post-title"><a href="http://asya999.github.io/post/findkillslowqueries/">How to Find and Kill Slow Running Queries</a></h1>
        <span class="post-date">Wed, Oct  29, 2014</span>
        

<h3 id="question:18aaff1e2e315456fa60158351f7ffe2">Question:</h3>

<p>Is there a way that I can prevent slow queries in MongoDB?  I&rsquo;d like to be able to set something on the server that will kill all queries running longer than a certain amount of time.</p>

<h3 id="answer:18aaff1e2e315456fa60158351f7ffe2">Answer:</h3>

<p>There are some options available on the <strong>client</strong> side, for example <a href="http://docs.mongodb.org/manual/reference/operator/meta/maxTimeMS/">$maxTimeMS</a> starting in 2.6 release.  This gives you a way to inject an option to your queries <em>before</em> they get to the server that tells the server to kill this query if it takes longer than a certain amount of time.  However, this does not help with any query which already got to the server without having this option added to it.</p>

<p>On the <strong>server</strong> side, there is no global option, because it would impact all databases and all operations, even ones that the system needs to be long running for internal operation (for example tailing the oplog for replication).  In addition, it may be okay for some of your queries to be longer running by design but not others.</p>

<p>The correct way to solve this would be to monitor currently running queries via a script and kill the ones that are both long running <em>and</em> user/client initiated - you can then build in exceptions for queries that are long running by design, or have different thresholds for different queries/collections/etc.</p>

<p>The way to implement this script is by using <a href="http://docs.mongodb.org/manual/reference/method/db.currentOp/">db.currentOp() command</a> (in the shell) to see all currently running operations.  The field &ldquo;secs_running&rdquo; indicates how long the operation has been running.  Other fields visible to you will be the namespace (&ldquo;ns&rdquo;) whether the operation is &ldquo;active&rdquo;, whether it&rsquo;s waiting on a lock and for how long.   The <a href="http://docs.mongodb.org/manual/reference/method/db.currentOp/#examples">docs contain some good examples</a>.</p>

<p>Be careful not to kill any long running operations that are not initiated by your application/client - it may be a necessary system operation, like chunk migration in a sharded cluster as just one example, replication threads would be another.</p>

      </div>
    
  
    
      <div class="post">
        <h1 class="post-title"><a href="http://asya999.github.io/post/revisitversions/">Further Thoughts on How to Track Versions with MongoDB</a></h1>
        <span class="post-date">Sun, Sep  7, 2014</span>
        

<h3 id="guest-post-by-paul-done-http-pauldone-blogspot-co-uk:a2980dc88478af7ab881c721f66e9b4d">GUEST POST by <a href="http://pauldone.blogspot.co.uk/">Paul Done</a> </h3>

<p>In a <a href="http://askasya.com/post/trackversions">previous Ask Asya blog post</a>,
Asya outlined various approaches for preserving historical versions of records for auditing purposes, whilst allowing current versions of records to be easily inserted and queried. Having found the post to be extremely useful for one of my projects, and following some further investigations of my own, I realised that two of the choices could be refined a little to be more optimal. Upon feeding back my findings, Asya graciously asked me to document them here, so here goes.</p>

<h4 id="revisit-of-choice-2-embed-versions-in-a-single-document:a2980dc88478af7ab881c721f66e9b4d">Revisit of Choice 2  (Embed Versions in a Single Document)</h4>

<p>The presented ‘compare-and-swap&rsquo; example code, to generate a new version and update version history, is very effective at ensuring consistency of versions in a thread-safe manner. However, I felt that there was scope
to reduce the update latency which will be particularly high when a document has grown, with many previous versions embedded.</p>

<p>For example, if a current document has tens of embedded previous versions, then projecting the whole document back to the client application, updating part of the document copy and then sending the whole document as an update to the database, will be slower than necessary. I prototyped a refactored version of the example code (shown below) which exhibits the same functional behaviour, but avoids projecting the entire document and uses an in-place update to push changes to the database.</p>

<p>Don&rsquo;t return all the old versions:</p>

<pre><code>    var doc = db.coll.findOne({&quot;_id&quot;: 174}, {&quot;prev&quot;: 0});  
    var currVersion = doc.current.v;
    var previous = doc.current;
    var current = {
          &quot;v&quot; : currVersion+1,
          &quot;attr1&quot;: doc.current.attr1,
          &quot;attr2&quot;: &quot;YYYY&quot;
    };
</code></pre>

<p>Perform in-place update of changes only: </p>

<pre><code>    var result = db.coll.update(
         { &quot;_id&quot; : 174, &quot;current.v&quot;: currVersion},
         { &quot;$set&quot; :  {&quot;current&quot;: current},
           &quot;$push&quot; :  {&quot;prev&quot;: previous}
         }
    );

    if (result.nModified != 1)  {
         print(&quot;Someone got there first, replay flow to try again&quot;);
    }
</code></pre>

<p>As a result, even when the number of versions held in a document
increases over time, the update latency for adding a new version remains roughly constant.</p>

<h4 id="revisit-of-choice-3-separate-collection-for-previous-versions:a2980dc88478af7ab881c721f66e9b4d">Revisit of Choice 3  (Separate Collection for Previous Versions)</h4>

<p>The original post implies that this choice is technically challenging to implement, to ingest a new document version whilst maintaining consistency with previous versions, in the face of system failure.  However, I don&rsquo;t feel it&rsquo;s that bad, if the update flow is crafted carefully. If the order of writes is implemented as &ldquo;write to previous collection before writing to current collection&rdquo;, then in a failure scenario, there is potential for a duplicate record version but not a lost record version. Also, there are ways for subsequent querying code to gracefully deal with the duplicate.</p>

<p>If the following three principles are acceptable to an application development team, then this is a viable versioning option and doesn&rsquo;t have the implementation complexity of choice 5, for example:</p>

<ol>
<li><p>System failure could result in a duplicate version, but not a lost version.</p></li>

<li><p>Any application code that wants to query all or some versions of the same entity, is happy to issue two queries simultaneously, one against the current collection (to get the current version) and one against the previous collection (to get historic versions), and then merge the results. In cases where a duplicate has been introduced (but not yet cleaned up - see next point), the application code just has to detect that the latest version in the current collection also appears as a record in the previous collection. When this occurs, the application code just ignores the duplicate, when constructing its results. In my experience, most &lsquo;normal&rsquo; queries issued by an application, will just query the current collection and be interested in latest versions of entities only. Therefore this &lsquo;double-query&rsquo; mechanism is only needed for the parts of an application where historic version analysis is required.</p></li>

<li><p>The next time a new version of a document is pushed into the system, the old duplicate in the previous collection (if the duplicate exists) will become a genuine previous version. The current collection will contain the new version and the previous collection will only contain previous versions. As a result, there is no need for any background clean up code mechanisms to be put in place.</p></li>
</ol>

<p>For clarity, I&rsquo;ve included a JavaScript example of the full update flow below, which can be run from the Mongo shell.</p>

<pre><code>    //
    // CREATE SAMPLE DATA
    //

    db.curr_coll.drop();
    db.prev_coll.drop();
    db.curr_coll.ensureIndex({&quot;docId&quot; : 1}, {&quot;unique&quot;: true});
    db.prev_coll.ensureIndex({&quot;docId&quot; : 1,  &quot;v&quot; :1}, {&quot;unique&quot;: true});
    db.curr_coll.insert([
         {&quot;docId&quot;: 174, &quot;v&quot;: 3, &quot;attr1&quot;: 184, &quot;attr2&quot;: &quot;A-1&quot;},
         {&quot;docId&quot;: 133, &quot;v&quot;: 3, &quot;attr1&quot;: 284, &quot;attr2&quot;: &quot;B-1&quot;}
    ]);

    db.prev_coll.insert([
         {&quot;docId&quot;: 174, &quot;v&quot;: 1, &quot;attr1&quot;: 165},
         {&quot;docId&quot;: 174, &quot;v&quot;: 2, &quot;attr1&quot;: 165, &quot;attr2&quot; : &quot;A-1&quot;},
         {&quot;docId&quot;: 133, &quot;v&quot;: 1, &quot;attr1&quot;: 265},
         {&quot;docId&quot;: 133, &quot;v&quot;: 2, &quot;attr1&quot;: 184, &quot;attr2&quot; : &quot;B-1&quot;}
    ]);

    //
    // EXAMPLE TEST RUN FLOW 
    //
    // UPSERT (NOT INSERT) IN CASE FAILURE OCCURED DURING PRIOR ATTEMPT.
    // THE PREV COLLECTION MAY ALREADY CONTAIN THE 'OLD' CURRENT VERSION.
    // IF ALREADY PRESENT, THIS UPSERT WILL BE A 'NO-OP', RETURNING:
    //  nMatched: 1, nUpserted: 0, nModified: 0.

    var previous = db.curr_coll.findOne({&quot;docId&quot;: 174}, {_id: 0});
    var currVersion = previous.v;
    var result = db.prev_coll.update(
         {&quot;docId&quot; : previous.docId, &quot;v&quot;: previous.v },
         { &quot;$set&quot;: previous }
       , {&quot;upsert&quot;: true});

    // &lt;-- STOP EXECUTION HERE ON A RUN TO SIMULATE FAILURE, THEN RUN
    //     FULL FLOW TO SHOW HOW THINGS WILL BE NATURALLY CLEANED-UP
    // UPDATE NEW VERSION IN CURR COLLECTION, USING THREAD-SAFE VERSION CHECK

    var current = {&quot;v&quot;: currVersion+1, &quot;attr1&quot;: previous.attr1, &quot;attr2&quot;:&quot;YYYY&quot;};
    var result = db.curr_coll.update({&quot;docId&quot;: 174, &quot;v&quot;: currVersion},
         {&quot;$set&quot;: current}
    );

    if (result.nModified != 1) {
         print(&quot;Someone got there first, replay flow to try again&quot;);
    }

    //
    // EXAMPLE VERSION HISTORY QUERY CODE
    //

    // BUILD LIST OF ALL VERSIONS OF ENTITY, STARTING WITH CURRENT VERSION

    var fullVersionHistory = [];
    var latest = db.curr_coll.findOne({&quot;docId&quot;: 174}, {_id: 0});
    var latestVersion = latest.v;
    fullVersionHistory.push(latest);

    // QUERY ALL PREVIOUS VERSIONS (EXCLUDES DUPLICATE CURRENT VERSION IF EXISTS)
    var previousVersionsCursor = db.prev_coll.find({
         &quot;$and&quot;: [
              {&quot;docId&quot;: 174},
              {&quot;v&quot;: {&quot;$ne&quot;: latestVersion}}
         ]
    }, {_id: 0}).sort({v: -1});

    // ADD ALL THESE PREVIOUS VERSIONS TO THE LIST
    previousVersionsCursor.forEach(function(doc) {
          fullVersionHistory.push(doc);
    });

    // DISPLAY ALL VERSIONS OF AN ENTITY (NO DUPLICATES ARE PRESENT)
    printjson(fullVersionHistory);
</code></pre>

<p>As a result of this approach, it is easy to query current versions of entities, easy to query the full version history of a given entity and easy to update an entity with a new version.</p>

<h3 id="in-summary:a2980dc88478af7ab881c721f66e9b4d">In Summary</h3>

<p>I&rsquo;ve taken the liberty of providing a modified version of Asya&rsquo;s summary table below, to expand out the criteria that may be relevant when choosing a versioning strategy for a given use case. My version of the table also reflects the improved results for choices 2 and 3, on the back of what has been discussed in this blog post.</p>

<p>Updated Table of Tradeoffs:</p>

<p><img src="https://dl.dropboxusercontent.com/u/35111849/table.png" alt="Updated Table of Tradeoffs" />.</p>

      </div>
    
  
    
      <div class="post">
        <h1 class="post-title"><a href="http://asya999.github.io/post/socialstatusfeed/">Social Status Feed in MongoDB</a></h1>
        <span class="post-date">Wed, Aug  27, 2014</span>
        

<h3 id="socialite:36d9d7ac96775ede158c079032b00e59">Socialite</h3>

<p>At MongoDBWorld, my colleague Darren Wood and I gave <a href="http://www.mongodb.com/search/google/socialite?query=socialite&amp;cx=017213726194841070573%3Ak6mpwzohlje&amp;cof=FORID%3A9&amp;sitesearch=">three back-to-back presentations</a> about an <a href="http://github.com/10gen-labs/socialite">open source project called Socialite</a> which is a reference architecture implementation of a social status feed service.  Darren was the one who wrote the bulk of the source code and I installed and ran extensive performance tests in different configurations to determine how the different schema and indexing options scale and to get an understanding of the resources required for various sizes and distributions of workloads.</p>

<p>The recordings and slides are <a href="http://www.mongodb.com/search/google/socialite?query=socialite&amp;cx=017213726194841070573%3Ak6mpwzohlje&amp;cof=FORID%3A9&amp;sitesearch=">now available on MongoDB website</a>, if you want to jump in and watch, but since we had to race through the material,  I&rsquo;m going to blog about some of the more interesting questions it raised, mainly about schema design, indexing and sharding choices and how to benchmark a large, complex application.</p>

<p>There were three talks because there was a large amount of material and because there are several rather complex orthogonal topics when considering a social status feed:</p>

<ol>
<li>How will you store the content long term</li>
<li>How will you store the user graph</li>
<li>How will you serve up the status feed for a particular user when they log in</li>
</ol>

<p>The last one is probably most interesting in that it has the most possible approaches, though as it turns out, some have very big downsides and would only be appropriate to pretty small systems.  User graph is fascinating because of its relevance to so many different domains beyond social networks of friends.  And performance considerations are complex and interdependent among all of them.   For each of the three talks we had two parts - Darren discussed possible schema designs, indexing considerations and if appropriate sharding implications, and I walked through the actual testing I did and whether each option held up as expected.</p>

<p>Unfortunately even across three sessions we were quite time limited, so all the various bits of material we have that didn&rsquo;t make it into these presentations will end up in one of several spots:</p>

<ul>
<li>here</li>
<li><a href="http://blog.mongodb.org">mongodb.org blog</a></li>
<li>advanced schema design class at <a href="https://univerity.mongodb.com">MongoDB University</a> (coming soon!)</li>
</ul>

<p>I get a lot of questions about schema design, and social data is both popular and very doable in MongoDB but the naive approach is usually bound to meet with failure, so the schema needs to be carefully considered with an eye towards the following two most important considerations:</p>

<ul>
<li>enduser latency</li>
<li>linear scaling</li>
</ul>

<p>As we said during the presentations, for every single decision, we had to consider as the most important goals keeping the user&rsquo;s first read latency as low and constant as possible (or else they would leave and go somewhere else) and our ability to scale any design we had linearly with scaling.  That meant that every single workload had to be scalable or partitionable in a way that would isolate the workload to a subset of data.</p>

<p>Over the next few months as I write up different parts of the system, and consider the schema, indexes and possible shard data distribution, you will see me return to these two litmus tests again and again.   In order to have highest chance of success at large scale any option that hinders one of these goals should be out of the running.</p>

      </div>
    
  
    
      <div class="post">
        <h1 class="post-title"><a href="http://asya999.github.io/post/bestversion/">Best Versions with MongoDB</a></h1>
        <span class="post-date">Fri, May  30, 2014</span>
        

<h3 id="question:897d8a06bfdbaecbf747ed08c519e7de">Question:</h3>

<p>Recall <a href="http://askasya.com/post/trackversions">our previous discussion</a> about ways to  recreate older version of a document that ever existed in a particular collection.</p>

<p>The goal was to preserve every state for each object, but to respond to queries with the &ldquo;current&rdquo; or &ldquo;latest&rdquo; version.   We had a requirement to be able to have an infrequent audit to review all or some previous versions of the document.</p>

<h3 id="answer:897d8a06bfdbaecbf747ed08c519e7de">Answer:</h3>

<p>I had suggested at the time that there was a different way to achieve this that I liked better than the discussed methods and I&rsquo;m going to describe it now.</p>

<h4 id="previous-discussion-summary:897d8a06bfdbaecbf747ed08c519e7de">Previous Discussion Summary:</h4>

<p>Up to this point, we considered keeping versions of the same document within one MongoDB document, in separate documents within the same collection, or by &ldquo;archiving off&rdquo; older versions of the document into a separate collection.</p>

<p>We looked at the trade-offs and decided that the important factors were our ability to</p>

<ul>
<li>return or match only the current document(s)</li>
<li>generate new version number to &ldquo;update&rdquo; existing and add new attributes

<ul>
<li>including recovering from failure in the middle of a set of operations (if there is more than one)</li>
</ul></li>
</ul>

<h5 id="where-we-left-off:897d8a06bfdbaecbf747ed08c519e7de">Where we left off:</h5>

<p>Here&rsquo;s a table that shows for each schema choice that we considered how well we can handle the reads, writes and if an update has to make more than one write, how easy it is to recover or to be in a relatively &ldquo;safe&rdquo; state:</p>

<pre><code>         Schema         | Fetch 1       | Fetch Many    | Update       | Recover if fail 
</code></pre>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;&mdash;&mdash;- | &mdash;&mdash;&mdash;&mdash;- | &mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;&mdash;&mdash;&mdash;
1) New doc for each         | Easy,Fast     | Not easy,Slow | Medium       | N/A
1a) New doc with &ldquo;current&rdquo;  | Easy,Fast     | Easy,Fast     | Medium       | Hard
2) Embedded in single doc   | Easy,Fastest  | Easy,Fastest  | Medium       | N/A
3) Sep Collection for prev. |  Easy,Fastest | Easy,Fastest  | Medium       |  Medium Hard
4) Deltas only in new doc   | Hard,Slow     | Hard,Slow     | Medium       | N/A
?) TBD                      |  Easy,Fastest | Easy,Fastest  | Easy,Fastest |  N/A</p>

<p>&ldquo;N/A&rdquo; for recovery means there is no inconsistent state possible - if we only have to make one write to create/add a new version, we are safe from any inconsistency.  So &ldquo;N/A&rdquo; is the &ldquo;easiest&rdquo; value there.</p>

<p>What we want is something that makes all our tasks easy, and does not have any performance issues nor consistency problems.   For creating this solution, we will pick and choose the best parts of the previously considered schema.</p>

<p>No doubt you noticed that fetching one or many is fastest and simplest when we keep the old versioned documents out of our &ldquo;current&rdquo; collection.  This makes our queries whether for one or all latest versions fast and they can use indexes whether you&rsquo;re querying, updating or aggregating.</p>

<p>How do we get fast updates that keep the current document current but save the previous version somewhere else?  We know that we don&rsquo;t have multi-statement transaction in MongoDB so we can&rsquo;t ensure that a regular  update of one document and an insert of another document are atomic.  However, there <em>is</em> something that&rsquo;s always updated atomically along with <em>every</em> write that happens in your collection, and that is the &ldquo;Oplog&rdquo;.</p>

<h4 id="the-oplog:897d8a06bfdbaecbf747ed08c519e7de">The Oplog</h4>

<p>The oplog (full name: &lsquo;oplog.rs&rsquo; collection in &lsquo;local&rsquo; database) is a special collection that&rsquo;s used by the replication mechanism.  Every single write operation is persisted into the oplog atomically with being applied to the data files, indexes and the journal.  You can read more about the oplog in <a href="http://docs.mongodb.org/manual/core/replica-set-oplog/">the docs</a>, but what I&rsquo;m going to show you is what it looks like in the oplog when an insert or update happens, and how we can use that for our own purposes.</p>

<p>If I perform this insert into my collection:</p>

<pre><code>&gt; db.docs.insert(
           {&quot;_id&quot;:ObjectId(&quot;5387edd9ba5871da01786f85&quot;), 
            &quot;docId&quot;:174, &quot;version&quot;:1, &quot;attr1&quot;:165});
WriteResult({ &quot;nInserted&quot; : 1 })
</code></pre>

<p>what I will see in the oplog will look like this:</p>

<pre><code>&gt; db.getSiblingDB(&quot;local&quot;).oplog.rs.find().sort({&quot;$natural&quot;:-1}).limit(-1).pretty();
{
         &quot;ts&quot; : Timestamp(1401417307, 1),
          &quot;h&quot; : NumberLong(&quot;-1030581192915920539&quot;), 
          &quot;v&quot; : 2, 
          &quot;op&quot; : &quot;i&quot;, 
          &quot;ns&quot; : &quot;blog.docs&quot;, 
          &quot;o&quot; : { 
                        &quot;_id&quot; : ObjectId(&quot;5387edd9ba5871da01786f85&quot;), 
                        &quot;docId&quot; : 174, 
                        &quot;version&quot; : 1, 
                        &quot;attr1&quot; : 165 
          } 
}
</code></pre>

<p>If I perform this update:</p>

<pre><code>&gt; db.docs.update( 
               { &quot;docId&quot; : 174 }, 
               { &quot;$inc&quot;:{&quot;version&quot;:1}, &quot;$set&quot;:{ &quot;attr2&quot;: &quot;A-1&quot; }  } 
    );
WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })
</code></pre>

<p>what I get in the oplog is this:</p>

<pre><code>{
        &quot;ts&quot; : Timestamp(1401417535, 1),
        &quot;h&quot; : NumberLong(&quot;2381950322402503088&quot;),
         &quot;v&quot; : 2,
         &quot;op&quot; : &quot;u&quot;,
         &quot;ns&quot; : &quot;blog.docs&quot;,
         &quot;o2&quot; : {
                 &quot;_id&quot; : ObjectId(&quot;5387edd9ba5871da01786f85&quot;)
         },
         &quot;o&quot; : {
                &quot;$set&quot; : {
                        &quot;version&quot; : 2,
                        &quot;attr2&quot; : &quot;A-1&quot;
                }
         }
}
</code></pre>

<p>It turns out that with versioned documents, I wouldn&rsquo;t actually ever do an insert, but rather I would just always do an update, with an upsert option, that way I don&rsquo;t need to test if a document with this <code>docId</code> already exists.</p>

<pre><code>&gt; db.docs.update( 
     { &quot;docId&quot; : 175 }, 
     { &quot;$inc&quot;:{&quot;version&quot;:1}, &quot;$set&quot;:{ &quot;attr1&quot;: 999 }  }, 
     { &quot;upsert&quot; : true } 
);
WriteResult({
    &quot;nMatched&quot; : 0,
    &quot;nUpserted&quot; : 1,
    &quot;nModified&quot; : 0,
    &quot;_id&quot; : ObjectId(&quot;5387eff7a08472e30040b4bc&quot;)
})
</code></pre>

<p>Let&rsquo;s see what the oplog entry for this upsert looks like:</p>

<pre><code>{ 
    &quot;ts&quot; : Timestamp(1401417719, 1), 
    &quot;h&quot; : NumberLong(&quot;2031090002854356513&quot;), 
    &quot;v&quot; : 2, 
    &quot;op&quot; : &quot;i&quot;, 
    &quot;ns&quot; : &quot;blog.docs&quot;, 
    &quot;o&quot; : {
         &quot;_id&quot; : ObjectId(&quot;5387eff7a08472e30040b4bc&quot;), 
         &quot;docId&quot; : 175, 
         &quot;version&quot; : 1, 
         &quot;attr1&quot; : 999 
    } 
}
</code></pre>

<p>Looks like the oplog entry reflects the actual operation that was performed, <strong>not</strong> the operation that I specified.  I asked for an update - when it&rsquo;s an update, the oplog will show it as an update, when it&rsquo;s turned into an upsert, the oplog will show it as an insert.  When it was an update, I had asked it to &ldquo;increment&rdquo; but what it put in the oplog was what the actual value saved was.<sup class="footnote-ref" id="fnref:897d8a06bfdbaecbf747ed08c519e7de:1"><a rel="footnote" href="#fn:897d8a06bfdbaecbf747ed08c519e7de:1">1</a></sup></p>

<p>I&rsquo;m sure most of you see where I&rsquo;m going with this.  Rather than fumbling with creating and updating documents in the &ldquo;previous versions&rdquo; collection when we perform an update to a document, we can do it asynchronously, the way MongoDB secondaries do it.</p>

<p>You may think it&rsquo;s not easy, but it turns out that there are lots of <a href="http://docs.mongodb.org/manual/reference/method/cursor.addOption/">helpers</a> for dealing with <a href="http://docs.mongodb.org/manual/core/capped-collections/">capped collections</a> (which is what the oplog is).  One of the most useful things you can do is &ldquo;tail the oplog&rdquo;.  This is the same mechanism that secondaries use to find out when new writes happen on the primary: they tail the oplog the same way you can do <code>tail -f logfile.txt</code> command - this will show you the last part of the file, but rather than giving you back the prompt when it&rsquo;s done, it will just sit there and wait.  When more things are written to the file, <code>tail -f</code> will echo them to the screen.   This is exactly how it works with <a href="http://docs.mongodb.org/manual/reference/method/cursor.addOption/#example">tailable cursors</a> on capped collections.  If you specify the <a href="http://docs.mongodb.org/meta-driver/latest/legacy/mongodb-wire-protocol/?pageVersion=106#op-query">right special options</a>, you can get data back, but when there is no more data, instead of timing out and having to re-query, you will just sit there and wait till more data shows up.</p>

<p>Here is a little demo.  The code and explanations are after the video, so feel free to browse ahead before watching, or you can watch first and read the explanations after.</p>

<h5 id="tailing-the-oplog-to-maintain-a-copy-of-a-collection-elsewhere:897d8a06bfdbaecbf747ed08c519e7de">Tailing the oplog to maintain a copy of a collection elsewhere</h5>

<p>For our first example, we&rsquo;ll do something simple - we will watch the oplog for changes to a specific collection, and then we will apply those changes to our own copy of the collection - we will call our collection something else.  Our example stores the copy in the same database, but of course, it could be anywhere else, including in a completely different replica set or standalone server.</p>

<pre>
<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
<iframe width="560" height="315" src="//www.youtube.com/embed/U-MVlb0cRHU" frameborder="0" allowfullscreen></iframe>
</pre>

<p>Code for set-up of variables with comments:</p>

<pre class="prettyprint lang-js">
/* shell likes to ”page” things for us every 20 documents,  *
 * so we will increase that value for this demo */
DBQuery.shellBatchSize=200;
var namespace="blog.docs"; /* the collection to watch */ 
var coll="docs_archive";  /* copy collection name */

db.getCollection(coll).drop();  /* only the very first time :) */
db.lastApplied.drop();
/* find the last timestamp we applied: this is where we restart */
var prevTS=(db.getCollection("lastApplied").count({"_id":coll})==0) ?
     db.getSiblingDB("local").oplog.rs.findOne({"ns":namespace}).ts :
     db.getCollection("lastApplied").findOne({"_id":coll}).ts;
/* initialize or update lastApplied for this collection */
db.getCollection("lastApplied").update(
                { "_id" : coll }, 
                { "$set : { "ts" : prevTS } },
                { "upsert" : true }
);
</pre>

<p>Code for setting up the cursor using <a href="http://docs.mongodb.org/manual/reference/method/cursor.addOption/#flags">appropriate options</a> allows us to find the right spot in the oplog quickly, and makes our cursor tail the data, asking server to forgo the usual cursor timeout based on inactivity:</p>

<pre class="prettyprint lang-js">
/* set up the cursor with appropriate filter and options */
var cursor=db.getSiblingDB("local").getCollection("oplog.rs"
    ).find({"ns":namespace,"ts":{"$gte":prevTS}}
    ).addOption(DBQuery.Option.oplogReplay
    ).addOption(DBQuery.Option.awaitData
    ).addOption(DBQuery.Option.tailable
    ).addOption(DBQuery.Option.noTimeout);
</pre>

<p>Code running on the right-hand-side (blue screen) which loops and inserts or updates the watched collection every second:</p>

<pre class="prettyprint lang-js">
for ( docId = 270; docId < 290; docId++ ) {
    print("waiting one second...");
    sleep(1000);
    printjson(db.docs.update(
          { "_id": docId },
          { "$inc" : {"version":1}, "$set":{"attr7":"xxx"+docId } },
          { "upsert" : true }));
}
</pre>

<p>Code that fetches documents from the tailable cursor and applies appropriate changes to our &ldquo;copy&rdquo; collection:</p>

<pre class="prettyprint lang-js">
while (cursor.hasNext()) {
       
    var doc=cursor.next();
    var operation = (doc.op=="u") ? "update" : "insert";
    print("TS: " + doc.ts + " for " + operation + " at " + new Date());
    
    if ( doc.op == "i") {  /* originally an upsert */
        result = db.getCollection(coll).save(doc.o);
        if (result.nUpserted==1) print("Inserted doc _id " + doc.o._id);
        else {
           if (result.nMatched==1 ) {
             if ( result.nModified==0) {
                 print("Doc " + doc.o._id + " exists.");
             } else  print("Doc " + doc.o._id + " may have been newer");
           } else {
             throw "Insert error " + tojson(doc)  + " " + tojson(result);
           }
        }
    } else if ( doc.op == "u" ) { /* originally an update */
        result = db.getCollection(coll).update(doc.o2, doc.o);
        if (result.nModified ==1) print("Updated doc _id " + doc.o2._id);
        else if (result.nMatched==1 && result.nModified==0) print(
                   "Already updated doc _id " + doc.o2._id);
        else  throw "No update for " + tojson(doc) + " " + tojson(result);
    } else if (doc.op != "c") throw "Unexpected op! " + tojson(doc);
    
    res=db.getCollection("lastApplied").update(
            { "_id" : coll },
            { "$set" : { "ts" : doc.ts } }
    );

    if (res.nModified==1) print("Updated lastApplied to " + doc.ts);
    else print("Repeated last applied op");

    prevTS=doc.ts; /* save in case we need to refetch from the oplog */
}
</pre>
Of course this code does minimal error checking and it's not set up to automatically restart if it loses connection to the primary, or the primary changes in the replica set.  This is because here we are reading from a local oplog when in real life you may be fetching data from another server or cluster entirely.  Even so, about 15 lines of code there are for error checking and information printing, so the actual "work" we do is quite simple.  

##### Creating a full archive from tailing the oplog #####

Now that we know how to replay original documents to maintain an indentical "copy" collection, let's see what we have to do differently when we want to insert a new version of the document without losing any of the old versions.

For simplicity, I put the docId in the example collection into the `_id` field, so I will need to structure the full archive collection schema differently, since it cannot have multiple documents with the same `_id`.[^2]  For simplicity, I will let MongoDB generate the `_id` and I will use the combination of docId and version with a unique index on them to prevent duplicate versions.  I could achieve something similar by using the combination of original `_id` (which is the docId) and `version` fields as a compound `_id` field but then I would need to do more complicated transformations on the oplog entry.  I always choose the method that is  simpler.

Now when we get an insert operation in the oplog, we should be able to insert the document the same way we were doing it before, except we want to move `_id` value into `docId` field.  If the save fails to insert a new document because of a duplicate constraint violation, then we already have that docId and version - we would expect that when we are replaying the same entry in the oplog more than once.   

If we get an update, it can be one of two kinds - it can be one that sets or unsets specific fields, or it can be the kind that overwrites the entire document with a new document (with the same `_id` of course).  The latter case can be handled by the same code we have for the insert, with an appropriate transformation of the document.   If it's the `$set` and `$unset` kind, then we have to fetch the previous version of this document and apply the changes to it before inserting it as a document representing a new version.[^3]

Here is our code, with comments:

The setup:
<pre class="prettyprint lang-js">
var coll="docs_full_archive";  // different collection
db.getCollection(coll).drop();     // first time only!
db.getCollection(coll).ensureIndex( 
         { "docId":1, "version": 1 },
         { "unique" : true, "background" : true } );
if (db.getCollection("lastApplied").count({"_id":coll})==0) {
   prevTS=db.getSiblingDB("local").oplog.rs.findOne({"ns":namespace}).ts;
   db.getCollection("lastApplied").update( { "_id" : coll }, 
      {"$set" : { "ts" : prevTS } }, { "upsert" : true } );
} else {
   prevTS=db.getCollection("lastApplied").findOne({"_id":coll}).ts;
}
</pre>
The cursor (same as before):
<pre class="prettyprint lang-js">
var cursor=db.getSiblingDB("local").getCollection("oplog.rs"
       ).find({"ns":namespace,"ts":{"$gte":prevTS}}
       ).addOption(DBQuery.Option.oplogReplay
       ).addOption(DBQuery.Option.awaitData
       ).addOption(DBQuery.Option.tailable
       ).addOption(DBQuery.Option.noTimeout);
</pre>
The worker loop (slightly adjusted):
<pre class="prettyprint lang-java">
while (cursor.hasNext()) {
    var doc=cursor.next();
    var operation = (doc.op=="u") ? "update" : "insert";
    var id = (doc.op=="u") ? doc.o2._id : doc.o._id;
    var newDoc={ };
    print("TS: " + doc.ts + " for " + operation + " at " + new Date());
    if ( doc.op == "i" || 
            (doc.op == "u" && doc.o.hasOwnProperty("_id")) ) {
        for (i in doc.o) {
                if (i=='_id') newDoc.docId=doc["o"][i];
                else newDoc[i]=doc["o"][i];
        }
    } else if ( doc.op == "u" ) {
        /* create new doc out of old document and the sets and unsets */
        var prevVersion = { "docId" : doc.o2._id, 
                     "version" : doc.o["$set"]["version"]-1 };
        var prevDoc = db.getCollection(coll).findOne("prevVersion", {"_id":0});
        if (prevDoc == null) {
                     throw "Couldn't find previous version in archive! " + 
                                  tojson(prevVersion) + tojson(doc);
        }
        newDoc = prevDoc;
        if (doc.o.hasOwnProperty("$set")) {
            for (i in doc.o["$set"]) {
                newDoc[i]=doc.o["$set"][i];
            }
        } else if (doc.o.hasOwnProperty("$unset")) { 
            for (i in doc.o["$unset"]) {
                delete(newDoc[i]);
            }
        } else throw "Can only handle update with '_id', '$set' or '$unset' ";
    } else if (doc.op != "c") throw "Unexpected op! " + tojson(doc);

    var  result = db.getCollection(coll).insert(newDoc);
    if (result.nInserted==1) {
       print("Inserted doc " + 
                newDoc.docId + " version " + newDoc.version);
    } else {
       if (result.getWriteError().code==11000 ) {
           print("Doc " + newDoc.docId + " version " + 
                  newDoc.version + " already exists.");
       } else throw "Error inserting " + tojson(doc)  + 
                  " as " + tojson(newDoc)+ "Result " + tojson(result);
    }

    var res=db.getCollection("lastApplied").update(
             { "_id" : coll },
             { "$set" : {ts:doc.ts} },
             { "upsert" : true }
    );
    var prevTS=doc.ts;
    print("Set lastApplied to " + doc.ts);
}
</pre>

<p>It turns out that the loop will be slightly simpler because no matter what comes in, we will always do an insert into the full archive collection.</p>

<h5 id="test-it-out:897d8a06bfdbaecbf747ed08c519e7de">Test it out!</h5>

<p>Let&rsquo;s run this code and then compare for a single docId the operations in the oplog, and what we end up with in the archive collection:</p>

<p>The oplog entries:
<pre class="prettyprint lang-java">
db.getSiblingDB(&ldquo;local&rdquo;).getCollection(&ldquo;oplog.rs&rdquo;).find( {
            &ldquo;ns&rdquo; : namespace,
            &ldquo;$or&rdquo; : [ { &ldquo;o._id&rdquo; : 279 }, { &ldquo;o2._id&rdquo; : 279 } ]
         },
         { &ldquo;o&rdquo; : 1 } );
{ &ldquo;o&rdquo; : { &ldquo;_id&rdquo; : 279, &ldquo;version&rdquo; : 1, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo; } }
{ &ldquo;o&rdquo; : { &ldquo;$set&rdquo; : { &ldquo;version&rdquo; : 2 } } }
{ &ldquo;o&rdquo; : { &ldquo;$set&rdquo; : { &ldquo;version&rdquo; : 3, &ldquo;attrCounter&rdquo; : 1, &ldquo;attr9&rdquo; : 1, &ldquo;attrArray&rdquo; : [ &ldquo;xxx&rdquo; ] } } }
{ &ldquo;o&rdquo; : { &ldquo;_id&rdquo; : 279, &ldquo;version&rdquo; : 4, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo;, &ldquo;attrCounter&rdquo; : 1, &ldquo;attr9&rdquo; : 1, &ldquo;attrArray&rdquo; : [ &ldquo;xxx&rdquo; ], &ldquo;attrNew&rdquo; : &ldquo;abc&rdquo; } }
{ &ldquo;o&rdquo; : { &ldquo;_id&rdquo; : 279, &ldquo;version&rdquo; : 5, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo;, &ldquo;attrCounter&rdquo; : 2, &ldquo;attr9&rdquo; : 1, &ldquo;attrArray&rdquo; : [ &ldquo;xxx&rdquo; ], &ldquo;attrNewReplacement&rdquo; : &ldquo;abc&rdquo; } }
{ &ldquo;o&rdquo; : { &ldquo;$set&rdquo; : { &ldquo;version&rdquo; : 6, &ldquo;attrCounter&rdquo; : 3, &ldquo;attrArray&rdquo; : [ ] }, &ldquo;$unset&rdquo; : { &ldquo;attr9&rdquo; : true } } }
{ &ldquo;o&rdquo; : { &ldquo;_id&rdquo; : 279, &ldquo;version&rdquo; : 7 } }
{ &ldquo;o&rdquo; : { &ldquo;$set&rdquo; : { &ldquo;version&rdquo; : 8, &ldquo;attrCounter&rdquo; : 1, &ldquo;a&rdquo; : 1 } } }
{ &ldquo;o&rdquo; : { &ldquo;$set&rdquo; : { &ldquo;version&rdquo; : 9 }, &ldquo;$unset&rdquo; : { &ldquo;a&rdquo; : true, &ldquo;attrCounter&rdquo; : true } } }
</pre>
The archive collection contents (slightly formatted for readability):
<pre class="prettyprint lang-js">
db.docs_full_archive.find( {&ldquo;docId&rdquo;:279}, {&rdquo;_id&rdquo;:0} )
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 1, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo; }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 2, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo; }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 3, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo;,
   &ldquo;attrCounter&rdquo; : 1, &ldquo;attr9&rdquo; : 1, &ldquo;attrArray&rdquo; : [ &ldquo;xxx&rdquo; ] }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 4, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo;,
   &ldquo;attrCounter&rdquo; : 1, &ldquo;attr9&rdquo; : 1, &ldquo;attrArray&rdquo; : [ &ldquo;xxx&rdquo; ], &ldquo;attrNew&rdquo; : &ldquo;abc&rdquo; }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 5, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo;,
   &ldquo;attrCounter&rdquo; : 2, &ldquo;attr9&rdquo; : 1, &ldquo;attrArray&rdquo; : [ &ldquo;xxx&rdquo; ], &ldquo;attrNewReplacement&rdquo; : &ldquo;abc&rdquo; }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 6, &ldquo;attr7&rdquo; : &ldquo;xxx279&rdquo;,
   &ldquo;attrCounter&rdquo; : 3, &ldquo;attr9&rdquo; : 1, &ldquo;attrArray&rdquo; : [ ], &ldquo;attrNewReplacement&rdquo; : &ldquo;abc&rdquo; }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 7 }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 8, &ldquo;attrCounter&rdquo; : 1, &ldquo;a&rdquo; : 1 }
{ &ldquo;docId&rdquo; : 279, &ldquo;version&rdquo; : 9, &ldquo;attrCounter&rdquo; : 1, &ldquo;a&rdquo; : 1 }
</pre></p>

<p>There you have it, my preferred way to isolate an infrequently used collection and keep it updated based on every write action that happens in the main DB.  I hope you can see how this can be extended for many different pub/sub needs as you can adapt your code to watch for different types of events on different collections.</p>

<p>Hope you found this educational and keep those questions coming!</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:897d8a06bfdbaecbf747ed08c519e7de:1">Because the oplog must write change operations in <a href="http://docs.mongodb.org/manual/reference/glossary/#term-idempotent">&ldquo;idemponent&rdquo;</a> form, all update operators are transformed into their equivalent <code>$set</code> operations.
 <a class="footnote-return" href="#fnref:897d8a06bfdbaecbf747ed08c519e7de:1"><sup>[return]</sup></a></li>
</ol>
</div>

      </div>
    
  
    
      <div class="post">
        <h1 class="post-title"><a href="http://asya999.github.io/post/mergeshapes/">How to Merge Shapes with Aggregation Framework</a></h1>
        <span class="post-date">Sat, May  24, 2014</span>
        

<h3 id="question:03d96d95380719f8d4c3b2ad2c7c545e">Question:</h3>

<p>Consider two separate shapes of data like this in a single collection:</p>

<pre><code>{   type: &quot;A&quot;,
    level: 0,
    color: &quot;red&quot;,
    locale: &quot;USA&quot;
}
{   type: &quot;A&quot;,
    level: 1,
    color: &quot;blue&quot;
}
</code></pre>

<p>The goal is to present a merged shape to the application with the level n data overridden by level n+1 if level n+1 data exists for type A, starting with n = 0.  In other words, the app wants to see this shape:</p>

<pre><code>{   type: &quot;A&quot;,
    level: 1, 
    color: &quot;blue&quot;,
    locale: &quot;USA&quot;
}
</code></pre>

<p>If no level 1 data exists, the app would see the default (level 0) shape.   Think of it as a layered merge.</p>

<h3 id="answer:03d96d95380719f8d4c3b2ad2c7c545e">Answer:</h3>

<p>In the <a href="http://askasya.com/post/trackversions">previous &ldquo;AskAsya&rdquo; on tracking versions</a> we looked at different ways of tracking all versions of changing objects, and this happens to be a complex variant of that problem that we considered as &ldquo;schema 4&rdquo; - it&rsquo;s a possible approach to versioning, but it presents an interesting challenge returning the &ldquo;full&rdquo; current object back to the client.</p>

<h4 id="merging-different-shapes:03d96d95380719f8d4c3b2ad2c7c545e">Merging Different Shapes</h4>

<p>This problem would be easily solved with aggregation framework query, except for the problem that we need to know the names of all the keys/fields, and we might not  know all of the possible fields that could exist in our documents. Without this information, the only way we have of merging documents is using MapReduce, which is both more complex <em>and</em> slower.   I will show both solutions and I&rsquo;ll leave it up to you to determine which will be more performant in your scenario (or whether you want to switch to a different versioning schema).</p>

<h5 id="aggregation-framework:03d96d95380719f8d4c3b2ad2c7c545e">Aggregation Framework</h5>

<p>This will be the fastest way if you either have all possible attribute names that your documents could have, or get them via a scan of the entire collection (note that the latter immediately becomes stale, as new documents with new attributes could show up as soon as you start querying, but that&rsquo;s inherently an issue that always exists in any system that doesn&rsquo;t provide repeatable read isolation).</p>

<p>Get the possible attribute names (I&rsquo;m assuming <code>type</code> and <code>level</code> are your &lsquo;id&rsquo; and &lsquo;version&rsquo;):</p>

<pre><code>var att = { };
var attrs = [ ];
db.coll.find( {}, {_id:0, type:0, level:0} ).forEach( function(d) {
    for ( i in d)
         if ( !att.hasOwnProperty(i) ) {
             att[i]=1;
             attrs.push(i);
         }
} );                   
</code></pre>

<p>You now have an array <code>attrs</code> which holds all the strings representing different attributes in your collection.</p>

<p>We now programmatically generate stage for <code>$project</code> that turns each attribute into a subdocument with its level first and attribute itself second.</p>

<pre><code>proj1={$project:{type:1, level:1}};
attrs.forEach(function(attr) { 
    _a=&quot;_&quot;+attr; 
    a=&quot;$&quot;+attr;   
    proj1[&quot;$project&quot;][_a]={}; 
    proj1[&quot;$project&quot;][_a][&quot;l&quot;]={&quot;$cond&quot;:{}};
    proj1[&quot;$project&quot;][_a][&quot;l&quot;][&quot;$cond&quot;]={if:{&quot;$gt&quot;:[a,null]},then:&quot;$level&quot;,else:-1};
    proj1[&quot;$project&quot;][_a][attr]=a;
} );
</code></pre>

<p>Since levels are increasing, this set us to be able to <code>$group</code> using the <code>$max</code> operator to keep the highest level for each attribute.</p>

<pre><code>group={$group:{_id:&quot;$type&quot;,lvl:{$max:&quot;$level&quot;}}};
attrs.forEach(function(attr) { 
    a=&quot;$_&quot;+attr;
    group[&quot;$group&quot;][attr]={&quot;$max&quot;:a};
} )
</code></pre>

<p>The last <code>$project</code> just transforms the fields of our document back into the same key names they had before.</p>

<pre><code>proj2={$project:{_id:0,type:&quot;$_id&quot;, level:&quot;$lvl&quot;}}
attrs.forEach(function(attr) {
    a=&quot;$&quot;+attr;  
    proj2[&quot;$project&quot;][attr]=a+&quot;.&quot;+attr;
} )
</code></pre>

<p>We are now all set to run the aggregation with your programmatically generated stages:</p>

<pre><code>db.coll.aggregate( proj1, group, proj2 );
</code></pre>

<p>To recap,<code>proj1</code> is the stage where we converted every attribute into a subdocument which included &ldquo;level&rdquo; (first) and attribute value (second).  If a given attribute didn&rsquo;t exist in a document, it went in with level:-1 and value:null.</p>

<p><code>group</code> is where we grouped by <code>type</code> which is our <code>docId</code> and kept the highest (max) &ldquo;subdocument&rdquo; for each possible attribute.  This works because MongoDB allows you to compare any types (including BSON) and level:-1 is always going to &ldquo;lose&rdquo; to a higher level.</p>

<p><code>proj2</code> is when we turned all the fields into readable format, or at least format resembling our initial document.</p>

<p>This now returned to us the merged documents.</p>

<p>If we had original documents like these:</p>

<pre><code>&gt; db.coll.find({},{_id:0}).sort({type:1,level:1})
{ &quot;type&quot; : &quot;A&quot;, &quot;level&quot; : 0, &quot;color&quot; : &quot;red&quot;, &quot;locale&quot; : &quot;USA&quot; }
{ &quot;type&quot; : &quot;A&quot;, &quot;level&quot; : 1, &quot;color&quot; : &quot;blue&quot; }
{ &quot;type&quot; : &quot;A&quot;, &quot;level&quot; : 2, &quot;priority&quot; : 5 }
{ &quot;type&quot; : &quot;A&quot;, &quot;level&quot; : 3, &quot;locale&quot; : &quot;EMEA&quot; }
{ &quot;type&quot; : &quot;B&quot;, &quot;level&quot; : 0, &quot;priority&quot; : 1 }
{ &quot;type&quot; : &quot;B&quot;, &quot;level&quot; : 1, &quot;color&quot; : &quot;purple&quot;, &quot;locale&quot; : &quot;Canada&quot; }
{ &quot;type&quot; : &quot;B&quot;, &quot;level&quot; : 2, &quot;color&quot; : &quot;green&quot; }
{ &quot;type&quot; : &quot;B&quot;, &quot;level&quot; : 3, &quot;priority&quot; : 2, &quot;locale&quot; : &quot;USA&quot; }
{ &quot;type&quot; : &quot;B&quot;, &quot;level&quot; : 4, &quot;color&quot; : &quot;NONE&quot; }
</code></pre>

<p>We got back results that looked like this:</p>

<pre><code>&gt; db.coll.aggregate( proj1, group, proj2 );
{ &quot;color&quot; : &quot;NONE&quot;, &quot;locale&quot; : &quot;USA&quot;, &quot;priority&quot; : 2, &quot;type&quot; : &quot;B&quot;, &quot;level&quot; : 4 }
{ &quot;color&quot; : &quot;blue&quot;, &quot;locale&quot; : &quot;EMEA&quot;, &quot;priority&quot; : 5, &quot;type&quot; : &quot;A&quot;, &quot;level&quot; : 3 }
</code></pre>

<p>Note that this is not performant for filtering on attributes since we can&rsquo;t apply the filter until we have &ldquo;merged&rdquo; all the documents, and that means that indexes can&rsquo;t be used effectively.  While this aggregation may be a good exercise, unless you are saving this output into a new collection that you then index by attributes for querying, it won&rsquo;t be a good schema if you need very fast responses.</p>

<p>Here is MapReduce for the same functionality:</p>

<pre><code>map = function () {
    var doc=this;
    delete(doc._id);
    var level=this.level;
    delete(doc.level);
    var t=this.type;
    delete(doc.type);
    for (i in doc) {
         val={level:level};
         val[i]={ l:level, v:doc[i]};
         emit(t, val);
    }
}

reduce = function (key,values) {
  result={level:-1};
  values.forEach(function(val) {
    if (result.level&lt;val.level) result.level=val.level;
    var attr=null;
    for (a in val) if (a!=&quot;level&quot;) { attr=a; break; }
    if (!result.hasOwnProperty(attr) || result[attr].l&lt;=val[attr].l) {
          result[attr]=val[attr];

    }
  })
  return result;
}
</code></pre>

      </div>
    
  
</div>

<div class="pagination">
  
    <span class="pagination-item older">Older</span>
  

  
    <span class="pagination-item newer">Newer</span>
  
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  </body>
</html>

