<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ask Asya </title>
    <link>http://asya999.github.io/post/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2014</rights>
    <updated>2014-10-29 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>How to Find and Kill Slow Running Queries</title>
          <link>http://asya999.github.io/post/findkillslowqueries/</link>
          <pubDate>Wed, 29 Oct 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/findkillslowqueries/</guid>
          <description>

&lt;h3 id=&#34;question:18aaff1e2e315456fa60158351f7ffe2&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Is there a way that I can prevent slow queries in MongoDB?  I&amp;rsquo;d like to be able to set something on the server that will kill all queries running longer than a certain amount of time.&lt;/p&gt;

&lt;h3 id=&#34;answer:18aaff1e2e315456fa60158351f7ffe2&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;There are some options available on the &lt;strong&gt;client&lt;/strong&gt; side, for example &lt;a href=&#34;http://docs.mongodb.org/manual/reference/operator/meta/maxTimeMS/&#34;&gt;$maxTimeMS&lt;/a&gt; starting in 2.6 release.  This gives you a way to inject an option to your queries &lt;em&gt;before&lt;/em&gt; they get to the server that tells the server to kill this query if it takes longer than a certain amount of time.  However, this does not help with any query which already got to the server without having this option added to it.&lt;/p&gt;

&lt;p&gt;On the &lt;strong&gt;server&lt;/strong&gt; side, there is no global option, because it would impact all databases and all operations, even ones that the system needs to be long running for internal operation (for example tailing the oplog for replication).  In addition, it may be okay for some of your queries to be longer running by design but not others.&lt;/p&gt;

&lt;p&gt;The correct way to solve this would be to monitor currently running queries via a script and kill the ones that are both long running &lt;em&gt;and&lt;/em&gt; user/client initiated - you can then build in exceptions for queries that are long running by design, or have different thresholds for different queries/collections/etc.&lt;/p&gt;

&lt;p&gt;The way to implement this script is by using &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/db.currentOp/&#34;&gt;db.currentOp() command&lt;/a&gt; (in the shell) to see all currently running operations.  The field &amp;ldquo;secs_running&amp;rdquo; indicates how long the operation has been running.  Other fields visible to you will be the namespace (&amp;ldquo;ns&amp;rdquo;) whether the operation is &amp;ldquo;active&amp;rdquo;, whether it&amp;rsquo;s waiting on a lock and for how long.   The &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/db.currentOp/#examples&#34;&gt;docs contain some good examples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Be careful not to kill any long running operations that are not initiated by your application/client - it may be a necessary system operation, like chunk migration in a sharded cluster as just one example, replication threads would be another.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Further Thoughts on How to Track Versions with MongoDB</title>
          <link>http://asya999.github.io/post/revisitversions/</link>
          <pubDate>Sun, 07 Sep 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/revisitversions/</guid>
          <description>

&lt;h3 id=&#34;guest-post-by-paul-done-http-pauldone-blogspot-co-uk:a2980dc88478af7ab881c721f66e9b4d&#34;&gt;GUEST POST by &lt;a href=&#34;http://pauldone.blogspot.co.uk/&#34;&gt;Paul Done&lt;/a&gt; &lt;/h3&gt;

&lt;p&gt;In a &lt;a href=&#34;http://askasya.com/post/trackversions&#34;&gt;previous Ask Asya blog post&lt;/a&gt;,
Asya outlined various approaches for preserving historical versions of records for auditing purposes, whilst allowing current versions of records to be easily inserted and queried. Having found the post to be extremely useful for one of my projects, and following some further investigations of my own, I realised that two of the choices could be refined a little to be more optimal. Upon feeding back my findings, Asya graciously asked me to document them here, so here goes.&lt;/p&gt;

&lt;h4 id=&#34;revisit-of-choice-2-embed-versions-in-a-single-document:a2980dc88478af7ab881c721f66e9b4d&#34;&gt;Revisit of Choice 2  (Embed Versions in a Single Document)&lt;/h4&gt;

&lt;p&gt;The presented ‘compare-and-swap&amp;rsquo; example code, to generate a new version and update version history, is very effective at ensuring consistency of versions in a thread-safe manner. However, I felt that there was scope
to reduce the update latency which will be particularly high when a document has grown, with many previous versions embedded.&lt;/p&gt;

&lt;p&gt;For example, if a current document has tens of embedded previous versions, then projecting the whole document back to the client application, updating part of the document copy and then sending the whole document as an update to the database, will be slower than necessary. I prototyped a refactored version of the example code (shown below) which exhibits the same functional behaviour, but avoids projecting the entire document and uses an in-place update to push changes to the database.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t return all the old versions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    var doc = db.coll.findOne({&amp;quot;_id&amp;quot;: 174}, {&amp;quot;prev&amp;quot;: 0});  
    var currVersion = doc.current.v;
    var previous = doc.current;
    var current = {
          &amp;quot;v&amp;quot; : currVersion+1,
          &amp;quot;attr1&amp;quot;: doc.current.attr1,
          &amp;quot;attr2&amp;quot;: &amp;quot;YYYY&amp;quot;
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perform in-place update of changes only: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    var result = db.coll.update(
         { &amp;quot;_id&amp;quot; : 174, &amp;quot;current.v&amp;quot;: currVersion},
         { &amp;quot;$set&amp;quot; :  {&amp;quot;current&amp;quot;: current},
           &amp;quot;$push&amp;quot; :  {&amp;quot;prev&amp;quot;: previous}
         }
    );

    if (result.nModified != 1)  {
         print(&amp;quot;Someone got there first, replay flow to try again&amp;quot;);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result, even when the number of versions held in a document
increases over time, the update latency for adding a new version remains roughly constant.&lt;/p&gt;

&lt;h4 id=&#34;revisit-of-choice-3-separate-collection-for-previous-versions:a2980dc88478af7ab881c721f66e9b4d&#34;&gt;Revisit of Choice 3  (Separate Collection for Previous Versions)&lt;/h4&gt;

&lt;p&gt;The original post implies that this choice is technically challenging to implement, to ingest a new document version whilst maintaining consistency with previous versions, in the face of system failure.  However, I don&amp;rsquo;t feel it&amp;rsquo;s that bad, if the update flow is crafted carefully. If the order of writes is implemented as &amp;ldquo;write to previous collection before writing to current collection&amp;rdquo;, then in a failure scenario, there is potential for a duplicate record version but not a lost record version. Also, there are ways for subsequent querying code to gracefully deal with the duplicate.&lt;/p&gt;

&lt;p&gt;If the following three principles are acceptable to an application development team, then this is a viable versioning option and doesn&amp;rsquo;t have the implementation complexity of choice 5, for example:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;System failure could result in a duplicate version, but not a lost version.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Any application code that wants to query all or some versions of the same entity, is happy to issue two queries simultaneously, one against the current collection (to get the current version) and one against the previous collection (to get historic versions), and then merge the results. In cases where a duplicate has been introduced (but not yet cleaned up - see next point), the application code just has to detect that the latest version in the current collection also appears as a record in the previous collection. When this occurs, the application code just ignores the duplicate, when constructing its results. In my experience, most &amp;lsquo;normal&amp;rsquo; queries issued by an application, will just query the current collection and be interested in latest versions of entities only. Therefore this &amp;lsquo;double-query&amp;rsquo; mechanism is only needed for the parts of an application where historic version analysis is required.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The next time a new version of a document is pushed into the system, the old duplicate in the previous collection (if the duplicate exists) will become a genuine previous version. The current collection will contain the new version and the previous collection will only contain previous versions. As a result, there is no need for any background clean up code mechanisms to be put in place.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For clarity, I&amp;rsquo;ve included a JavaScript example of the full update flow below, which can be run from the Mongo shell.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    //
    // CREATE SAMPLE DATA
    //

    db.curr_coll.drop();
    db.prev_coll.drop();
    db.curr_coll.ensureIndex({&amp;quot;docId&amp;quot; : 1}, {&amp;quot;unique&amp;quot;: true});
    db.prev_coll.ensureIndex({&amp;quot;docId&amp;quot; : 1,  &amp;quot;v&amp;quot; :1}, {&amp;quot;unique&amp;quot;: true});
    db.curr_coll.insert([
         {&amp;quot;docId&amp;quot;: 174, &amp;quot;v&amp;quot;: 3, &amp;quot;attr1&amp;quot;: 184, &amp;quot;attr2&amp;quot;: &amp;quot;A-1&amp;quot;},
         {&amp;quot;docId&amp;quot;: 133, &amp;quot;v&amp;quot;: 3, &amp;quot;attr1&amp;quot;: 284, &amp;quot;attr2&amp;quot;: &amp;quot;B-1&amp;quot;}
    ]);

    db.prev_coll.insert([
         {&amp;quot;docId&amp;quot;: 174, &amp;quot;v&amp;quot;: 1, &amp;quot;attr1&amp;quot;: 165},
         {&amp;quot;docId&amp;quot;: 174, &amp;quot;v&amp;quot;: 2, &amp;quot;attr1&amp;quot;: 165, &amp;quot;attr2&amp;quot; : &amp;quot;A-1&amp;quot;},
         {&amp;quot;docId&amp;quot;: 133, &amp;quot;v&amp;quot;: 1, &amp;quot;attr1&amp;quot;: 265},
         {&amp;quot;docId&amp;quot;: 133, &amp;quot;v&amp;quot;: 2, &amp;quot;attr1&amp;quot;: 184, &amp;quot;attr2&amp;quot; : &amp;quot;B-1&amp;quot;}
    ]);

    //
    // EXAMPLE TEST RUN FLOW 
    //
    // UPSERT (NOT INSERT) IN CASE FAILURE OCCURED DURING PRIOR ATTEMPT.
    // THE PREV COLLECTION MAY ALREADY CONTAIN THE &#39;OLD&#39; CURRENT VERSION.
    // IF ALREADY PRESENT, THIS UPSERT WILL BE A &#39;NO-OP&#39;, RETURNING:
    //  nMatched: 1, nUpserted: 0, nModified: 0.

    var previous = db.curr_coll.findOne({&amp;quot;docId&amp;quot;: 174}, {_id: 0});
    var currVersion = previous.v;
    var result = db.prev_coll.update(
         {&amp;quot;docId&amp;quot; : previous.docId, &amp;quot;v&amp;quot;: previous.v },
         { &amp;quot;$set&amp;quot;: previous }
       , {&amp;quot;upsert&amp;quot;: true});

    // &amp;lt;-- STOP EXECUTION HERE ON A RUN TO SIMULATE FAILURE, THEN RUN
    //     FULL FLOW TO SHOW HOW THINGS WILL BE NATURALLY CLEANED-UP
    // UPDATE NEW VERSION IN CURR COLLECTION, USING THREAD-SAFE VERSION CHECK

    var current = {&amp;quot;v&amp;quot;: currVersion+1, &amp;quot;attr1&amp;quot;: previous.attr1, &amp;quot;attr2&amp;quot;:&amp;quot;YYYY&amp;quot;};
    var result = db.curr_coll.update({&amp;quot;docId&amp;quot;: 174, &amp;quot;v&amp;quot;: currVersion},
         {&amp;quot;$set&amp;quot;: current}
    );

    if (result.nModified != 1) {
         print(&amp;quot;Someone got there first, replay flow to try again&amp;quot;);
    }

    //
    // EXAMPLE VERSION HISTORY QUERY CODE
    //

    // BUILD LIST OF ALL VERSIONS OF ENTITY, STARTING WITH CURRENT VERSION

    var fullVersionHistory = [];
    var latest = db.curr_coll.findOne({&amp;quot;docId&amp;quot;: 174}, {_id: 0});
    var latestVersion = latest.v;
    fullVersionHistory.push(latest);

    // QUERY ALL PREVIOUS VERSIONS (EXCLUDES DUPLICATE CURRENT VERSION IF EXISTS)
    var previousVersionsCursor = db.prev_coll.find({
         &amp;quot;$and&amp;quot;: [
              {&amp;quot;docId&amp;quot;: 174},
              {&amp;quot;v&amp;quot;: {&amp;quot;$ne&amp;quot;: latestVersion}}
         ]
    }, {_id: 0}).sort({v: -1});

    // ADD ALL THESE PREVIOUS VERSIONS TO THE LIST
    previousVersionsCursor.forEach(function(doc) {
          fullVersionHistory.push(doc);
    });

    // DISPLAY ALL VERSIONS OF AN ENTITY (NO DUPLICATES ARE PRESENT)
    printjson(fullVersionHistory);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result of this approach, it is easy to query current versions of entities, easy to query the full version history of a given entity and easy to update an entity with a new version.&lt;/p&gt;

&lt;h3 id=&#34;in-summary:a2980dc88478af7ab881c721f66e9b4d&#34;&gt;In Summary&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ve taken the liberty of providing a modified version of Asya&amp;rsquo;s summary table below, to expand out the criteria that may be relevant when choosing a versioning strategy for a given use case. My version of the table also reflects the improved results for choices 2 and 3, on the back of what has been discussed in this blog post.&lt;/p&gt;

&lt;p&gt;Updated Table of Tradeoffs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://dl.dropboxusercontent.com/u/35111849/table.png&#34; alt=&#34;Updated Table of Tradeoffs&#34; /&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Social Status Feed in MongoDB</title>
          <link>http://asya999.github.io/post/socialstatusfeed/</link>
          <pubDate>Wed, 27 Aug 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/socialstatusfeed/</guid>
          <description>

&lt;h3 id=&#34;socialite:36d9d7ac96775ede158c079032b00e59&#34;&gt;Socialite&lt;/h3&gt;

&lt;p&gt;At MongoDBWorld, my colleague Darren Wood and I gave &lt;a href=&#34;http://www.mongodb.com/search/google/socialite?query=socialite&amp;amp;cx=017213726194841070573%3Ak6mpwzohlje&amp;amp;cof=FORID%3A9&amp;amp;sitesearch=&#34;&gt;three back-to-back presentations&lt;/a&gt; about an &lt;a href=&#34;http://github.com/10gen-labs/socialite&#34;&gt;open source project called Socialite&lt;/a&gt; which is a reference architecture implementation of a social status feed service.  Darren was the one who wrote the bulk of the source code and I installed and ran extensive performance tests in different configurations to determine how the different schema and indexing options scale and to get an understanding of the resources required for various sizes and distributions of workloads.&lt;/p&gt;

&lt;p&gt;The recordings and slides are &lt;a href=&#34;http://www.mongodb.com/search/google/socialite?query=socialite&amp;amp;cx=017213726194841070573%3Ak6mpwzohlje&amp;amp;cof=FORID%3A9&amp;amp;sitesearch=&#34;&gt;now available on MongoDB website&lt;/a&gt;, if you want to jump in and watch, but since we had to race through the material,  I&amp;rsquo;m going to blog about some of the more interesting questions it raised, mainly about schema design, indexing and sharding choices and how to benchmark a large, complex application.&lt;/p&gt;

&lt;p&gt;There were three talks because there was a large amount of material and because there are several rather complex orthogonal topics when considering a social status feed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How will you store the content long term&lt;/li&gt;
&lt;li&gt;How will you store the user graph&lt;/li&gt;
&lt;li&gt;How will you serve up the status feed for a particular user when they log in&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The last one is probably most interesting in that it has the most possible approaches, though as it turns out, some have very big downsides and would only be appropriate to pretty small systems.  User graph is fascinating because of its relevance to so many different domains beyond social networks of friends.  And performance considerations are complex and interdependent among all of them.   For each of the three talks we had two parts - Darren discussed possible schema designs, indexing considerations and if appropriate sharding implications, and I walked through the actual testing I did and whether each option held up as expected.&lt;/p&gt;

&lt;p&gt;Unfortunately even across three sessions we were quite time limited, so all the various bits of material we have that didn&amp;rsquo;t make it into these presentations will end up in one of several spots:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;here&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.mongodb.org&#34;&gt;mongodb.org blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;advanced schema design class at &lt;a href=&#34;https://univerity.mongodb.com&#34;&gt;MongoDB University&lt;/a&gt; (coming soon!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I get a lot of questions about schema design, and social data is both popular and very doable in MongoDB but the naive approach is usually bound to meet with failure, so the schema needs to be carefully considered with an eye towards the following two most important considerations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;enduser latency&lt;/li&gt;
&lt;li&gt;linear scaling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we said during the presentations, for every single decision, we had to consider as the most important goals keeping the user&amp;rsquo;s first read latency as low and constant as possible (or else they would leave and go somewhere else) and our ability to scale any design we had linearly with scaling.  That meant that every single workload had to be scalable or partitionable in a way that would isolate the workload to a subset of data.&lt;/p&gt;

&lt;p&gt;Over the next few months as I write up different parts of the system, and consider the schema, indexes and possible shard data distribution, you will see me return to these two litmus tests again and again.   In order to have highest chance of success at large scale any option that hinders one of these goals should be out of the running.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Best Versions with MongoDB</title>
          <link>http://asya999.github.io/post/bestversion/</link>
          <pubDate>Fri, 30 May 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/bestversion/</guid>
          <description>

&lt;h3 id=&#34;question:897d8a06bfdbaecbf747ed08c519e7de&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Recall &lt;a href=&#34;http://askasya.com/post/trackversions&#34;&gt;our previous discussion&lt;/a&gt; about ways to  recreate older version of a document that ever existed in a particular collection.&lt;/p&gt;

&lt;p&gt;The goal was to preserve every state for each object, but to respond to queries with the &amp;ldquo;current&amp;rdquo; or &amp;ldquo;latest&amp;rdquo; version.   We had a requirement to be able to have an infrequent audit to review all or some previous versions of the document.&lt;/p&gt;

&lt;h3 id=&#34;answer:897d8a06bfdbaecbf747ed08c519e7de&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;I had suggested at the time that there was a different way to achieve this that I liked better than the discussed methods and I&amp;rsquo;m going to describe it now.&lt;/p&gt;

&lt;h4 id=&#34;previous-discussion-summary:897d8a06bfdbaecbf747ed08c519e7de&#34;&gt;Previous Discussion Summary:&lt;/h4&gt;

&lt;p&gt;Up to this point, we considered keeping versions of the same document within one MongoDB document, in separate documents within the same collection, or by &amp;ldquo;archiving off&amp;rdquo; older versions of the document into a separate collection.&lt;/p&gt;

&lt;p&gt;We looked at the trade-offs and decided that the important factors were our ability to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;return or match only the current document(s)&lt;/li&gt;
&lt;li&gt;generate new version number to &amp;ldquo;update&amp;rdquo; existing and add new attributes

&lt;ul&gt;
&lt;li&gt;including recovering from failure in the middle of a set of operations (if there is more than one)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;where-we-left-off:897d8a06bfdbaecbf747ed08c519e7de&#34;&gt;Where we left off:&lt;/h5&gt;

&lt;p&gt;Here&amp;rsquo;s a table that shows for each schema choice that we considered how well we can handle the reads, writes and if an update has to make more than one write, how easy it is to recover or to be in a relatively &amp;ldquo;safe&amp;rdquo; state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;         Schema         | Fetch 1       | Fetch Many  | | Update        | Recover if fail 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-|-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;
1) New doc for each         | Easy,Fast     | Not easy,Slow | | Medium | N/A
1a) New doc with &amp;ldquo;current&amp;rdquo;  | Easy,Fast     | Easy,Fast | | Medium | Hard
2) Embedded in single doc   | Easy,Fastest  | Easy,Fastest | | Medium | N/A
3) Sep Collection for prev. |  Easy,Fastest | Easy,Fastest  | | Medium |  Medium Hard
4) Deltas only in new doc   | Hard,Slow     | Hard,Slow | | Medium | N/A
?) TBD                      |  Easy,Fastest | Easy,Fastest  | | Easy,Fastest |  N/A&lt;/p&gt;

&lt;p&gt;&amp;ldquo;N/A&amp;rdquo; for recovery means there is no inconsistent state possible - if we only have to make one write to create/add a new version, we are safe from any inconsistency.  So &amp;ldquo;N/A&amp;rdquo; is the &amp;ldquo;easiest&amp;rdquo; value there.&lt;/p&gt;

&lt;p&gt;What we want is something that makes all our tasks easy, and does not have any performance issues nor consistency problems.   For creating this solution, we will pick and choose the best parts of the previously considered schema.&lt;/p&gt;

&lt;p&gt;No doubt you noticed that fetching one or many is fastest and simplest when we keep the old versioned documents out of our &amp;ldquo;current&amp;rdquo; collection.  This makes our queries whether for one or all latest versions fast and they can use indexes whether you&amp;rsquo;re querying, updating or aggregating.&lt;/p&gt;

&lt;p&gt;How do we get fast updates that keep the current document current but save the previous version somewhere else?  We know that we don&amp;rsquo;t have multi-statement transaction in MongoDB so we can&amp;rsquo;t ensure that a regular  update of one document and an insert of another document are atomic.  However, there &lt;em&gt;is&lt;/em&gt; something that&amp;rsquo;s always updated atomically along with &lt;em&gt;every&lt;/em&gt; write that happens in your collection, and that is the &amp;ldquo;Oplog&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;the-oplog:897d8a06bfdbaecbf747ed08c519e7de&#34;&gt;The Oplog&lt;/h4&gt;

&lt;p&gt;The oplog (full name: &amp;lsquo;oplog.rs&amp;rsquo; collection in &amp;lsquo;local&amp;rsquo; database) is a special collection that&amp;rsquo;s used by the replication mechanism.  Every single write operation is persisted into the oplog atomically with being applied to the data files, indexes and the journal.  You can read more about the oplog in &lt;a href=&#34;http://docs.mongodb.org/manual/core/replica-set-oplog/&#34;&gt;the docs&lt;/a&gt;, but what I&amp;rsquo;m going to show you is what it looks like in the oplog when an insert or update happens, and how we can use that for our own purposes.&lt;/p&gt;

&lt;p&gt;If I perform this insert into my collection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.docs.insert(
           {&amp;quot;_id&amp;quot;:ObjectId(&amp;quot;5387edd9ba5871da01786f85&amp;quot;), 
            &amp;quot;docId&amp;quot;:174, &amp;quot;version&amp;quot;:1, &amp;quot;attr1&amp;quot;:165});
WriteResult({ &amp;quot;nInserted&amp;quot; : 1 })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;what I will see in the oplog will look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.getSiblingDB(&amp;quot;local&amp;quot;).oplog.rs.find().sort({&amp;quot;$natural&amp;quot;:-1}).limit(-1).pretty();
{
         &amp;quot;ts&amp;quot; : Timestamp(1401417307, 1),
          &amp;quot;h&amp;quot; : NumberLong(&amp;quot;-1030581192915920539&amp;quot;), 
          &amp;quot;v&amp;quot; : 2, 
          &amp;quot;op&amp;quot; : &amp;quot;i&amp;quot;, 
          &amp;quot;ns&amp;quot; : &amp;quot;blog.docs&amp;quot;, 
          &amp;quot;o&amp;quot; : { 
                        &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;5387edd9ba5871da01786f85&amp;quot;), 
                        &amp;quot;docId&amp;quot; : 174, 
                        &amp;quot;version&amp;quot; : 1, 
                        &amp;quot;attr1&amp;quot; : 165 
          } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I perform this update:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.docs.update( 
               { &amp;quot;docId&amp;quot; : 174 }, 
               { &amp;quot;$inc&amp;quot;:{&amp;quot;version&amp;quot;:1}, &amp;quot;$set&amp;quot;:{ &amp;quot;attr2&amp;quot;: &amp;quot;A-1&amp;quot; }  } 
    );
WriteResult({ &amp;quot;nMatched&amp;quot; : 1, &amp;quot;nUpserted&amp;quot; : 0, &amp;quot;nModified&amp;quot; : 1 })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;what I get in the oplog is this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
        &amp;quot;ts&amp;quot; : Timestamp(1401417535, 1),
        &amp;quot;h&amp;quot; : NumberLong(&amp;quot;2381950322402503088&amp;quot;),
         &amp;quot;v&amp;quot; : 2,
         &amp;quot;op&amp;quot; : &amp;quot;u&amp;quot;,
         &amp;quot;ns&amp;quot; : &amp;quot;blog.docs&amp;quot;,
         &amp;quot;o2&amp;quot; : {
                 &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;5387edd9ba5871da01786f85&amp;quot;)
         },
         &amp;quot;o&amp;quot; : {
                &amp;quot;$set&amp;quot; : {
                        &amp;quot;version&amp;quot; : 2,
                        &amp;quot;attr2&amp;quot; : &amp;quot;A-1&amp;quot;
                }
         }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It turns out that with versioned documents, I wouldn&amp;rsquo;t actually ever do an insert, but rather I would just always do an update, with an upsert option, that way I don&amp;rsquo;t need to test if a document with this &lt;code&gt;docId&lt;/code&gt; already exists.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.docs.update( 
     { &amp;quot;docId&amp;quot; : 175 }, 
     { &amp;quot;$inc&amp;quot;:{&amp;quot;version&amp;quot;:1}, &amp;quot;$set&amp;quot;:{ &amp;quot;attr1&amp;quot;: 999 }  }, 
     { &amp;quot;upsert&amp;quot; : true } 
);
WriteResult({
    &amp;quot;nMatched&amp;quot; : 0,
    &amp;quot;nUpserted&amp;quot; : 1,
    &amp;quot;nModified&amp;quot; : 0,
    &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;5387eff7a08472e30040b4bc&amp;quot;)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see what the oplog entry for this upsert looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ 
    &amp;quot;ts&amp;quot; : Timestamp(1401417719, 1), 
    &amp;quot;h&amp;quot; : NumberLong(&amp;quot;2031090002854356513&amp;quot;), 
    &amp;quot;v&amp;quot; : 2, 
    &amp;quot;op&amp;quot; : &amp;quot;i&amp;quot;, 
    &amp;quot;ns&amp;quot; : &amp;quot;blog.docs&amp;quot;, 
    &amp;quot;o&amp;quot; : {
         &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;5387eff7a08472e30040b4bc&amp;quot;), 
         &amp;quot;docId&amp;quot; : 175, 
         &amp;quot;version&amp;quot; : 1, 
         &amp;quot;attr1&amp;quot; : 999 
    } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like the oplog entry reflects the actual operation that was performed, &lt;strong&gt;not&lt;/strong&gt; the operation that I specified.  I asked for an update - when it&amp;rsquo;s an update, the oplog will show it as an update, when it&amp;rsquo;s turned into an upsert, the oplog will show it as an insert.  When it was an update, I had asked it to &amp;ldquo;increment&amp;rdquo; but what it put in the oplog was what the actual value saved was.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:897d8a06bfdbaecbf747ed08c519e7de:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:897d8a06bfdbaecbf747ed08c519e7de:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m sure most of you see where I&amp;rsquo;m going with this.  Rather than fumbling with creating and updating documents in the &amp;ldquo;previous versions&amp;rdquo; collection when we perform an update to a document, we can do it asynchronously, the way MongoDB secondaries do it.&lt;/p&gt;

&lt;p&gt;You may think it&amp;rsquo;s not easy, but it turns out that there are lots of &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/cursor.addOption/&#34;&gt;helpers&lt;/a&gt; for dealing with &lt;a href=&#34;http://docs.mongodb.org/manual/core/capped-collections/&#34;&gt;capped collections&lt;/a&gt; (which is what the oplog is).  One of the most useful things you can do is &amp;ldquo;tail the oplog&amp;rdquo;.  This is the same mechanism that secondaries use to find out when new writes happen on the primary: they tail the oplog the same way you can do &lt;code&gt;tail -f logfile.txt&lt;/code&gt; command - this will show you the last part of the file, but rather than giving you back the prompt when it&amp;rsquo;s done, it will just sit there and wait.  When more things are written to the file, &lt;code&gt;tail -f&lt;/code&gt; will echo them to the screen.   This is exactly how it works with &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/cursor.addOption/#example&#34;&gt;tailable cursors&lt;/a&gt; on capped collections.  If you specify the &lt;a href=&#34;http://docs.mongodb.org/meta-driver/latest/legacy/mongodb-wire-protocol/?pageVersion=106#op-query&#34;&gt;right special options&lt;/a&gt;, you can get data back, but when there is no more data, instead of timing out and having to re-query, you will just sit there and wait till more data shows up.&lt;/p&gt;

&lt;p&gt;Here is a little demo.  The code and explanations are after the video, so feel free to browse ahead before watching, or you can watch first and read the explanations after.&lt;/p&gt;

&lt;h5 id=&#34;tailing-the-oplog-to-maintain-a-copy-of-a-collection-elsewhere:897d8a06bfdbaecbf747ed08c519e7de&#34;&gt;Tailing the oplog to maintain a copy of a collection elsewhere&lt;/h5&gt;

&lt;p&gt;For our first example, we&amp;rsquo;ll do something simple - we will watch the oplog for changes to a specific collection, and then we will apply those changes to our own copy of the collection - we will call our collection something else.  Our example stores the copy in the same database, but of course, it could be anywhere else, including in a completely different replica set or standalone server.&lt;/p&gt;

&lt;pre&gt;
&lt;script src=&#34;https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js&#34;&gt;&lt;/script&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;//www.youtube.com/embed/U-MVlb0cRHU&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/pre&gt;

&lt;p&gt;Code for set-up of variables with comments:&lt;/p&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
/* shell likes to ”page” things for us every 20 documents,  *
 * so we will increase that value for this demo */
DBQuery.shellBatchSize=200;
var namespace=&#34;blog.docs&#34;; /* the collection to watch */ 
var coll=&#34;docs_archive&#34;;  /* copy collection name */

db.getCollection(coll).drop();  /* only the very first time :) */
db.lastApplied.drop();
/* find the last timestamp we applied: this is where we restart */
var prevTS=(db.getCollection(&#34;lastApplied&#34;).count({&#34;_id&#34;:coll})==0) ?
     db.getSiblingDB(&#34;local&#34;).oplog.rs.findOne({&#34;ns&#34;:namespace}).ts :
     db.getCollection(&#34;lastApplied&#34;).findOne({&#34;_id&#34;:coll}).ts;
/* initialize or update lastApplied for this collection */
db.getCollection(&#34;lastApplied&#34;).update(
                { &#34;_id&#34; : coll }, 
                { &#34;$set : { &#34;ts&#34; : prevTS } },
                { &#34;upsert&#34; : true }
);
&lt;/pre&gt;

&lt;p&gt;Code for setting up the cursor using &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/cursor.addOption/#flags&#34;&gt;appropriate options&lt;/a&gt; allows us to find the right spot in the oplog quickly, and makes our cursor tail the data, asking server to forgo the usual cursor timeout based on inactivity:&lt;/p&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
/* set up the cursor with appropriate filter and options */
var cursor=db.getSiblingDB(&#34;local&#34;).getCollection(&#34;oplog.rs&#34;
    ).find({&#34;ns&#34;:namespace,&#34;ts&#34;:{&#34;$gte&#34;:prevTS}}
    ).addOption(DBQuery.Option.oplogReplay
    ).addOption(DBQuery.Option.awaitData
    ).addOption(DBQuery.Option.tailable
    ).addOption(DBQuery.Option.noTimeout);
&lt;/pre&gt;

&lt;p&gt;Code running on the right-hand-side (blue screen) which loops and inserts or updates the watched collection every second:&lt;/p&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
for ( docId = 270; docId &lt; 290; docId++ ) {
    print(&#34;waiting one second...&#34;);
    sleep(1000);
    printjson(db.docs.update(
          { &#34;_id&#34;: docId },
          { &#34;$inc&#34; : {&#34;version&#34;:1}, &#34;$set&#34;:{&#34;attr7&#34;:&#34;xxx&#34;+docId } },
          { &#34;upsert&#34; : true }));
}
&lt;/pre&gt;

&lt;p&gt;Code that fetches documents from the tailable cursor and applies appropriate changes to our &amp;ldquo;copy&amp;rdquo; collection:&lt;/p&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
while (cursor.hasNext()) {
       
    var doc=cursor.next();
    var operation = (doc.op==&#34;u&#34;) ? &#34;update&#34; : &#34;insert&#34;;
    print(&#34;TS: &#34; + doc.ts + &#34; for &#34; + operation + &#34; at &#34; + new Date());
    
    if ( doc.op == &#34;i&#34;) {  /* originally an upsert */
        result = db.getCollection(coll).save(doc.o);
        if (result.nUpserted==1) print(&#34;Inserted doc _id &#34; + doc.o._id);
        else {
           if (result.nMatched==1 ) {
             if ( result.nModified==0) {
                 print(&#34;Doc &#34; + doc.o._id + &#34; exists.&#34;);
             } else  print(&#34;Doc &#34; + doc.o._id + &#34; may have been newer&#34;);
           } else {
             throw &#34;Insert error &#34; + tojson(doc)  + &#34; &#34; + tojson(result);
           }
        }
    } else if ( doc.op == &#34;u&#34; ) { /* originally an update */
        result = db.getCollection(coll).update(doc.o2, doc.o);
        if (result.nModified ==1) print(&#34;Updated doc _id &#34; + doc.o2._id);
        else if (result.nMatched==1 &amp;&amp; result.nModified==0) print(
                   &#34;Already updated doc _id &#34; + doc.o2._id);
        else  throw &#34;No update for &#34; + tojson(doc) + &#34; &#34; + tojson(result);
    } else if (doc.op != &#34;c&#34;) throw &#34;Unexpected op! &#34; + tojson(doc);
    
    res=db.getCollection(&#34;lastApplied&#34;).update(
            { &#34;_id&#34; : coll },
            { &#34;$set&#34; : { &#34;ts&#34; : doc.ts } }
    );

    if (res.nModified==1) print(&#34;Updated lastApplied to &#34; + doc.ts);
    else print(&#34;Repeated last applied op&#34;);

    prevTS=doc.ts; /* save in case we need to refetch from the oplog */
}
&lt;/pre&gt;
Of course this code does minimal error checking and it&#39;s not set up to automatically restart if it loses connection to the primary, or the primary changes in the replica set.  This is because here we are reading from a local oplog when in real life you may be fetching data from another server or cluster entirely.  Even so, about 15 lines of code there are for error checking and information printing, so the actual &#34;work&#34; we do is quite simple.  

##### Creating a full archive from tailing the oplog #####

Now that we know how to replay original documents to maintain an indentical &#34;copy&#34; collection, let&#39;s see what we have to do differently when we want to insert a new version of the document without losing any of the old versions.

For simplicity, I put the docId in the example collection into the `_id` field, so I will need to structure the full archive collection schema differently, since it cannot have multiple documents with the same `_id`.[^2]  For simplicity, I will let MongoDB generate the `_id` and I will use the combination of docId and version with a unique index on them to prevent duplicate versions.  I could achieve something similar by using the combination of original `_id` (which is the docId) and `version` fields as a compound `_id` field but then I would need to do more complicated transformations on the oplog entry.  I always choose the method that is  simpler.

Now when we get an insert operation in the oplog, we should be able to insert the document the same way we were doing it before, except we want to move `_id` value into `docId` field.  If the save fails to insert a new document because of a duplicate constraint violation, then we already have that docId and version - we would expect that when we are replaying the same entry in the oplog more than once.   

If we get an update, it can be one of two kinds - it can be one that sets or unsets specific fields, or it can be the kind that overwrites the entire document with a new document (with the same `_id` of course).  The latter case can be handled by the same code we have for the insert, with an appropriate transformation of the document.   If it&#39;s the `$set` and `$unset` kind, then we have to fetch the previous version of this document and apply the changes to it before inserting it as a document representing a new version.[^3]

Here is our code, with comments:

The setup:
&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
var coll=&#34;docs_full_archive&#34;;  // different collection
db.getCollection(coll).drop();     // first time only!
db.getCollection(coll).ensureIndex( 
         { &#34;docId&#34;:1, &#34;version&#34;: 1 },
         { &#34;unique&#34; : true, &#34;background&#34; : true } );
if (db.getCollection(&#34;lastApplied&#34;).count({&#34;_id&#34;:coll})==0) {
   prevTS=db.getSiblingDB(&#34;local&#34;).oplog.rs.findOne({&#34;ns&#34;:namespace}).ts;
   db.getCollection(&#34;lastApplied&#34;).update( { &#34;_id&#34; : coll }, 
      {&#34;$set&#34; : { &#34;ts&#34; : prevTS } }, { &#34;upsert&#34; : true } );
} else {
   prevTS=db.getCollection(&#34;lastApplied&#34;).findOne({&#34;_id&#34;:coll}).ts;
}
&lt;/pre&gt;
The cursor (same as before):
&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
var cursor=db.getSiblingDB(&#34;local&#34;).getCollection(&#34;oplog.rs&#34;
       ).find({&#34;ns&#34;:namespace,&#34;ts&#34;:{&#34;$gte&#34;:prevTS}}
       ).addOption(DBQuery.Option.oplogReplay
       ).addOption(DBQuery.Option.awaitData
       ).addOption(DBQuery.Option.tailable
       ).addOption(DBQuery.Option.noTimeout);
&lt;/pre&gt;
The worker loop (slightly adjusted):
&lt;pre class=&#34;prettyprint lang-java&#34;&gt;
while (cursor.hasNext()) {
    var doc=cursor.next();
    var operation = (doc.op==&#34;u&#34;) ? &#34;update&#34; : &#34;insert&#34;;
    var id = (doc.op==&#34;u&#34;) ? doc.o2._id : doc.o._id;
    var newDoc={ };
    print(&#34;TS: &#34; + doc.ts + &#34; for &#34; + operation + &#34; at &#34; + new Date());
    if ( doc.op == &#34;i&#34; || 
            (doc.op == &#34;u&#34; &amp;&amp; doc.o.hasOwnProperty(&#34;_id&#34;)) ) {
        for (i in doc.o) {
                if (i==&#39;_id&#39;) newDoc.docId=doc[&#34;o&#34;][i];
                else newDoc[i]=doc[&#34;o&#34;][i];
        }
    } else if ( doc.op == &#34;u&#34; ) {
        /* create new doc out of old document and the sets and unsets */
        var prevVersion = { &#34;docId&#34; : doc.o2._id, 
                     &#34;version&#34; : doc.o[&#34;$set&#34;][&#34;version&#34;]-1 };
        var prevDoc = db.getCollection(coll).findOne(&#34;prevVersion&#34;, {&#34;_id&#34;:0});
        if (prevDoc == null) {
                     throw &#34;Couldn&#39;t find previous version in archive! &#34; + 
                                  tojson(prevVersion) + tojson(doc);
        }
        newDoc = prevDoc;
        if (doc.o.hasOwnProperty(&#34;$set&#34;)) {
            for (i in doc.o[&#34;$set&#34;]) {
                newDoc[i]=doc.o[&#34;$set&#34;][i];
            }
        } else if (doc.o.hasOwnProperty(&#34;$unset&#34;)) { 
            for (i in doc.o[&#34;$unset&#34;]) {
                delete(newDoc[i]);
            }
        } else throw &#34;Can only handle update with &#39;_id&#39;, &#39;$set&#39; or &#39;$unset&#39; &#34;;
    } else if (doc.op != &#34;c&#34;) throw &#34;Unexpected op! &#34; + tojson(doc);

    var  result = db.getCollection(coll).insert(newDoc);
    if (result.nInserted==1) {
       print(&#34;Inserted doc &#34; + 
                newDoc.docId + &#34; version &#34; + newDoc.version);
    } else {
       if (result.getWriteError().code==11000 ) {
           print(&#34;Doc &#34; + newDoc.docId + &#34; version &#34; + 
                  newDoc.version + &#34; already exists.&#34;);
       } else throw &#34;Error inserting &#34; + tojson(doc)  + 
                  &#34; as &#34; + tojson(newDoc)+ &#34;Result &#34; + tojson(result);
    }

    var res=db.getCollection(&#34;lastApplied&#34;).update(
             { &#34;_id&#34; : coll },
             { &#34;$set&#34; : {ts:doc.ts} },
             { &#34;upsert&#34; : true }
    );
    var prevTS=doc.ts;
    print(&#34;Set lastApplied to &#34; + doc.ts);
}
&lt;/pre&gt;

&lt;p&gt;It turns out that the loop will be slightly simpler because no matter what comes in, we will always do an insert into the full archive collection.&lt;/p&gt;

&lt;h5 id=&#34;test-it-out:897d8a06bfdbaecbf747ed08c519e7de&#34;&gt;Test it out!&lt;/h5&gt;

&lt;p&gt;Let&amp;rsquo;s run this code and then compare for a single docId the operations in the oplog, and what we end up with in the archive collection:&lt;/p&gt;

&lt;p&gt;The oplog entries:
&lt;pre class=&#34;prettyprint lang-java&#34;&gt;
db.getSiblingDB(&amp;ldquo;local&amp;rdquo;).getCollection(&amp;ldquo;oplog.rs&amp;rdquo;).find( {
            &amp;ldquo;ns&amp;rdquo; : namespace,
            &amp;ldquo;$or&amp;rdquo; : [ { &amp;ldquo;o._id&amp;rdquo; : 279 }, { &amp;ldquo;o2._id&amp;rdquo; : 279 } ]
         },
         { &amp;ldquo;o&amp;rdquo; : 1 } );
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;_id&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 1, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo; } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;$set&amp;rdquo; : { &amp;ldquo;version&amp;rdquo; : 2 } } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;$set&amp;rdquo; : { &amp;ldquo;version&amp;rdquo; : 3, &amp;ldquo;attrCounter&amp;rdquo; : 1, &amp;ldquo;attr9&amp;rdquo; : 1, &amp;ldquo;attrArray&amp;rdquo; : [ &amp;ldquo;xxx&amp;rdquo; ] } } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;_id&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 4, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo;, &amp;ldquo;attrCounter&amp;rdquo; : 1, &amp;ldquo;attr9&amp;rdquo; : 1, &amp;ldquo;attrArray&amp;rdquo; : [ &amp;ldquo;xxx&amp;rdquo; ], &amp;ldquo;attrNew&amp;rdquo; : &amp;ldquo;abc&amp;rdquo; } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;_id&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 5, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo;, &amp;ldquo;attrCounter&amp;rdquo; : 2, &amp;ldquo;attr9&amp;rdquo; : 1, &amp;ldquo;attrArray&amp;rdquo; : [ &amp;ldquo;xxx&amp;rdquo; ], &amp;ldquo;attrNewReplacement&amp;rdquo; : &amp;ldquo;abc&amp;rdquo; } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;$set&amp;rdquo; : { &amp;ldquo;version&amp;rdquo; : 6, &amp;ldquo;attrCounter&amp;rdquo; : 3, &amp;ldquo;attrArray&amp;rdquo; : [ ] }, &amp;ldquo;$unset&amp;rdquo; : { &amp;ldquo;attr9&amp;rdquo; : true } } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;_id&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 7 } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;$set&amp;rdquo; : { &amp;ldquo;version&amp;rdquo; : 8, &amp;ldquo;attrCounter&amp;rdquo; : 1, &amp;ldquo;a&amp;rdquo; : 1 } } }
{ &amp;ldquo;o&amp;rdquo; : { &amp;ldquo;$set&amp;rdquo; : { &amp;ldquo;version&amp;rdquo; : 9 }, &amp;ldquo;$unset&amp;rdquo; : { &amp;ldquo;a&amp;rdquo; : true, &amp;ldquo;attrCounter&amp;rdquo; : true } } }
&lt;/pre&gt;
The archive collection contents (slightly formatted for readability):
&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
db.docs_full_archive.find( {&amp;ldquo;docId&amp;rdquo;:279}, {&amp;rdquo;_id&amp;rdquo;:0} )
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 1, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo; }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 2, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo; }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 3, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo;,
   &amp;ldquo;attrCounter&amp;rdquo; : 1, &amp;ldquo;attr9&amp;rdquo; : 1, &amp;ldquo;attrArray&amp;rdquo; : [ &amp;ldquo;xxx&amp;rdquo; ] }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 4, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo;,
   &amp;ldquo;attrCounter&amp;rdquo; : 1, &amp;ldquo;attr9&amp;rdquo; : 1, &amp;ldquo;attrArray&amp;rdquo; : [ &amp;ldquo;xxx&amp;rdquo; ], &amp;ldquo;attrNew&amp;rdquo; : &amp;ldquo;abc&amp;rdquo; }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 5, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo;,
   &amp;ldquo;attrCounter&amp;rdquo; : 2, &amp;ldquo;attr9&amp;rdquo; : 1, &amp;ldquo;attrArray&amp;rdquo; : [ &amp;ldquo;xxx&amp;rdquo; ], &amp;ldquo;attrNewReplacement&amp;rdquo; : &amp;ldquo;abc&amp;rdquo; }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 6, &amp;ldquo;attr7&amp;rdquo; : &amp;ldquo;xxx279&amp;rdquo;,
   &amp;ldquo;attrCounter&amp;rdquo; : 3, &amp;ldquo;attr9&amp;rdquo; : 1, &amp;ldquo;attrArray&amp;rdquo; : [ ], &amp;ldquo;attrNewReplacement&amp;rdquo; : &amp;ldquo;abc&amp;rdquo; }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 7 }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 8, &amp;ldquo;attrCounter&amp;rdquo; : 1, &amp;ldquo;a&amp;rdquo; : 1 }
{ &amp;ldquo;docId&amp;rdquo; : 279, &amp;ldquo;version&amp;rdquo; : 9, &amp;ldquo;attrCounter&amp;rdquo; : 1, &amp;ldquo;a&amp;rdquo; : 1 }
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;There you have it, my preferred way to isolate an infrequently used collection and keep it updated based on every write action that happens in the main DB.  I hope you can see how this can be extended for many different pub/sub needs as you can adapt your code to watch for different types of events on different collections.&lt;/p&gt;

&lt;p&gt;Hope you found this educational and keep those questions coming!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:897d8a06bfdbaecbf747ed08c519e7de:1&#34;&gt;Because the oplog must write change operations in &lt;a href=&#34;http://docs.mongodb.org/manual/reference/glossary/#term-idempotent&#34;&gt;&amp;ldquo;idemponent&amp;rdquo;&lt;/a&gt; form, all update operators are transformed into their equivalent &lt;code&gt;$set&lt;/code&gt; operations.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:897d8a06bfdbaecbf747ed08c519e7de:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Merge Shapes with Aggregation Framework</title>
          <link>http://asya999.github.io/post/mergeshapes/</link>
          <pubDate>Sat, 24 May 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/mergeshapes/</guid>
          <description>

&lt;h3 id=&#34;question:03d96d95380719f8d4c3b2ad2c7c545e&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Consider two separate shapes of data like this in a single collection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{   type: &amp;quot;A&amp;quot;,
    level: 0,
    color: &amp;quot;red&amp;quot;,
    locale: &amp;quot;USA&amp;quot;
}
{   type: &amp;quot;A&amp;quot;,
    level: 1,
    color: &amp;quot;blue&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The goal is to present a merged shape to the application with the level n data overridden by level n+1 if level n+1 data exists for type A, starting with n = 0.  In other words, the app wants to see this shape:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{   type: &amp;quot;A&amp;quot;,
    level: 1, 
    color: &amp;quot;blue&amp;quot;,
    locale: &amp;quot;USA&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If no level 1 data exists, the app would see the default (level 0) shape.   Think of it as a layered merge.&lt;/p&gt;

&lt;h3 id=&#34;answer:03d96d95380719f8d4c3b2ad2c7c545e&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;In the &lt;a href=&#34;http://askasya.com/post/trackversions&#34;&gt;previous &amp;ldquo;AskAsya&amp;rdquo; on tracking versions&lt;/a&gt; we looked at different ways of tracking all versions of changing objects, and this happens to be a complex variant of that problem that we considered as &amp;ldquo;schema 4&amp;rdquo; - it&amp;rsquo;s a possible approach to versioning, but it presents an interesting challenge returning the &amp;ldquo;full&amp;rdquo; current object back to the client.&lt;/p&gt;

&lt;h4 id=&#34;merging-different-shapes:03d96d95380719f8d4c3b2ad2c7c545e&#34;&gt;Merging Different Shapes&lt;/h4&gt;

&lt;p&gt;This problem would be easily solved with aggregation framework query, except for the problem that we need to know the names of all the keys/fields, and we might not  know all of the possible fields that could exist in our documents. Without this information, the only way we have of merging documents is using MapReduce, which is both more complex &lt;em&gt;and&lt;/em&gt; slower.   I will show both solutions and I&amp;rsquo;ll leave it up to you to determine which will be more performant in your scenario (or whether you want to switch to a different versioning schema).&lt;/p&gt;

&lt;h5 id=&#34;aggregation-framework:03d96d95380719f8d4c3b2ad2c7c545e&#34;&gt;Aggregation Framework&lt;/h5&gt;

&lt;p&gt;This will be the fastest way if you either have all possible attribute names that your documents could have, or get them via a scan of the entire collection (note that the latter immediately becomes stale, as new documents with new attributes could show up as soon as you start querying, but that&amp;rsquo;s inherently an issue that always exists in any system that doesn&amp;rsquo;t provide repeatable read isolation).&lt;/p&gt;

&lt;p&gt;Get the possible attribute names (I&amp;rsquo;m assuming &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;level&lt;/code&gt; are your &amp;lsquo;id&amp;rsquo; and &amp;lsquo;version&amp;rsquo;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var att = { };
var attrs = [ ];
db.coll.find( {}, {_id:0, type:0, level:0} ).forEach( function(d) {
    for ( i in d)
         if ( !att.hasOwnProperty(i) ) {
             att[i]=1;
             attrs.push(i);
         }
} );                   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You now have an array &lt;code&gt;attrs&lt;/code&gt; which holds all the strings representing different attributes in your collection.&lt;/p&gt;

&lt;p&gt;We now programmatically generate stage for &lt;code&gt;$project&lt;/code&gt; that turns each attribute into a subdocument with its level first and attribute itself second.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;proj1={$project:{type:1, level:1}};
attrs.forEach(function(attr) { 
    _a=&amp;quot;_&amp;quot;+attr; 
    a=&amp;quot;$&amp;quot;+attr;   
    proj1[&amp;quot;$project&amp;quot;][_a]={}; 
    proj1[&amp;quot;$project&amp;quot;][_a][&amp;quot;l&amp;quot;]={&amp;quot;$cond&amp;quot;:{}};
    proj1[&amp;quot;$project&amp;quot;][_a][&amp;quot;l&amp;quot;][&amp;quot;$cond&amp;quot;]={if:{&amp;quot;$gt&amp;quot;:[a,null]},then:&amp;quot;$level&amp;quot;,else:-1};
    proj1[&amp;quot;$project&amp;quot;][_a][attr]=a;
} );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since levels are increasing, this set us to be able to &lt;code&gt;$group&lt;/code&gt; using the &lt;code&gt;$max&lt;/code&gt; operator to keep the highest level for each attribute.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;group={$group:{_id:&amp;quot;$type&amp;quot;,lvl:{$max:&amp;quot;$level&amp;quot;}}};
attrs.forEach(function(attr) { 
    a=&amp;quot;$_&amp;quot;+attr;
    group[&amp;quot;$group&amp;quot;][attr]={&amp;quot;$max&amp;quot;:a};
} )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last &lt;code&gt;$project&lt;/code&gt; just transforms the fields of our document back into the same key names they had before.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;proj2={$project:{_id:0,type:&amp;quot;$_id&amp;quot;, level:&amp;quot;$lvl&amp;quot;}}
attrs.forEach(function(attr) {
    a=&amp;quot;$&amp;quot;+attr;  
    proj2[&amp;quot;$project&amp;quot;][attr]=a+&amp;quot;.&amp;quot;+attr;
} )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now all set to run the aggregation with your programmatically generated stages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.coll.aggregate( proj1, group, proj2 );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To recap,&lt;code&gt;proj1&lt;/code&gt; is the stage where we converted every attribute into a subdocument which included &amp;ldquo;level&amp;rdquo; (first) and attribute value (second).  If a given attribute didn&amp;rsquo;t exist in a document, it went in with level:-1 and value:null.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;group&lt;/code&gt; is where we grouped by &lt;code&gt;type&lt;/code&gt; which is our &lt;code&gt;docId&lt;/code&gt; and kept the highest (max) &amp;ldquo;subdocument&amp;rdquo; for each possible attribute.  This works because MongoDB allows you to compare any types (including BSON) and level:-1 is always going to &amp;ldquo;lose&amp;rdquo; to a higher level.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;proj2&lt;/code&gt; is when we turned all the fields into readable format, or at least format resembling our initial document.&lt;/p&gt;

&lt;p&gt;This now returned to us the merged documents.&lt;/p&gt;

&lt;p&gt;If we had original documents like these:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.coll.find({},{_id:0}).sort({type:1,level:1})
{ &amp;quot;type&amp;quot; : &amp;quot;A&amp;quot;, &amp;quot;level&amp;quot; : 0, &amp;quot;color&amp;quot; : &amp;quot;red&amp;quot;, &amp;quot;locale&amp;quot; : &amp;quot;USA&amp;quot; }
{ &amp;quot;type&amp;quot; : &amp;quot;A&amp;quot;, &amp;quot;level&amp;quot; : 1, &amp;quot;color&amp;quot; : &amp;quot;blue&amp;quot; }
{ &amp;quot;type&amp;quot; : &amp;quot;A&amp;quot;, &amp;quot;level&amp;quot; : 2, &amp;quot;priority&amp;quot; : 5 }
{ &amp;quot;type&amp;quot; : &amp;quot;A&amp;quot;, &amp;quot;level&amp;quot; : 3, &amp;quot;locale&amp;quot; : &amp;quot;EMEA&amp;quot; }
{ &amp;quot;type&amp;quot; : &amp;quot;B&amp;quot;, &amp;quot;level&amp;quot; : 0, &amp;quot;priority&amp;quot; : 1 }
{ &amp;quot;type&amp;quot; : &amp;quot;B&amp;quot;, &amp;quot;level&amp;quot; : 1, &amp;quot;color&amp;quot; : &amp;quot;purple&amp;quot;, &amp;quot;locale&amp;quot; : &amp;quot;Canada&amp;quot; }
{ &amp;quot;type&amp;quot; : &amp;quot;B&amp;quot;, &amp;quot;level&amp;quot; : 2, &amp;quot;color&amp;quot; : &amp;quot;green&amp;quot; }
{ &amp;quot;type&amp;quot; : &amp;quot;B&amp;quot;, &amp;quot;level&amp;quot; : 3, &amp;quot;priority&amp;quot; : 2, &amp;quot;locale&amp;quot; : &amp;quot;USA&amp;quot; }
{ &amp;quot;type&amp;quot; : &amp;quot;B&amp;quot;, &amp;quot;level&amp;quot; : 4, &amp;quot;color&amp;quot; : &amp;quot;NONE&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We got back results that looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.coll.aggregate( proj1, group, proj2 );
{ &amp;quot;color&amp;quot; : &amp;quot;NONE&amp;quot;, &amp;quot;locale&amp;quot; : &amp;quot;USA&amp;quot;, &amp;quot;priority&amp;quot; : 2, &amp;quot;type&amp;quot; : &amp;quot;B&amp;quot;, &amp;quot;level&amp;quot; : 4 }
{ &amp;quot;color&amp;quot; : &amp;quot;blue&amp;quot;, &amp;quot;locale&amp;quot; : &amp;quot;EMEA&amp;quot;, &amp;quot;priority&amp;quot; : 5, &amp;quot;type&amp;quot; : &amp;quot;A&amp;quot;, &amp;quot;level&amp;quot; : 3 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this is not performant for filtering on attributes since we can&amp;rsquo;t apply the filter until we have &amp;ldquo;merged&amp;rdquo; all the documents, and that means that indexes can&amp;rsquo;t be used effectively.  While this aggregation may be a good exercise, unless you are saving this output into a new collection that you then index by attributes for querying, it won&amp;rsquo;t be a good schema if you need very fast responses.&lt;/p&gt;

&lt;p&gt;Here is MapReduce for the same functionality:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;map = function () {
    var doc=this;
    delete(doc._id);
    var level=this.level;
    delete(doc.level);
    var t=this.type;
    delete(doc.type);
    for (i in doc) {
         val={level:level};
         val[i]={ l:level, v:doc[i]};
         emit(t, val);
    }
}

reduce = function (key,values) {
  result={level:-1};
  values.forEach(function(val) {
    if (result.level&amp;lt;val.level) result.level=val.level;
    var attr=null;
    for (a in val) if (a!=&amp;quot;level&amp;quot;) { attr=a; break; }
    if (!result.hasOwnProperty(attr) || result[attr].l&amp;lt;=val[attr].l) {
          result[attr]=val[attr];

    }
  })
  return result;
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Track Versions with MongoDB</title>
          <link>http://asya999.github.io/post/trackversions/</link>
          <pubDate>Wed, 21 May 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/trackversions/</guid>
          <description>

&lt;h3 id=&#34;question:4d494242d33833a567dbfba012f85510&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Consider requirement that we have to be able to recreate/query any version of a document that ever existed in a particular collection.   So we start out with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{   docId: &amp;quot;A&amp;quot;,
    version: 1,
    color: &amp;quot;red&amp;quot;,
    locale: &amp;quot;USA&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we need to set color to &amp;ldquo;blue&amp;rdquo;, instead of updating the &amp;ldquo;color&amp;rdquo; field from &amp;ldquo;red&amp;rdquo; to &amp;ldquo;blue&amp;rdquo;, we have to create a new version of the document which now has its full &amp;ldquo;current&amp;rdquo; state, and preserve somehow the old version of the document.   So we insert&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{   docId: &amp;quot;A&amp;quot;,
    version: 2,
    color: &amp;quot;blue&amp;quot;,
    locale: &amp;quot;USA&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The goal is to preserve every state for each object, but we only respond to queries with the &amp;ldquo;current&amp;rdquo; or &amp;ldquo;latest&amp;rdquo; version, we just have a requirement to be able to have an audit (which would be very infrequent so it&amp;rsquo;s okay if it&amp;rsquo;s slow).    Is keeping each version as we do in this example the best schema/approach to this problem?&lt;/p&gt;

&lt;h3 id=&#34;answer:4d494242d33833a567dbfba012f85510&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;Versioning can be tricky to get right if you don&amp;rsquo;t know all of the requirements of the application and approximate expected loads for various operations.  I&amp;rsquo;ll lay out a few possible approaches and point out their strength and weaknesses.&lt;/p&gt;

&lt;h4 id=&#34;problem-statement:4d494242d33833a567dbfba012f85510&#34;&gt;Problem Statement:&lt;/h4&gt;

&lt;p&gt;In some systems, rather than updating an existing object and overwriting its various attributes there is a business requirement to preserve the original document and to create a new version of this document, instead of updating it.  This raises the following interesting challenges:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You must correctly generate the new version number in a multithreaded system&lt;/li&gt;
&lt;li&gt;You must return only the current version of each document when there is a query&lt;/li&gt;
&lt;li&gt;You must &amp;ldquo;update&amp;rdquo; correctly by including all current attributes in addition to newly provided attributes&lt;/li&gt;
&lt;li&gt;If the system fails at any point, you must either have a consistent state of the data, or it must be possible on re-start to infer the state of the data and clean it up, or otherwise bring it to consistent state.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;possible-schema-approaches:4d494242d33833a567dbfba012f85510&#34;&gt;Possible Schema Approaches:&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;Store full document each write with monotonically increasing version number.

&lt;ul&gt;
&lt;li&gt;1a. possibily with a field in latest version identifying it as such.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Store all document versions inside a single document.&lt;/li&gt;
&lt;li&gt;Store current document in your &amp;ldquo;primary&amp;rdquo; collection, and keep previous versions in a second collection.&lt;/li&gt;
&lt;li&gt;Store only &amp;ldquo;deltas&amp;rdquo; with each increasing version.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;generating-correct-version-number:4d494242d33833a567dbfba012f85510&#34;&gt;Generating correct version number&lt;/h5&gt;

&lt;p&gt;No matter which schema you choose, the issue of generating the correct &amp;ldquo;higher&amp;rdquo; version number will come down to one of three possible approaches:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;maintain a separate collection which hands out the next version for each document. This is probably the worst approach as it can be prone to contention and edge cases in multi-shard, multithreaded environment.&lt;/li&gt;
&lt;li&gt;use optimistic locking to read the current document, increment its version and save new document contingent on some constraint keeping you from succeeding simultaneously from two different threads.  This can be handled differently for different schemas, and it&amp;rsquo;s definitely a common and feasible approach.&lt;/li&gt;
&lt;li&gt;use a fine-grain timestamp - current time to the millisecond may be good enough, depending on your ability to synchronize all the clocks, unfortunately it only works with two of our four schema options.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I like optimistic locking option and it works well with all four of our schema options.  It involves a &amp;ldquo;compare-and-swap&amp;rdquo; technique where you read the current document,  do the appropriate calculation of new version and adding new attributes, and then try the insert or update, contingent on no one having updated the document ahead of you.  If it&amp;rsquo;s an update, you include parts of the original document in your query condition, and if it&amp;rsquo;s an insert you must have a unique constraint to prevent success of multiple simultaneous attempts to version the same document.   In both cases you must check the result of your write - knowing if it succeeded or failed is how the thread knows it has to try again.&lt;/p&gt;

&lt;h4 id=&#34;how-does-each-schema:4d494242d33833a567dbfba012f85510&#34;&gt;How does each schema:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;return only the current document&lt;/li&gt;
&lt;li&gt;generate new version number to &amp;ldquo;update&amp;rdquo; existing and add new attributes

&lt;ul&gt;
&lt;li&gt;this includes recovering from failure in the middle of a set of operations (if there is more than one)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h5 id=&#34;choice-1:4d494242d33833a567dbfba012f85510&#34;&gt;Choice 1&lt;/h5&gt;

&lt;p&gt;Store full document each time there is a write with monotonically increasing version number inside.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 1,  &amp;quot;attr1&amp;quot;: 165 }   /*version 1 */
    {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 2,  &amp;quot;attr1&amp;quot;: 165, &amp;quot;attr2&amp;quot;: &amp;quot;A-1&amp;quot; } 
    {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 3,  &amp;quot;attr1&amp;quot;: 184, &amp;quot;attr2&amp;quot; : &amp;quot;A-1&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each docId value the document with the highest &amp;ldquo;v&amp;rdquo; represents the full current object state.  In this example, docId 174 v:3 represents the total current state.   There is a unique index on &lt;code&gt;{&amp;quot;docId&amp;quot;:1,&amp;quot;v&amp;quot;:1}&lt;/code&gt;&lt;/p&gt;

&lt;h6 id=&#34;to-return-only-current-document:4d494242d33833a567dbfba012f85510&#34;&gt;To return only current document&lt;/h6&gt;

&lt;p&gt;If the query is for a single docId, the query would be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.docs.find({&amp;quot;docId&amp;quot;:174}).sort({&amp;quot;v&amp;quot;:-1}).limit(-1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will find the documents with &amp;ldquo;docId&amp;rdquo; 174 and return the one that has the highest &amp;ldquo;v&amp;rdquo; only.  This will efficiently use our index on docId and v to only scan a single document.  The &lt;code&gt;-1&lt;/code&gt; for &lt;code&gt;limit&lt;/code&gt; just tells the server to close the cursor when the document is returned as we are done with it (it&amp;rsquo;s what &lt;code&gt;findOne&lt;/code&gt; functionally does under the covers).&lt;/p&gt;

&lt;p&gt;But what if you want to query for all documents that match a particular condition, but what you expect is that only the latest version of each document would be returned?   Now you have to use the aggregation framework to &amp;ldquo;merge&amp;rdquo; your document set to only keep those with the highest version and apply your filter then:&lt;/p&gt;

&lt;p&gt;Careful that you don&amp;rsquo;t do it the &lt;strong&gt;wrong&lt;/strong&gt; way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.docs.aggregate( [  
    {&amp;quot;$match&amp;quot;:{&amp;lt;your-match-condition&amp;gt;}}, /* WRONG */
    {&amp;quot;$sort&amp;quot;:{&amp;quot;docId&amp;quot;:-1,&amp;quot;v&amp;quot;:-1}},
    {&amp;quot;$group&amp;quot;:{&amp;quot;_id&amp;quot;:&amp;quot;$docId&amp;quot;,&amp;quot;doc&amp;quot;:{&amp;quot;$first&amp;quot;:&amp;quot;$$ROOT&amp;quot;}}}
] )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that applying your filter &lt;em&gt;before&lt;/em&gt; you filter out all but the latest version of each document may not return the current version of some of the documents!   So &lt;strong&gt;don&amp;rsquo;t&lt;/strong&gt; do this!&lt;/p&gt;

&lt;p&gt;Instead we have to do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sort={&amp;quot;$sort&amp;quot;: { &amp;quot;documentId&amp;quot; : 1, &amp;quot;version&amp;quot; : -1 } };
group={&amp;quot;$group&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;$documentId&amp;quot;,
                    &amp;quot;doc&amp;quot;: { &amp;quot;$first&amp;quot; : &amp;quot;$$ROOT&amp;quot; }
                   } };
match={&amp;quot;$match&amp;quot;:{&amp;quot;doc.attrN&amp;quot;:&amp;lt;value&amp;gt;}}; /* RIGHT */
db.collection.aggregate( sort, group, match )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is not efficient, sadly, as the indexes on our collection can only be effectively used &lt;strong&gt;before&lt;/strong&gt; the first group operation.&lt;/p&gt;

&lt;h6 id=&#34;to-generate-new-version-and-update:4d494242d33833a567dbfba012f85510&#34;&gt;To generate new version and update&lt;/h6&gt;

&lt;p&gt;Optimistic locking: each thread reads in the most current document (in this case &lt;code&gt;docId:174, v:3&lt;/code&gt;) makes attribute changes, increments &lt;code&gt;v&lt;/code&gt; by one and then tries the insert of this document:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.docs.insert({&amp;quot;docId&amp;quot;:174,&amp;quot;v&amp;quot;:4, &amp;quot;attr1&amp;quot;:184,&amp;quot;attr2&amp;quot;:&amp;quot;A-1&amp;quot;,&amp;quot;attr3&amp;quot;:&amp;quot;blue&amp;quot;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the insert succeeds, it&amp;rsquo;s done, but if it gets a unique constraint violation, it means another thread has already inserted a new version of this document, and this thread needs to try again (making sure to read the new &lt;code&gt;&amp;quot;v&amp;quot;:4&amp;quot;&lt;/code&gt; or whatever the latest version of the document is and trying its change till it succeeds.&lt;/p&gt;

&lt;p&gt;Failure does &lt;em&gt;not&lt;/em&gt; create an inconsistent state, since there is only a single write.&lt;/p&gt;

&lt;h5 id=&#34;choice-1a:4d494242d33833a567dbfba012f85510&#34;&gt;Choice 1a&lt;/h5&gt;

&lt;p&gt;In a variant of 1. we might add a field in the &amp;ldquo;current&amp;rdquo; version of each documentId&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 1,  &amp;quot;attr1&amp;quot;: 165 }
     {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 2,  &amp;quot;attr1&amp;quot;: 165, &amp;quot;attr2&amp;quot;: &amp;quot;A-1&amp;quot; }
     {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 3,  &amp;quot;attr1&amp;quot;: 184, &amp;quot;attr2&amp;quot; : &amp;quot;A-1&amp;quot;, current: true }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fetching the current document should now be easy, just query for { docId:174, current:true }, and when querying for multiple documents, add {&amp;ldquo;current&amp;rdquo;:true} to the query predicate, solving that problem.&lt;/p&gt;

&lt;p&gt;Updating becomes difficult now, since there is no method to insert one document and update another document &amp;ldquo;as one&amp;rdquo;.  So we would want to first insert a new version, based on currently highest version, and then  update the previously current document to $unset the &amp;ldquo;current&amp;rdquo; field.&lt;/p&gt;

&lt;p&gt;Now if our process fails between those two write operations, we will have two documents for a particular docId that have &amp;ldquo;current&amp;rdquo; set to true and that means all of our queries will have to guard against that possibility - that  seems unnecessarily complex, so let&amp;rsquo;s hold off on this method for now.&lt;/p&gt;

&lt;h5 id=&#34;choice-2:4d494242d33833a567dbfba012f85510&#34;&gt;Choice 2&lt;/h5&gt;

&lt;p&gt;Store all document versions inside a single document.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    {  &amp;quot;_id&amp;quot; : 174, &amp;quot;current&amp;quot; : { &amp;quot;v&amp;quot; :3, &amp;quot;attr1&amp;quot;: 184, &amp;quot;attr2&amp;quot; : &amp;quot;A-1&amp;quot; },
        &amp;quot;prev&amp;quot; : [ 
              {  &amp;quot;v&amp;quot; : 1,  &amp;quot;attr1&amp;quot;: 165 },
              {  &amp;quot;v&amp;quot; : 2,  &amp;quot;attr1&amp;quot;: 165, &amp;quot;attr2&amp;quot;: &amp;quot;A-1&amp;quot; }
        ]
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each docId, the current state is represented by its &amp;ldquo;current&amp;rdquo; subdocument.   Since &amp;ldquo;docId&amp;rdquo; is unique it can be stored in the &amp;ldquo;_id&amp;rdquo; field.&lt;/p&gt;

&lt;p&gt;The merits of multiple possible solutions mostly depend on how many versions of each object you expect to have and how long you have to keep them.  If the lifetime of a document usually has a number of versions that are in the single digits or low double digits, you can do well embedding the versions inside of the single document that represents each object.&lt;/p&gt;

&lt;h6 id=&#34;to-return-only-current-document-1:4d494242d33833a567dbfba012f85510&#34;&gt;To return only current document&lt;/h6&gt;

&lt;p&gt;Since there is only one document, we just search by docId or other attributes of &amp;ldquo;current&amp;rdquo; and we use projection to exclude the previous array: &lt;code&gt;db.collection.find({&amp;quot;docId&amp;quot;:174}, {&amp;quot;prev&amp;quot;:0})&lt;/code&gt; except in cases we want to see it.&lt;/p&gt;

&lt;h6 id=&#34;to-generate-new-version-and-update-1:4d494242d33833a567dbfba012f85510&#34;&gt;To generate new version and update&lt;/h6&gt;

&lt;p&gt;Even though you only have one document per docId, you really can&amp;rsquo;t create a new version with a single update, since it depends on knowing what the current subdocument is, but what you can do is utilize the &amp;ldquo;compare-and-swap&amp;rdquo; technique where you read the current document, move &amp;ldquo;current&amp;rdquo; field to the end of the previous array, set the new &amp;ldquo;current&amp;rdquo; field appropriately, and then do the update &lt;strong&gt;contingent on no one having updated the document ahead of you&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var doc = db.collection.findOne( { &amp;quot;_id&amp;quot; : 174 });
/* save the current version */
var currVersion = doc.current.v;
/* push the current subdocument to the end of prev array */
doc.prev.push(doc.current);
/* construct the new &amp;quot;current&amp;quot; subdocument */
doc.current = { &amp;quot;v&amp;quot; : currVersion+1, &amp;quot;attr1&amp;quot; : &amp;lt;value&amp;gt;, &amp;quot;attr2&amp;quot; : &amp;lt;value&amp;gt; }
var result = db.collection.update( { &amp;quot;_id&amp;quot; : 174, &amp;quot;current.v&amp;quot; : currVersion },  { &amp;quot;$set&amp;quot; : doc } )
if (result.nModified != 1) {
    print(&amp;quot;Someone must have gotten there first, re-fetch the new document, try again&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;atomicity-and-maintainability:4d494242d33833a567dbfba012f85510&#34;&gt;atomicity and maintainability&lt;/h6&gt;

&lt;p&gt;There are many pros in this approach:
- atomic updates of document (single operation both, sets new current and updates previous)
- querying only needs to happen on &amp;ldquo;current&amp;rdquo; attributes, since you only need to access previous infrequently
- it&amp;rsquo;s simple to return just current attributes or to exclude previous from being returned to the application
- the &lt;code&gt;_id&lt;/code&gt; index can be used for the unique docId preventing duplicates being accidentally inserted
- creating the next version number is simple and thread-safe&lt;/p&gt;

&lt;p&gt;The cons are all performance based:
- when each document grows beyond its previously allocated size, it has to be moved which makes some updates more time consuming
- this won&amp;rsquo;t work at all for documents that have thousands of versions over their lifetime (the documents would get too big, unwieldy, and could potentially exceed 16MB limit)
- there&amp;rsquo;ll likely be more fragmentation than with the more &amp;ldquo;naive&amp;rdquo; approach of inserting new versions as new documents
- if the system has a very high level of concurrency when multiple threads are trying to each make different update to a specific document, a single thread could keep getting beat and it might take multiple re-tries to persist its update&lt;/p&gt;

&lt;p&gt;The last &amp;ldquo;con&amp;rdquo; shouldn&amp;rsquo;t really be a concern if you are genuinely talking about a system where each document is only going to have a handful of versions (or updates) in its lifecycle, and this schema isn&amp;rsquo;t appropriate for systems where a single document will live through hundreds (or thousands) of updates.&lt;/p&gt;

&lt;h5 id=&#34;choice-3:4d494242d33833a567dbfba012f85510&#34;&gt;Choice 3&lt;/h5&gt;

&lt;p&gt;Store current document in your &amp;ldquo;primary&amp;rdquo; collection, and keep older versions in another collection.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    CurrentCollection:     {  &amp;quot;docId&amp;quot; : 174,  &amp;quot;v&amp;quot; :3, &amp;quot;attr1&amp;quot;: 184, &amp;quot;attr2&amp;quot; : &amp;quot;A-1&amp;quot; }
    CollectionOfPrevious:
              {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 1,  &amp;quot;attr1&amp;quot;: 165 }
              {  &amp;quot;docId&amp;quot; : 174, &amp;quot;v&amp;quot; : 2,  &amp;quot;attr1&amp;quot;: 165, &amp;quot;attr2&amp;quot;: &amp;quot;A-1&amp;quot; }

For each documentId, there is only one document which represented its current state.
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;to-return-only-current-document-2:4d494242d33833a567dbfba012f85510&#34;&gt;To return only current document&lt;/h6&gt;

&lt;p&gt;This one is the simplest of them all.  Since the old versions are in another collection, you just query normally when you need to find a single or multiple documents - they will all be the current version.&lt;/p&gt;

&lt;h6 id=&#34;to-generate-new-version-and-update-2:4d494242d33833a567dbfba012f85510&#34;&gt;To generate new version and update&lt;/h6&gt;

&lt;p&gt;This one may be the hardest.  Technically, you only need to do a few things: read the current document, construct out of it the new current document, save the new current document on top of previous one and if successful, then insert the old current document into the &amp;ldquo;previous&amp;rdquo; collection.   Of course we would use the same compare-and-swap update to make sure that no one changed the document between our read and our write, and only insert into previous collection if we successfully update current.&lt;/p&gt;

&lt;p&gt;The problem is that you may fail before that last write and now you&amp;rsquo;ll be missing a version of this document from the &amp;ldquo;previous&amp;rdquo; collection.&lt;/p&gt;

&lt;p&gt;What if we  switch the order of writes to save into the previous collection first?   We read the current document, we write it to previous collection, we now change it to be &amp;ldquo;new&amp;rdquo; current and save it into &amp;ldquo;current&amp;rdquo; collection.  This has several advantages:
- if someone else is trying to update this document, they will also be saving into &amp;ldquo;previous&amp;rdquo; collection, so having a unique index on docId, version will tell us if we lost the race and now have to try again.&lt;br /&gt;
- if the thread dies in the middle (after insert into previous and before updating current) it&amp;rsquo;s not the end of the world, as your current collection was not affected, but you do need a way to &amp;ldquo;clean up&amp;rdquo; your &amp;ldquo;previous&amp;rdquo; collection, first because you need to remove the version of the document that never existed in &amp;ldquo;current&amp;rdquo; and second because it will block all other &amp;ldquo;updates&amp;rdquo; on this document by using an invalid &amp;ldquo;docId&amp;rdquo;, &amp;ldquo;version&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Luckily, clean-up may be simple, as any time we detect that there exists a docId, version in &amp;ldquo;current&amp;rdquo; that also exists in &amp;ldquo;previous&amp;rdquo; it means either there is an update &amp;ldquo;in progress&amp;rdquo; or it means that an update &amp;ldquo;died&amp;rdquo; and we should clean up.  Of course the devil is in the details - and it could cause delays in the system since you have to wait long enough to be sure that this &amp;ldquo;in progress&amp;rdquo; update actually died.  Or you can have another field that you update after successful writes in both places (now making it easier to recover, but needing to do three writes before you&amp;rsquo;re done with a single document update!)  Let&amp;rsquo;s call this Medium Hard still.&lt;/p&gt;

&lt;h5 id=&#34;choice-4:4d494242d33833a567dbfba012f85510&#34;&gt;Choice 4&lt;/h5&gt;

&lt;p&gt;Store only &amp;ldquo;deltas&amp;rdquo; with increasing versions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    {  &amp;quot;documentId&amp;quot; : 174, &amp;quot;v&amp;quot; : 1,  &amp;quot;attr1&amp;quot;: 165 }
    {  &amp;quot;documentId&amp;quot; : 174, &amp;quot;v&amp;quot; : 2,  &amp;quot;attr2&amp;quot;: &amp;quot;A-1&amp;quot; }
    {  &amp;quot;documentId&amp;quot; : 174, &amp;quot;v&amp;quot; : 3,  &amp;quot;attr1&amp;quot;: 184 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each docId, the current state must be derived by &amp;ldquo;merging&amp;rdquo; all the documents with matching docId, keeping the &amp;ldquo;latest&amp;rdquo; or &amp;ldquo;highest&amp;rdquo; version&amp;rsquo;s value of each attribute if it occurs in more than one version.&lt;/p&gt;

&lt;p&gt;This one is quite complex, and the answer is getting quite long, so let&amp;rsquo;s call it hard and defer the details of how we accomplish this merge till &lt;a href=&#34;http://askasya.com/post/mergeshapes&#34;&gt;next time&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;let-s-summarize:4d494242d33833a567dbfba012f85510&#34;&gt;Let&amp;rsquo;s Summarize&lt;/h4&gt;

&lt;p&gt;Here&amp;rsquo;s a table that shows for each schema choice how well we can handle the reads, writes and if an update has to make more than one write, how easy it is to recover or to be in a relatively &amp;ldquo;safe&amp;rdquo; state:&lt;/p&gt;

&lt;p&gt;|Schema | Fetch 1 | Fetch Many || Update | Recover if fail|
&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-
|1) New doc for each | Easy,Fast  | Not easy,Slow | | Medium | N/A |
|1a) New doc with &amp;ldquo;current&amp;rdquo; | Easy,Fast  | Easy,Fast | | Medium | Hard |
|2) Embedded in single doc | Easy,Fastest  | Easy,Fastest | | Medium | N/A |
|3) Sep Collection for prev. |  Easy,Fastest  | Easy,Fastest  | | Medium |  Medium Hard |
|4) Deltas only in new doc | TBD/Hard | TBD/Hard | | Medium | N/A |
|?) TBD |  Easy,Fastest  | Easy,Fastest  | | Easy,Fastest |  N/A |&lt;/p&gt;

&lt;p&gt;&amp;ldquo;N/A&amp;rdquo; for recovery means there is no inconsistent state possible - if we only have to make one write to create/add a new version, we are safe from any inconsistency.  So &amp;ldquo;N/A&amp;rdquo; is the &amp;ldquo;easiest&amp;rdquo; value there.&lt;/p&gt;

&lt;p&gt;I deferred describing what it would take to query the &amp;ldquo;Store deltas only&amp;rdquo; option till the next &amp;ldquo;Ask Asya&amp;rdquo; but let me foreshadow and tell you that it&amp;rsquo;s not particularly easy - it involves a long and tricky aggregation.&lt;/p&gt;

&lt;p&gt;But you can see I filled in a yet undescribed way that magically somehow makes all our tasks easy, and yet seems to not have any performance issues nor consistency problems.  If you&amp;rsquo;ve stuck with me this far, I promise that I will describe the magical &amp;ldquo;winner&amp;rdquo; for version keeping in the next &amp;ldquo;Ask Asya&amp;rdquo; after the one that shows how to aggregate deltas of document into one.&lt;/p&gt;

&lt;p&gt;Since I just enabled comments and discussion on these pages, if you see a possible schema approach I didn&amp;rsquo;t mention, feel free to suggest it.  Free &amp;ldquo;MongoDB&amp;rdquo; t-shirt for you if you guess the &amp;ldquo;TBD&amp;rdquo; schema I have in mind.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>What Does FindAndModify Do</title>
          <link>http://asya999.github.io/post/findandmodify/</link>
          <pubDate>Mon, 19 May 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/findandmodify/</guid>
          <description>

&lt;h3 id=&#34;question:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;I saw &lt;a href=&#34;1&#34; title=&#34;Actually [William_Shakespeare][2] said it. 
&#34;&gt;your answer on SO&lt;/a&gt; about the difference between &amp;ldquo;update&amp;rdquo; and &amp;ldquo;findAndModify&amp;rdquo;, could you explain in more detail what the difference is, and why MongoDB findAndModify is named what it is?&lt;/p&gt;

&lt;h3 id=&#34;answer:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;What&amp;rsquo;s in a name?  that which we call a rose&lt;br /&gt;
 By any other name would smell as sweet&lt;/em&gt;;
&lt;p style=&#34;text-align:right&#34; markdown=&#34;1&#34;&gt; - said Juliet&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8efa43574f94dcc0d1d4b4a79faa202f:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8efa43574f94dcc0d1d4b4a79faa202f:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, a lot is in a name.  A poorly chosen name can confuse many users, year after year.  I believe &lt;code&gt;findAndModify&lt;/code&gt; was probably not the best name for the role that it plays.&lt;/p&gt;

&lt;h5 id=&#34;update:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;update&lt;/h5&gt;

&lt;p&gt;An &lt;a href=&#34;http://docs.mongodb.org/manual/reference/method/db.collection.update/&#34;&gt;update&lt;/a&gt; finds an appropriate document (by default it&amp;rsquo;s just one) and then it changes its contents according to your specification.&lt;/p&gt;

&lt;h5 id=&#34;findandmodify:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;findAndModify&lt;/h5&gt;

&lt;p&gt;The &lt;a href=&#34;http://docs.mongodb.org/manual/reference/command/findAndModify/#dbcmd.findAndModify&#34;&gt;findAndModify command&lt;/a&gt; finds an appropriate document (it&amp;rsquo;s always just one) and then it changes its contents according to your specification and &lt;em&gt;then it returns that exact document that it changed&lt;/em&gt; (old version or new version, depending on which you ask for)&lt;/p&gt;

&lt;h5 id=&#34;what-s-the-difference:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;What&amp;rsquo;s the Difference?&lt;/h5&gt;

&lt;p&gt;They both find a document and update it atomically.  What that means is that it&amp;rsquo;s not possible for another thread to change part of this document between the time we find it and start updating it and when we finish updating it.   It also means that no other thread will see this document in &amp;ldquo;half-updated&amp;rdquo; state.  That&amp;rsquo;s what &amp;ldquo;atomic&amp;rdquo; means - all-or-nothing.&lt;/p&gt;

&lt;h5 id=&#34;why-do-we-even-need-findandmodify-then:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;Why do we even need &lt;code&gt;findAndModify&lt;/code&gt; then?&lt;/h5&gt;

&lt;p&gt;What if we need to get the full document that we just updated (like marking an item in a queue &amp;ldquo;yours&amp;rdquo; and then working on it)?&lt;/p&gt;

&lt;p&gt;What I said on &lt;a href=&#34;http://stackoverflow.com&#34;&gt;StackOverflow&lt;/a&gt; was:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you fetch an item and then update it, there may be an update by another thread between those two steps. If you update an item first and then fetch it, there may be another update in-between and you will get back a different item than what you updated.&lt;/p&gt;

&lt;p&gt;Doing it &amp;ldquo;atomically&amp;rdquo; means you are guaranteed that you are getting back the exact same item you are updating - i.e. no other operation can happen in between.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That&amp;rsquo;s why you&amp;rsquo;ll hear people talk about &lt;code&gt;findAndModify&lt;/code&gt; in the context of implementing a queue mechanism - &lt;code&gt;findAndModify&lt;/code&gt; can update a single document to indicate that you are now working on it, and return that same document to you in one operation.&lt;/p&gt;

&lt;h5 id=&#34;when-not-to-use-findandmodify:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;When Not to Use &lt;code&gt;findAndModify&lt;/code&gt;&lt;/h5&gt;

&lt;p&gt;There are scenarios where &lt;code&gt;findAndModify&lt;/code&gt; cannot help you.   If you need to update a document based on existing values of a document, you can use many &lt;a href=&#34;http://docs.mongodb.org/manual/reference/operator/update/#id1&#34;&gt;update operators&lt;/a&gt; which are atomic and allow you to change a field value without knowing what its current value is, like &lt;code&gt;$inc&lt;/code&gt; and &lt;code&gt;$addToSet&lt;/code&gt; and &lt;code&gt;$min&lt;/code&gt;  and &lt;code&gt;$max&lt;/code&gt;, etc.  They allow you to modify a field without having to read the value of that field first.  And they work with a regular &lt;code&gt;update&lt;/code&gt; as well as with &lt;code&gt;findAndModify&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But if you need to set field &lt;code&gt;a1&lt;/code&gt; based on the current value of the field &lt;code&gt;b2&lt;/code&gt; then you would have to read the document first and then when executing your update, you would have to ensure that the update is conditional on no one else having changed that document in the meantime and/or by having unique constraints to guarantee it.&lt;/p&gt;

&lt;p&gt;There is no way to utilize &lt;code&gt;findAndModify&lt;/code&gt; here, because it&amp;rsquo;s limited to the exact set of operators that &lt;code&gt;update&lt;/code&gt; uses, all it adds is the ability to return the exact document you modified.  Of course, &lt;code&gt;findAndModify&lt;/code&gt; has to do more work than &lt;code&gt;update&lt;/code&gt; so for best performance you should only use &lt;code&gt;findAndModify&lt;/code&gt; when you must have the document that you just updated back in the application.   If you just want to know if an &lt;code&gt;update&lt;/code&gt; succeeded, you can examine the &lt;a href=&#34;http://docs.mongodb.org/manual/reference/command/update/#output&#34;&gt;WriteResult&lt;/a&gt; that update returns.&lt;/p&gt;

&lt;h3 id=&#34;proposal:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;Proposal&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s rename &lt;code&gt;findAndModify&lt;/code&gt; to a name that more accurately describes its function.  It updates a document and returns it, but to maintain a small connection to its current name, I nearby propose we rename it:&lt;/p&gt;

&lt;h2 id=&#34;modifyandreturn:8efa43574f94dcc0d1d4b4a79faa202f&#34;&gt;modifyAndReturn&lt;/h2&gt;

&lt;p&gt;Who&amp;rsquo;s with me? &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8efa43574f94dcc0d1d4b4a79faa202f:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8efa43574f94dcc0d1d4b4a79faa202f:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:8efa43574f94dcc0d1d4b4a79faa202f:1&#34;&gt;Actually &lt;a href=&#34;2&#34; title=&#34;Please vote for [SERVER-13979][a] if you agree.
&#34;&gt;William_Shakespeare&lt;/a&gt; said it.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8efa43574f94dcc0d1d4b4a79faa202f:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8efa43574f94dcc0d1d4b4a79faa202f:2&#34;&gt;Please vote for &lt;a href=&#34;https://jira.mongodb.org/browse/SERVER-13979&#34;&gt;SERVER-13979&lt;/a&gt; if you agree.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8efa43574f94dcc0d1d4b4a79faa202f:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Balance Collections Across Your Sharded Cluster</title>
          <link>http://asya999.github.io/post/taggedcollectionbalancing/</link>
          <pubDate>Tue, 29 Apr 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/taggedcollectionbalancing/</guid>
          <description>

&lt;h3 id=&#34;question:1bc8c2f688d199692ef36348e0b0b4a8&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Is it possible to use &lt;a href=&#34;http://docs.mongodb.org/manual/core/tag-aware-sharding/&#34;&gt;&amp;ldquo;Tag aware sharding&amp;rdquo;&lt;/a&gt;  feature without having to use a special shard key?  The example in the tutorial makes it look like we would have to change our shard key to have a prefix value that we can define tag ranges on but we&amp;rsquo;re already sharded.  We have many collections in this database and we want to limit each collection to a subset of the shards so we can isolate the busy ones from each other.&lt;/p&gt;

&lt;h3 id=&#34;answer:1bc8c2f688d199692ef36348e0b0b4a8&#34;&gt;Answer:&lt;/h3&gt;

&lt;p&gt;Yes, that is absolutely possible, and it&amp;rsquo;s one of the cool capabilities of tag aware sharding.   A quick review of the feature.&lt;/p&gt;

&lt;h5 id=&#34;tag-aware-sharding-feature:1bc8c2f688d199692ef36348e0b0b4a8&#34;&gt;Tag aware sharding feature&lt;/h5&gt;

&lt;p&gt;Tags associate specific ranges of shard key values with specific shards for use in managing deployment patterns.
What this means is that in your sharded cluster you can assign zero, one or more tags (or labels) to each shard.  Then you can assign ranges of shard key values in various sharded collections to these tags.  The balancer then moves the appropriate chunks to appropriate shards to keep things the way you &amp;ldquo;assigned&amp;rdquo; them.&lt;/p&gt;

&lt;h5 id=&#34;the-balancer-diversion-into-migration-details:1bc8c2f688d199692ef36348e0b0b4a8&#34;&gt;The Balancer: Diversion into migration details&lt;/h5&gt;

&lt;p&gt;The whole balancing and migrations process is worthy of its own separate write-up but for now, I will simplify most of it and point out at the high level that the balancer is a thread that runs on mongos that wakes up periodically and checks (1) if it should be running (2) if there is anything for it to do.†  For the balancer &amp;ldquo;something to do&amp;rdquo; is always about moving chunks between shards.  The priorities that it considers when deciding which chunks need to be moved are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;draining shards: if one of the shards is &amp;ldquo;draining&amp;rdquo; - i.e. you plan to decommission it - then this will always be the first priority for all migrations unless it has no data left.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;tagged shards: if any chunks are on the &amp;ldquo;wrong&amp;rdquo; tagged shard for  its range, then it has to be moved to a &amp;ldquo;correct&amp;rdquo; shard.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;balance the remaining chunks: if the shard with the most chunks has nine+ more chunks than the shard with the fewest chunks, then the balancer will move chunks to try to keep things in balance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;how-do-you-tag-shards-and-ranges:1bc8c2f688d199692ef36348e0b0b4a8&#34;&gt;How do you tag shards and ranges?&lt;/h5&gt;

&lt;p&gt;All you have to do for tagging to work is mark some shards with &amp;ldquo;tags&amp;rdquo; and specify which ranges of shard key values will be associated with that tag.   The relevant commands are &lt;code&gt;sh.addTag(&amp;quot;shardName&amp;quot;,&amp;quot;tagName&amp;quot;)&lt;/code&gt; and &lt;code&gt;sh.addTagRange(&amp;quot;namespace&amp;quot;, { shardKey: minValue }, { shardKey: maxValue }, &amp;quot;tagName&amp;quot;)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The MongoDB docs have a great tutorial that you always see used as an example for tag aware sharding - your shard key has to include a prefix field that can be used to figure out which geographical region the user is in, and the range of shard key values that starts with certain regions will be associated with shards in that data center.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all fine and good, but I&amp;rsquo;ll show you that it doesn&amp;rsquo;t have to be nearly that complex.&lt;/p&gt;

&lt;h5 id=&#34;how-you-can-use-tags-to-designate-which-shards-a-sharded-collection-can-use:1bc8c2f688d199692ef36348e0b0b4a8&#34;&gt;How you can use tags to designate which shards a sharded collection can use.&lt;/h5&gt;

&lt;p&gt;Let&amp;rsquo;s walk through an example.   I have three shards in my test cluster:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #f0f0f0&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;    tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; db.getSiblingDB(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;config&amp;quot;&lt;/span&gt;).shards.find()
    { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0000&amp;quot;&lt;/span&gt;, &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;host&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;localhost:30000&amp;quot;&lt;/span&gt; }
    { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0001&amp;quot;&lt;/span&gt;, &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;host&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;localhost:30001&amp;quot;&lt;/span&gt; }
    { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0002&amp;quot;&lt;/span&gt;, &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;host&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;localhost:30002&amp;quot;&lt;/span&gt; }
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;I will add two tags, each to two shards.  Let&amp;rsquo;s say that shards 0000 and 0001 have a lot of RAM, and shards 0001 and 0002 have very fast flash storage and I plan to distribute my data to take advantage of the different physical resources:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #f0f0f0&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;    tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addShardTag(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0000&amp;quot;&lt;/span&gt;,&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;HI_MEM&amp;quot;&lt;/span&gt;)
    tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addShardTag(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0002&amp;quot;&lt;/span&gt;,&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;FLASH&amp;quot;&lt;/span&gt;)
    tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addShardTag(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0001&amp;quot;&lt;/span&gt;,&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;FLASH&amp;quot;&lt;/span&gt;)
    tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addShardTag(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0001&amp;quot;&lt;/span&gt;,&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;HI_MEM&amp;quot;&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Now that I tagged my shards, I will add tag ranges to two different collections.  Note, I don&amp;rsquo;t have these collections yet, and I haven&amp;rsquo;t even sharded them yet, but I want to have the tags ready for them when they get created:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #f0f0f0&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;    tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addTagRange(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.bigidx&amp;quot;&lt;/span&gt;, {_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;MinKey},{_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;MaxKey},&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;HI_MEM&amp;quot;&lt;/span&gt;);
    tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addTagRange(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.bigdata&amp;quot;&lt;/span&gt;, {_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;MinKey},{_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;MaxKey},&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;FLASH&amp;quot;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;I have a collection with big indexes (called bigidx) that I want to constrain only to shards tagged &amp;ldquo;HI_MEM&amp;rdquo; and I have another collection with a lot of data (called bigdata) that I want to keep on shards that have flash storage because I know the data will be read from disk a lot.  Note that I only needed to know what I will be using as my shard key, and I specified MinKey to MaxKey as my range - that means &lt;em&gt;all&lt;/em&gt; of the chunks!&lt;/p&gt;

&lt;p&gt;I will now shard the collections and take a look at how things are shaping up:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #f0f0f0&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.enableSharding(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb&amp;quot;&lt;/span&gt;)
{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; }
tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.shardCollection(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.bigdata&amp;quot;&lt;/span&gt;, {_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;hashed&amp;quot;&lt;/span&gt;})
{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;collectionsharded&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.bigdata&amp;quot;&lt;/span&gt;, &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; }
tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.shardCollection(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.bigidx&amp;quot;&lt;/span&gt;, {_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;hashed&amp;quot;&lt;/span&gt;})
{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;collectionsharded&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.bigidx&amp;quot;&lt;/span&gt;, &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; }
tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.status()
&lt;span style=&#34;color: #666666&#34;&gt;---&lt;/span&gt; Sharding Status &lt;span style=&#34;color: #666666&#34;&gt;---&lt;/span&gt;
  sharding version&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; {
	&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt;,
	&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;version&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;4&lt;/span&gt;,
	&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;minCompatibleVersion&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;4&lt;/span&gt;,
	&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;currentVersion&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;5&lt;/span&gt;,
	&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;clusterId&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; ObjectId(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;535be5d7d5274545e9d01426&amp;quot;&lt;/span&gt;)
  }
  shards&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;
	{  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0000&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;host&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;localhost:30000&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tags&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; [ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;HI_MEM&amp;quot;&lt;/span&gt; ] }
	{  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0001&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;host&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;localhost:30001&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tags&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; [ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;FLASH&amp;quot;&lt;/span&gt;, &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;HI_MEM&amp;quot;&lt;/span&gt; ] }
	{  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0002&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;host&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;localhost:30002&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tags&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; [ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;FLASH&amp;quot;&lt;/span&gt; ] }
  databases&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;
	{  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;admin&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;partitioned&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #007020; font-weight: bold&#34;&gt;false&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;primary&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;config&amp;quot;&lt;/span&gt; }
	{  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb&amp;quot;&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;partitioned&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #007020; font-weight: bold&#34;&gt;true&lt;/span&gt;,  &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;primary&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0001&amp;quot;&lt;/span&gt; }
		tagdb.bigdata
			shard key&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;hashed&amp;quot;&lt;/span&gt; }
			chunks&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;
				shard0001	&lt;span style=&#34;color: #40a070&#34;&gt;3&lt;/span&gt;
				shard0002	&lt;span style=&#34;color: #40a070&#34;&gt;3&lt;/span&gt;
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$minKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0001
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0002
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0001
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0001
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0002
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$maxKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0002
			 tag&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; FLASH  { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$minKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$maxKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } }
		tagdb.bigidx
			shard key&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;hashed&amp;quot;&lt;/span&gt; }
			chunks&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;
				shard0000	&lt;span style=&#34;color: #40a070&#34;&gt;3&lt;/span&gt;
				shard0001	&lt;span style=&#34;color: #40a070&#34;&gt;3&lt;/span&gt;
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$minKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0000
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0000
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0001
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0001
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;3074457345618258602&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0000
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;6148914691236517204&lt;/span&gt; } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$maxKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0001
			 tag&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; HI_MEM  { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$minKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$maxKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } }
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;h5 id=&#34;how-you-can-use-tags-to-make-collection-migrate-from-one-shard-to-another:1bc8c2f688d199692ef36348e0b0b4a8&#34;&gt;How you can use tags to make collection migrate from one shard to another&lt;/h5&gt;

&lt;p&gt;What if you have a number of unsharded collections in your sharded database and you don&amp;rsquo;t want for all of them to hang out on the primary shard for this DB?   Well, you might need unique tags for each shard, but then you can do this to move collection one to &lt;code&gt;shard0001&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #f0f0f0&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addShardTag(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard0002&amp;quot;&lt;/span&gt;,&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard2&amp;quot;&lt;/span&gt;)
tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.addTagRange(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.one&amp;quot;&lt;/span&gt;, {_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;MinKey},{_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;MaxKey},&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;shard2&amp;quot;&lt;/span&gt;)
tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.shardCollection(&lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.one&amp;quot;&lt;/span&gt;,{_id&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt;})
{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;collectionsharded&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;tagdb.one&amp;quot;&lt;/span&gt;, &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; }
tagdb&lt;span style=&#34;border: 1px solid #FF0000&#34;&gt;@&lt;/span&gt;mongos(&lt;span style=&#34;color: #40a070&#34;&gt;2.6&lt;/span&gt;.&lt;span style=&#34;color: #40a070&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color: #666666&#34;&gt;&amp;gt;&lt;/span&gt; sh.status()
   ...
 		tagdb.one
			shard key&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; }
			chunks&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt;
				shard0002	&lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt;
			{ &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$minKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$maxKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } on &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard0002
			 tag&lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; shard2  { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$minKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } } &lt;span style=&#34;color: #666666&#34;&gt;--&amp;gt;&amp;gt;&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; { &lt;span style=&#34;color: #4070a0&#34;&gt;&amp;quot;$maxKey&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #40a070&#34;&gt;1&lt;/span&gt; } }
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;If we peek inside the config database, we should see our tags in the &lt;code&gt;config.tags&lt;/code&gt; collection, our shard ranges attached to chunks in &lt;code&gt;config.chunks&lt;/code&gt; and we can find evidence of the chunk moves due to tag policy in the &lt;code&gt;config.changelog&lt;/code&gt; collection, as well as the &lt;code&gt;mongos&lt;/code&gt; and &lt;code&gt;mongod&lt;/code&gt; log files.&lt;/p&gt;

&lt;p&gt;To summarize: tag aware sharding can be easily used to distribute a single collection a particular way across all shards,  to isolate whole collections on a subset of shards, and even to move an entire collection from one shard to another.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;† This is definitely a gross simplification of all the steps the balancer goes through - look for a more detailed write-up demystifying the inner workings of migrations some time soon.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Model Dynamic Attributes</title>
          <link>http://asya999.github.io/post/dynamicattributes/</link>
          <pubDate>Tue, 15 Apr 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/dynamicattributes/</guid>
          <description>

&lt;h3 id=&#34;question:bfa6b69d18d3a970bc8ca44586b86424&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ve heard that MongoDB can be effectively used to model &amp;ldquo;dynamic attributes&amp;rdquo; - where you don&amp;rsquo;t know up front all the different attributes and not all attributes apply to all items.  Can you describe how that can be done, and in particular how it can be effectively indexed?&lt;/p&gt;

&lt;h3 id=&#34;answer:bfa6b69d18d3a970bc8ca44586b86424&#34;&gt;Answer:&lt;/h3&gt;

&lt;h5 id=&#34;the-problem:bfa6b69d18d3a970bc8ca44586b86424&#34;&gt;The problem:&lt;/h5&gt;

&lt;p&gt;Imagine you are building an e-commerce site and you aspire to be as big as amazon.com some day, which means you will be selling many different types of products.  It&amp;rsquo;s easy to see that there will be sets of attributes that will only apply to some of the products you sell.&lt;/p&gt;

&lt;p&gt;Product document may look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  SKU: &amp;quot;XRD12349&amp;quot;,
  type: &amp;quot;book&amp;quot;,
  title: &amp;quot;MongoDB, The Definitive Guide&amp;quot;,
  ISBN: &amp;quot;xxx&amp;quot;,
  author: [ &amp;quot;Kristina Chodorow&amp;quot;, &amp;quot;Mike Dieroff&amp;quot;],
  genre: [&amp;quot;computing&amp;quot;, &amp;quot;databases&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  SKU: &amp;quot;Y32944EW&amp;quot;,
  type: &amp;quot;shoes&amp;quot;,
  manufacturer: &amp;quot;ShoesForAll&amp;quot;,
  color: &amp;quot;blue&amp;quot;,
  style: &amp;quot;comfort&amp;quot;,
  size: &amp;quot;7B&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see how it would be extremely challenging to manage a collection that has an incredibly wide variety of document &amp;ldquo;shapes&amp;rdquo;.  Now, while some people call MongoDB &amp;ldquo;schemaless&amp;rdquo; I am not a fan of this designation.  The schema of each document is defined by the document itself.  To be able to build a robust applications you need to decide what the schema of the documents will be, otherwise your application will spend as much time examining the documents to learn their schema as providing actual functionality.&lt;/p&gt;

&lt;h5 id=&#34;possible-solutions:bfa6b69d18d3a970bc8ca44586b86424&#34;&gt;Possible solutions:&lt;/h5&gt;

&lt;p&gt;One way to index the attributes you want to be able to search by is by creating an index on each such attribute in a schema like the one above.  This is not practical, even if you use &lt;a href=&#34;http://docs.mongodb.org/manual/core/index-sparse/&#34;&gt;&amp;ldquo;sparse&amp;rdquo; indexes&lt;/a&gt; (since many attributes will be set only on a small subset of the products), because you may end up with dozens, if not hundreds of indexes.  In addition, every time a new attribute is introduced, a new index has to be added on the collection.  Not very practical.&lt;/p&gt;

&lt;p&gt;The other solution, which is a nice generalization of storing attributes which are numerous and not known up-front, is to use an array of key-value pairs.&lt;/p&gt;

&lt;p&gt;Our two sample documents might then become:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  SKU: &amp;quot;XRD12349&amp;quot;,
  type: &amp;quot;book&amp;quot;,
  attr: [
      { &amp;quot;k&amp;quot;: &amp;quot;title&amp;quot;, 
        &amp;quot;v&amp;quot;: &amp;quot;MongoDB, The Definitive Guide, 1st Edition&amp;quot;
      },
      { &amp;quot;k&amp;quot;: &amp;quot;ISBN&amp;quot;,
        &amp;quot;v&amp;quot;: &amp;quot;xxx&amp;quot;
      },
      { &amp;quot;k&amp;quot;: &amp;quot;author&amp;quot;,
        &amp;quot;v&amp;quot;: &amp;quot;Kristina Chodorow&amp;quot;
      },
      { &amp;quot;k&amp;quot;: &amp;quot;author&amp;quot;,
        &amp;quot;v&amp;quot;: &amp;quot;Mike Dieroff&amp;quot;
      },
      { &amp;quot;k&amp;quot;: &amp;quot;genre&amp;quot;,
        &amp;quot;v&amp;quot;: [&amp;quot;computing&amp;quot;, &amp;quot;databases&amp;quot;] 
      }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  SKU: &amp;quot;Y32944EW&amp;quot;,
  type: &amp;quot;shoes&amp;quot;,
  attr: [
      { &amp;quot;k&amp;quot;: &amp;quot;manufacturer&amp;quot;, 
        &amp;quot;v&amp;quot;: &amp;quot;ShoesForAll&amp;quot;,
      },
      { &amp;quot;k&amp;quot;: &amp;quot;color&amp;quot;, 
        &amp;quot;v&amp;quot;: &amp;quot;blue&amp;quot;,
      },
      { &amp;quot;k&amp;quot;: &amp;quot;style&amp;quot;, 
        &amp;quot;v&amp;quot;: &amp;quot;comfort&amp;quot;,
      },
      { &amp;quot;k&amp;quot;: &amp;quot;size&amp;quot;, 
        &amp;quot;v&amp;quot;: &amp;quot;7B&amp;quot;
      }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that for an attribute that can have multiple values you have a choice of storing it as an array in a single &amp;ldquo;key&amp;rdquo; or you can repeat keys that can have more than one value.&lt;/p&gt;

&lt;p&gt;Now we can index all of these attribute values with the following:&lt;/p&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
PRIMARY(2.6.0) &gt; db.products.ensureIndex( { &#34;attr.k&#34;:1, &#34;attr.v&#34;:1 } )
&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at how the queries will execute and use the index by using the &amp;ldquo;explain()&amp;rdquo; helper in MongoDB shell.  When filtering based on attribute key-value pair, remember to use the &lt;code&gt;$elemMatch&lt;/code&gt; operator to indicate that both conditions must be satisfied by the same element of the array.&lt;/p&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
PRIMARY(2.6.0) &gt; db.products.find( 
                                { &#34;attr&#34;: { &#34;$elemMatch&#34; : { &#34;k&#34;:&#34;size&#34;, &#34;v&#34;:&#34;8B&#34; } }
                     } ).explain()
{
    &#34;cursor&#34; : &#34;BtreeCursor attr.k_1_attr.v_1&#34;,
    &#34;isMultiKey&#34; : true,
    &#34;n&#34; : 104,
    &#34;nscannedObjects&#34; : 104,
    &#34;nscanned&#34; : 104,
    &#34;nscannedObjectsAllPlans&#34; : 104,
    &#34;nscannedAllPlans&#34; : 104,
    &#34;scanAndOrder&#34; : false,
    &#34;indexOnly&#34; : false,
    &#34;nYields&#34; : 0,
    &#34;nChunkSkips&#34; : 0,
    &#34;millis&#34; : 2,
    &#34;indexBounds&#34; : {
        &#34;attr.k&#34; : [
            [
                &#34;size&#34;,
                &#34;size&#34;
            ]
        ],
        &#34;attr.v&#34; : [
            [
                &#34;8B&#34;,
                &#34;8B&#34;
            ]
        ]
    },
    &#34;server&#34; : &#34;asyasmacbook.local:27017&#34;,
    &#34;filterSet&#34; : false
}
&lt;/pre&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
PRIMARY(2.6.0) &gt; db.products.find(
                  { &#34;attr&#34; :  { &#34;$elemMatch&#34; : { &#34;k&#34;:&#34;color&#34;, &#34;v&#34;:&#34;blue&#34;}}
              } ).explain()
{
    &#34;cursor&#34; : &#34;BtreeCursor attr.k_1_attr.v_1&#34;,
    &#34;isMultiKey&#34; : true,
    &#34;n&#34; : 98,
    &#34;nscannedObjects&#34; : 98,
    &#34;nscanned&#34; : 98,
    &#34;nscannedObjectsAllPlans&#34; : 98,
    &#34;nscannedAllPlans&#34; : 98,
    &#34;scanAndOrder&#34; : false,
    &#34;indexOnly&#34; : false,
    &#34;nYields&#34; : 0,
    &#34;nChunkSkips&#34; : 0,
    &#34;millis&#34; : 0,
    &#34;indexBounds&#34; : {
        &#34;attr.k&#34; : [
            [
                &#34;color&#34;,
                &#34;color&#34;
            ]
        ],
        &#34;attr.v&#34; : [
            [
                &#34;blue&#34;,
                &#34;blue&#34;
            ]
        ]
    },
    &#34;server&#34; : &#34;asyasmacbook.local:27017&#34;,
    &#34;filterSet&#34; : false
}
&lt;/pre&gt;

&lt;p&gt;Now I&amp;rsquo;ll use both criteria, and I&amp;rsquo;ll add another one for attribute &amp;ldquo;style&amp;rdquo; - since I want to match only when &lt;em&gt;all&lt;/em&gt; are true (rather than when any is true) I will use the &lt;code&gt;$all&lt;/code&gt; operator.  Passing &amp;ldquo;true&amp;rdquo; as an argument to explain will show all considered plans and not just the winning plan.&lt;/p&gt;

&lt;pre class=&#34;prettyprint lang-js&#34;&gt;
PRIMARY(2.6.0) &gt; db.products.find( { &#34;attr&#34; : { &#34;$all&#34; : [ 
                    { &#34;$elemMatch&#34; : { &#34;k&#34;:&#34;style&#34;, &#34;v&#34;:&#34;comfort&#34; } }, 
                    { &#34;$elemMatch&#34; : { &#34;k&#34;:&#34;color&#34;, &#34;v&#34;:&#34;blue&#34; } },
                    { &#34;$elemMatch&#34; : { &#34;k&#34;:&#34;size&#34;, &#34;v&#34;:&#34;8B&#34; } } 
                  ] } } ).explain(true)
{
    &#34;cursor&#34; : &#34;BtreeCursor attr.k_1_attr.v_1&#34;,
    &#34;isMultiKey&#34; : true,
    &#34;n&#34; : 1,
    &#34;nscannedObjects&#34; : 98,
    &#34;nscanned&#34; : 98,
    &#34;nscannedObjectsAllPlans&#34; : 296,
    &#34;nscannedAllPlans&#34; : 298,
    &#34;scanAndOrder&#34; : false,
    &#34;indexOnly&#34; : false,
    &#34;nYields&#34; : 2,
    &#34;nChunkSkips&#34; : 0,
    &#34;millis&#34; : 1,
    &#34;indexBounds&#34; : {
        &#34;attr.k&#34; : [
            [
                &#34;color&#34;,
                &#34;color&#34;
            ]
        ],
        &#34;attr.v&#34; : [
            [
                &#34;blue&#34;,
                &#34;blue&#34;
            ]
        ]
    },
    &#34;allPlans&#34; : [
        {
            &#34;cursor&#34; : &#34;BtreeCursor attr.k_1_attr.v_1&#34;,
            &#34;isMultiKey&#34; : true,
            &#34;n&#34; : 1,
            &#34;nscannedObjects&#34; : 98,
            &#34;nscanned&#34; : 98,
            &#34;scanAndOrder&#34; : false,
            &#34;indexOnly&#34; : false,
            &#34;nChunkSkips&#34; : 0,
            &#34;indexBounds&#34; : {
                &#34;attributes.name&#34; : [
                    [
                        &#34;color&#34;,
                        &#34;color&#34;
                    ]
                ],
                &#34;attributes.value&#34; : [
                    [
                        &#34;blue&#34;,
                        &#34;blue&#34;
                    ]
                ]
            }
        },
        {
            &#34;cursor&#34; : &#34;BtreeCursor attr.k_1_attr.v_1&#34;,
            &#34;isMultiKey&#34; : true,
            &#34;n&#34; : 1,
            &#34;nscannedObjects&#34; : 99,
            &#34;nscanned&#34; : 100,
            &#34;scanAndOrder&#34; : false,
            &#34;indexOnly&#34; : false,
            &#34;nChunkSkips&#34; : 0,
            &#34;indexBounds&#34; : {
                &#34;attributes.name&#34; : [
                    [
                        &#34;style&#34;,
                        &#34;style&#34;
                    ]
                ],
                &#34;attributes.value&#34; : [
                    [
                        &#34;comfort&#34;,
                        &#34;comfort&#34;
                    ]
                ]
            }
        },
        {
            &#34;cursor&#34; : &#34;BtreeCursor attr.k_1_attr.v_1&#34;,
            &#34;isMultiKey&#34; : true,
            &#34;n&#34; : 1,
            &#34;nscannedObjects&#34; : 99,
            &#34;nscanned&#34; : 100,
            &#34;scanAndOrder&#34; : false,
            &#34;indexOnly&#34; : false,
            &#34;nChunkSkips&#34; : 0,
            &#34;indexBounds&#34; : {
                &#34;attributes.name&#34; : [
                    [
                        &#34;size&#34;,
                        &#34;size&#34;
                    ]
                ],
                &#34;attributes.value&#34; : [
                    [
                        &#34;8B&#34;,
                        &#34;8B&#34;
                    ]
                ]
            }
        }
    ],
    &#34;server&#34; : &#34;asyasmacbook.local:27017&#34;,
    &#34;filterSet&#34; : false,
    &#34;stats&#34; : {
        &#34;type&#34; : &#34;KEEP_MUTATIONS&#34;,
        &#34;works&#34; : 100,
        &#34;yields&#34; : 2,
        &#34;unyields&#34; : 2,
        &#34;invalidates&#34; : 0,
        &#34;advanced&#34; : 1,
        &#34;needTime&#34; : 97,
        &#34;needFetch&#34; : 0,
        &#34;isEOF&#34; : 1,
        &#34;children&#34; : [
            {
                &#34;type&#34; : &#34;FETCH&#34;,
                &#34;works&#34; : 99,
                &#34;yields&#34; : 2,
                &#34;unyields&#34; : 2,
                &#34;invalidates&#34; : 0,
                &#34;advanced&#34; : 1,
                &#34;needTime&#34; : 97,
                &#34;needFetch&#34; : 0,
                &#34;isEOF&#34; : 1,
                &#34;alreadyHasObj&#34; : 0,
                &#34;forcedFetches&#34; : 0,
                &#34;matchTested&#34; : 1,
                &#34;children&#34; : [
                    {
                        &#34;type&#34; : &#34;IXSCAN&#34;,
                        &#34;works&#34; : 98,
                        &#34;yields&#34; : 2,
                        &#34;unyields&#34; : 2,
                        &#34;invalidates&#34; : 0,
                        &#34;advanced&#34; : 98,
                        &#34;needTime&#34; : 0,
                        &#34;needFetch&#34; : 0,
                        &#34;isEOF&#34; : 1,
                        &#34;keyPattern&#34; : &#34;{ attr.k: 1.0, attr.v: 1.0 }&#34;,
                        &#34;boundsVerbose&#34; : &#34;field #0[&#39;attr.k&#39;]: [\&#34;color\&#34;, \&#34;color\&#34;], field #1[&#39;attr.v&#39;]: [\&#34;blue\&#34;, \&#34;blue\&#34;]&#34;,
                        &#34;isMultiKey&#34; : 1,
                        &#34;yieldMovedCursor&#34; : 0,
                        &#34;dupsTested&#34; : 98,
                        &#34;dupsDropped&#34; : 0,
                        &#34;seenInvalidated&#34; : 0,
                        &#34;matchTested&#34; : 0,
                        &#34;keysExamined&#34; : 98,
                        &#34;children&#34; : [ ]
                    }
                ]
            }
        ]
    }
}
&lt;/pre&gt;

&lt;p&gt;What does this mean?   If we look at &lt;code&gt;allPlans&lt;/code&gt; we see that the optimizer tried our attribute index separately (but in parallel) with each of the clauses inside the $all array.  The winning plan was for &amp;ldquo;color&amp;rdquo; attribute because it turned out to be the most selective.&lt;/p&gt;

&lt;p&gt;In MongoDB 2.4 this was not possible and unfortunately the optimizer would use the index for the first clause of the &lt;code&gt;$all&lt;/code&gt; expression.  If it happened to have low selectivity, then you didn&amp;rsquo;t get as good performance as you might have, had you ordered your conditions differently.  In 2.6 the order of expressions inside &lt;code&gt;$all&lt;/code&gt; does not make a difference as the one that&amp;rsquo;s most selective will be the one used by the query optimizer.&lt;/p&gt;

&lt;p&gt;Depending on how you need to query your attributes, there are different ways of structuring the attribute array.  You can use key-value pairs as I showed, you can use the attribute name as the key value, or you can even store a single string value &amp;ldquo;attrname::attrvalue&amp;rdquo; - best thing is to take a look at the types of queries and updates you will be running and try it different ways, benchmark which one works best and use that one.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Schema Design - Blog Posts and Comments revisited</title>
          <link>http://asya999.github.io/post/embedorlink/</link>
          <pubDate>Sat, 29 Mar 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/embedorlink/</guid>
          <description>

&lt;h3 id=&#34;question:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;I have a question about whether I should store comments inside the blog post entry or in a separate collection. It&amp;rsquo;d be nice to see examples of how to access various fields in both cases, how to index and in general how to know when to embed and when to link.&lt;/p&gt;

&lt;h3 id=&#34;answer:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;Answer:&lt;/h3&gt;

&lt;h4 id=&#34;the-blog-schema:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;The Blog Schema&lt;/h4&gt;

&lt;p&gt;There has been a lot of discussion and write-ups about how to model a simple blog that allows comments on posts - it&amp;rsquo;s a fairly simple example that everyone can understand, and at the same time it offers several opportunities to choose different ways to structure the schema.  The example usually consists of four concepts: users(or authors), posts, tags on posts and comments on posts.&lt;/p&gt;

&lt;h5 id=&#34;authors:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;Authors&lt;/h5&gt;

&lt;p&gt;Typically everyone agrees that the authors or users are stored in a  collection of their own where you keep their information - everything from their username, password, when they last logged in, when they signed up for the service, etc.&lt;/p&gt;

&lt;h5 id=&#34;posts:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;Posts&lt;/h5&gt;

&lt;p&gt;There is also little argument that posts should be stored separately from authors - I don&amp;rsquo;t think I&amp;rsquo;ve ever heard anyone advocate for embedding posts within author document - that makes no sense for many reason, not the least of them are the fact that you want to avoid unbounded growth of the author document, and querying over posts is a natural function of the use case so posts really should be first class object.&lt;/p&gt;

&lt;p&gt;What isn&amp;rsquo;t always agreed on is whether the author of the post should have just their unique primary key (or username) saved in each post or whether some of the information, like their full name, should also be denormalized into each post.&lt;/p&gt;

&lt;h5 id=&#34;tags:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;Tags&lt;/h5&gt;

&lt;p&gt;Tags being simple strings should be stored inside the post document.  The it advantage of document model over relational is that it allows you to embed an array with multiple values without sacrificing the ability to index the tags:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ &amp;quot;_id&amp;quot; : &amp;lt;Id&amp;gt;,
    &amp;quot;author&amp;quot; :  { &amp;quot;id&amp;quot; :  &amp;lt;authorId&amp;gt;, &amp;quot;name&amp;quot; :  &amp;quot;Asya Kamsky&amp;quot; },
    &amp;quot;tags&amp;quot; :  [ &amp;quot;schema&amp;quot;, &amp;quot;embed&amp;quot;, &amp;quot;link&amp;quot; ],
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can index tags with &lt;code&gt;db.posts.ensureIndex( { tags:1 } )&lt;/code&gt; which will be used in queries like
    db.posts.find( { &amp;ldquo;tags&amp;rdquo; : { &amp;ldquo;$in&amp;rdquo;:  [&amp;ldquo;schema&amp;rdquo;, &amp;ldquo;performance&amp;rdquo;] } } )&lt;/p&gt;

&lt;p&gt;You probably noticed that I happen to think it&amp;rsquo;s right to denormalize the author&amp;rsquo;s name into the post - I&amp;rsquo;m a strong believer in optimizing for the common case, not exceptional one&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7db8f99d4cb4bf37c187790fa7b90df5:fn-f1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7db8f99d4cb4bf37c187790fa7b90df5:fn-f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and I think optimizing query performance is more important than trying to minimize storage at the cost of performance.&lt;/p&gt;

&lt;h5 id=&#34;comments:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;Comments&lt;/h5&gt;

&lt;p&gt;Comment documents, or rather where to store them, usually generates the most discussion and disagreement.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider both options and see what we can gain from each:&lt;/p&gt;

&lt;h6 id=&#34;embed-comments:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;embed comments&lt;/h6&gt;

&lt;pre&gt;&lt;code&gt;{
     _id: &amp;lt;Id&amp;gt;,
     author: { id: &amp;lt;authorId&amp;gt;, name: &amp;quot;Asya Kamsky&amp;quot; },
     tags: [ &amp;quot;schema&amp;quot;, &amp;quot;embed&amp;quot;, &amp;quot;link&amp;quot; ],
     comments: [
         { author : { id:&amp;lt;authorId&amp;gt;,name:&amp;quot;Joe Shmoe&amp;quot;}, 
           date:ISODate(&amp;quot; &amp;quot;), 
           text:&amp;quot;Blah Blah Blah&amp;quot; },
         { author : { id:&amp;lt;authorId&amp;gt;,name:&amp;quot;Jane Doe&amp;quot;}, 
           date:ISODate(&amp;quot; &amp;quot;), 
           text:&amp;quot;Blah Blah Blah&amp;quot; },
         { author : { id:&amp;lt;authorId&amp;gt;,name:&amp;quot;Asya Kamsky&amp;quot;}, 
           date:ISODate(&amp;quot; &amp;quot;), 
           text:&amp;quot;Blah Blah Blah&amp;quot; },
         ...
     ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to other indexes we already plan to have on posts, we will probably need to add several indexes to support querying for comments or by comments.  For example, when someone logs in, I can see wanting to show them all the threads/posts that they commented on, which means we need to index on &amp;ldquo;comments.author.id&amp;rdquo; so that we can query for posts that this author commented on.  We also might need to include fields inside the comments array to track which comments are responses to which other comments, and the biggest downside of them all, if the discussion in comments gets really heated, we will end up with a huge array inside this post.&lt;/p&gt;

&lt;h6 id=&#34;have-separate-comments-collection:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;have separate comments collection&lt;/h6&gt;

&lt;pre&gt;&lt;code&gt;{  post : &amp;lt;postId&amp;gt;,
    author : { id:&amp;lt;authorId&amp;gt;,name:&amp;quot;Joe Shmoe&amp;quot;}, 
    date:ISODate(&amp;quot; &amp;quot;), 
    text:&amp;quot;Blah Blah Blah&amp;quot; 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A collection of comments would have to have an index on the postId so that we can look up the comments for a particular post, probably compound index with date so that we can query for the most recent posts.  We would want to index author.id and date as well.  But the nice thing is that here we can control how many comments we want returned, and even though querying for all comments for a post might involve some random IO, we can minimize it by only querying for as many comments as we intend to display.  The fact is that most of the time the reader of the blog post won&amp;rsquo;t even look at the comments, and if they do then they might read a few and never click on &amp;ldquo;show more&amp;rdquo; which we would normally have.&lt;/p&gt;

&lt;p&gt;Is there a third option?&lt;/p&gt;

&lt;h6 id=&#34;hybrid-option:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;hybrid option&lt;/h6&gt;

&lt;p&gt;The nice thing about flexible schema is that in cases like these you can keep comments in separate collection but also choose to denormalize some small number of comments into the post itself, either first few or the last few or whatever fits your requirements best.&lt;/p&gt;

&lt;p&gt;This hybrid approach may be analogous to the product collection for an e-commerce site where they store reviews of products separately from the product itself, but keep the highest voted reviews  (one positive and one negative) embedded in the product. This is a good schema because when you display the product, you want to display a few most helpful reviews, but you don&amp;rsquo;t need to display all the reviews at that time.&lt;/p&gt;

&lt;h4 id=&#34;summary:7db8f99d4cb4bf37c187790fa7b90df5&#34;&gt;Summary&lt;/h4&gt;

&lt;p&gt;The general principal to use when trying to decide between embedding and linking is this:
- consider which objects are first class entities and which are properties of such entities
- consider what your use case requires to display fast and what allows for additional queries
- when two choices both seem to be viable, prototype both and see which works better&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:7db8f99d4cb4bf37c187790fa7b90df5:fn-f1&#34;&gt;Someone always brings up the possibility that the author will change their name, as if that&amp;rsquo;s an everyday occurrence
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7db8f99d4cb4bf37c187790fa7b90df5:fn-f1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Can I use more replica nodes to scale?</title>
          <link>http://asya999.github.io/post/canreplicashelpscaling/</link>
          <pubDate>Thu, 20 Feb 2014 00:00:00 UTC</pubDate>
          <author></author>
          <guid>http://asya999.github.io/post/canreplicashelpscaling/</guid>
          <description>

&lt;h3 id=&#34;question:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;Question:&lt;/h3&gt;

&lt;p&gt;Do replica sets help with read scaling?  If there are more servers to service all my read requests, why wouldn&amp;rsquo;t they be able to service more read requests, or service the same number of read requests faster?&lt;/p&gt;

&lt;h3 id=&#34;answer:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;Answer:&lt;/h3&gt;

&lt;h5 id=&#34;replica-sets:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;Replica Sets.&lt;/h5&gt;

&lt;p&gt;Replica sets are an awesome feature of MongoDB.  They give you &amp;ldquo;High Availability&amp;rdquo; - meaning that when the primary node becomes unavailable (crashes, gets unplugged from the network, gets DOS&amp;rsquo;ed by another process on the same box) the rest of the nodes will &lt;strong&gt;elect&lt;/strong&gt; a new primary and the driver (which your application uses to communicate with MongoDB) will automatically track all nodes and when the primary role changes from one server to another, it will automatically detect to send requests there.&lt;/p&gt;

&lt;h5 id=&#34;single-master:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;Single Master:&lt;/h5&gt;

&lt;p&gt;MongoDB Replica Sets are a &amp;ldquo;single master&amp;rdquo; architecture.  That means that all writes must go to the one primary and from there they are asynchronously replicated to all secondaries.   Your reads also go to the primary, meaning you can always read your own writes.  Your read requests would &lt;em&gt;never&lt;/em&gt; be sent to a secondary unless your application &lt;em&gt;explicitly&lt;/em&gt; requests that the read go somewhere other than the primary, so you would never be getting &amp;ldquo;stale&amp;rdquo; data without being aware of it.&lt;/p&gt;

&lt;h5 id=&#34;the-questions-we-have-are:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;The questions we have are:&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;Should you use secondary reads?&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;Will they help you handle more reads?&lt;/li&gt;
&lt;li&gt;Will they help you handle same reads faster?&lt;/li&gt;
&lt;li&gt;Are there downsides?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;When should you use secondary reads?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because secondary reads create complexity for your application which now needs to handle (possibly very) stale data and situations where just-made-write is not available for read, I would caution users to be sure that they are getting some benefits from secondary reads.  I’m going to look at possible benefits and discuss whether they are likely to be realized using secondary replica nodes or not.&lt;/p&gt;

&lt;h5 id=&#34;will-more-servers-help-you-handle-same-reads-faster:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;Will more servers help you handle same reads faster?&lt;/h5&gt;

&lt;p&gt;I think the answer for simple operational reads is obviously no.  If a read takes 10μs then it&amp;rsquo;s not likely to take 1/5th of that just because there are five servers - this is a single unit of work.  That&amp;rsquo;s the actual duration of the read.&lt;/p&gt;

&lt;h5 id=&#34;will-more-servers-help-you-handle-more-reads:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;Will more servers help you handle more reads?&lt;/h5&gt;

&lt;p&gt;Intuitively, it feels like the answer should be “yes” - but that would only be the case if the reads somehow interfered with each other on the single node.  If they are reading the same “hot” data then they can be working in parallel up to the limit of your CPUs.   So in real life, the answer to whether all your replica nodes together can handle more reads than just your primary is maybe yes and maybe no. Usually no. It all depends on why your single primary cannot handle all of the reads by itself.&lt;/p&gt;

&lt;p&gt;My assumption is if your primary can handle all of the reads by itself, then you would have very little reason to even consider reading stale data from a secondary - you gain very little and lose strong consistency of data. There can be some scenarios where reading from a secondary will reduce the latency to the server (not the actual duration of the read) but those are rather specific use cases I’ll point out at the end of the article.&lt;/p&gt;

&lt;p&gt;Okay, so that leaves us with the sad case of a primary that is not able to handle all the reads all by itself. Now we must ask ourselves why it cannot handle all the reads. Depending on the reason why, we can try to predict whether directing some of those reads to secondaries will help the overall situation.&lt;/p&gt;

&lt;p&gt;If reads are not handled by primary alone because they are too slow then it doesn&amp;rsquo;t matter why they are too slow - they will be too slow on the secondaries as well. You can try to tune your queries, but it’s more likely that the queries are slow because of underlying root causes&amp;hellip; let&amp;rsquo;s look at those&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Indexes don&amp;rsquo;t fit in RAM - oops! we know that every replica set member has identical data so if indexes don&amp;rsquo;t fit in RAM on the primary, they don&amp;rsquo;t fit in RAM on the secondary either!&lt;/li&gt;
&lt;li&gt;Too much data being scanned (usually because working set doesn&amp;rsquo;t fit in RAM) and the disks are slow - pretty much the same as (1) since the secondaries will have the same limitation&lt;/li&gt;
&lt;li&gt;The large number of writes are starving out the reads (i.e. readers have to wait for writers and if there are too many writers when writers yield other writers go and reads can get starved in extreme cases).  But the writes that happen on the primary also have to happen on the secondary!  So moving the readers to the secondary will just starve them on the secondary instead of starving them on the primary.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I know what some of you may be thinking on (3) - you&amp;rsquo;re thinking that all the reads may be starving on the primary but if you split the reads between the primary and the secondary maybe they won&amp;rsquo;t get starved?&lt;/p&gt;

&lt;p&gt;Maybe.  But you know what you did when you achieved your read
requirements by splitting the load between the primary and the
secondary?   &lt;strong&gt;You gave up HA&lt;/strong&gt;.  Remember, if you can only service the incoming load when all of your nodes are up, then you don&amp;rsquo;t have &lt;em&gt;any&lt;/em&gt; high availability, as losing a node will basically start starving out some of the incoming requests which means you won&amp;rsquo;t meet your SLAs.&lt;/p&gt;

&lt;p&gt;Since replication is for High Availability, it means some of its capacity simply must not be tied up so that it can be “standing by” in case of a failover. If you want to use some extra capacity that you perceive is otherwise going &amp;ldquo;wasted&amp;rdquo; to service your every day load, maybe you can do it, as long as you have a very clear understanding that you may have given up some of that High Availability, and I would definitely recommend against that.&lt;/p&gt;

&lt;p&gt;One thing I invite you to try is to set up a simple test &lt;code&gt;mongod&lt;/code&gt; instance where you take some collection with some indexes that all fit in RAM and start up a bunch of multi-threaded clients (from other machines otherwise you&amp;rsquo;ll be testing something else) and have those clients hammer the server with read requests. See how many clients you have to add and how many requests you have to throw at &lt;code&gt;mongod&lt;/code&gt; before you can&amp;rsquo;t add any more clients requesting more reads without performance of existing reads suffering.  Trying it out will give you an idea of the maximum read throughput that a single node can handle.&lt;/p&gt;

&lt;p&gt;Of course, this is not the complete story.  It turns out that there are some excellent use cases for secondary reads, some more common, some less so.&lt;/p&gt;

&lt;h5 id=&#34;the-types-of-reads-to-route-to-secondaries:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;The types of reads to route to secondaries&lt;/h5&gt;

&lt;p&gt;There are two types of use cases for reads that you do want to route to secondaries. One I already alluded to: if you have a read heavy system and reads are not super-sensitive to staleness of data but they &lt;em&gt;are&lt;/em&gt; super sensitive to overall latency of satisfying a read request, you can realize a big win reading from the nearest member of the replica set rather than the primary.  If your SLA says reads must return in under 100ms but your one way network latency to the primary is 75ms how can you satisfy the SLA from various parts of the world? That&amp;rsquo;s a use case for distributing data all over the world via secondaries in your replica set!  If you have nodes in data centers on different continents (and the PRIMARY near your write source) you can specify read preference of &amp;ldquo;nearest&amp;rdquo; and make sure that each read request goes to the node that has the lowest network latency from the requester.&lt;/p&gt;

&lt;p&gt;Note that this is &lt;em&gt;not&lt;/em&gt; technically &amp;ldquo;scaling&amp;rdquo; your read capacity, this is basically taking advantage of replication already having pulled the data over the long network connection and reducing your network latency to the DB.  Don’t forget that if you are relying on reading from “nearest” to meet your SLAs for total round trip to get the data, you have to consider whether your SLAs will still be met if a secondary in a particular region fails.&lt;/p&gt;

&lt;p&gt;The second use case is about reads that are not &amp;ldquo;typical&amp;rdquo; of your &amp;ldquo;normal&amp;rdquo; operational load.  Now, “typical” and “normal” is going to be different for different use cases, but some common examples are things like nightly ETL jobs, ad hoc &amp;ldquo;historical&amp;rdquo; or analytical queries, regular backup jobs, or a Hadoop job.  It could also be something else.  The reason you want to isolate this “atypical” load to the secondary is because it will &amp;ldquo;mess up&amp;rdquo; your memory-resident data set on the primary!&lt;/p&gt;

&lt;p&gt;Imagine you worked very hard to optimize your queries so that indexes are always in RAM and there is just enough RAM left over for the &amp;ldquo;hot&amp;rdquo; subset of data to give you excellent performance. You then start running an analytics query and it&amp;rsquo;s pulling &lt;em&gt;all&lt;/em&gt; the data from mongo which means your entire data set, hot and cold, now gets pulled into resident memory. Guess what just got evicted to make room for it? Yep, your &amp;ldquo;normal&amp;rdquo; data set, or at least a very large portion of it.†&lt;/p&gt;

&lt;p&gt;To keep &amp;ldquo;atypical&amp;rdquo; jobs from interfering with your &amp;ldquo;typical&amp;rdquo; work load, configure the atypical ones to use read preference Secondary. Note that Secondary is different from SecondaryPreferred as the latter will go to the Primary if there is no available Secondary but the former will return an error if there is no Secondary to read from.  Since these jobs are usually not urgent, you’d rather have them wait and retry later than interfere with operational responsiveness of your app anyway.  You can even use &lt;a href=&#34;http://docs.mongodb.org/manual/tutorial/configure-replica-set-tag-sets/&#34;&gt;&amp;ldquo;tags&amp;rdquo;&lt;/a&gt; to isolate different jobs to specific nodes even further.&lt;/p&gt;

&lt;h5 id=&#34;bottom-line:d15f8c1ea7d4b31deafbbed0f16c3829&#34;&gt;Bottom line&lt;/h5&gt;

&lt;p&gt;Make your primary do the work that the application relies on every second.  Make sure that &lt;em&gt;at least&lt;/em&gt; one secondary is idle and ready to take over for the primary in case of failure.  Use additional secondaries (&lt;em&gt;not&lt;/em&gt; the hot standby one!) for all other needs whether it’s backups, analytical reports, ad hoc queries, ETL, etc.  This way, you know that you can handle the application requirements after a failover just as well as before.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;†  By the way, this is exactly what happens when you have a query that doesn&amp;rsquo;t have a good index to use - it does a full collection scan - more on that in a future &amp;ldquo;answer&amp;rdquo;.&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
